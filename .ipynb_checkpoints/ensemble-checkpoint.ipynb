{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc709d59-3ab6-401e-a803-4ff31993ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "\n",
    "from finrl import config\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import data_split\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dfed73f-b71d-450e-8532-471fc33abde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_symbols = [\n",
    "#     'ADANIENT', 'ADANIPORTS', 'APOLLOHOSP', 'ASIANPAINT', 'AXISBANK',\n",
    "#     'BAJAJ-AUTO', 'BAJFINANCE', 'BAJAJFINSV', 'BPCL', 'BHARTIARTL',\n",
    "#     'BRITANNIA', 'CIPLA', 'COALINDIA', 'DIVISLAB', 'DRREDDY', 'EICHERMOT',\n",
    "#     'GRASIM', 'HCLTECH', 'HDFCBANK', 'HDFCLIFE', 'HEROMOTOCO', 'HINDALCO',\n",
    "#     'HINDUNILVR', 'ICICIBANK', 'INDUSINDBK', 'INFY', 'ITC', 'JSWSTEEL',\n",
    "#     'KOTAKBANK', 'LT', 'LTIM', 'M&M', 'MARUTI', 'NESTLEIND', 'NTPC', 'ONGC',\n",
    "#     'POWERGRID', 'RELIANCE', 'SBILIFE', 'SBIN', 'SUNPHARMA', 'TATAMOTORS',\n",
    "#     'TATASTEEL', 'TCS', 'TATACONSUM', 'TECHM', 'TITAN', 'ULTRACEMCO', 'UPL',\n",
    "#     'WIPRO'\n",
    "# ]\n",
    "\n",
    "# ns_company_symbols = [symbol + '.NS' for symbol in company_symbols]\n",
    "\n",
    "# print(ns_company_symbols)\n",
    "# symbols=ns_company_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09ec60f-d1f4-4b21-97a6-60fe0c7cc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = '2010-01-01'\n",
    "TRAIN_END_DATE = '2020-07-01'\n",
    "TRADE_START_DATE = '2020-07-01'\n",
    "TRADE_END_DATE = '2023-05-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70746a8e-df0e-49b5-a146-21dab4e23a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "#                                 end_date = TRADE_END_DATE,\n",
    "#                                 ticker_list = symbols).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47b33b42-1022-48ba-9bb4-724b1fa70c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw=pd.read_csv('datasets/BSE30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5e44b-1124-4426-87de-925b8c8fff98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19eecd6d-8e85-4f7e-a6b3-542e23cab110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105703 entries, 0 to 105702\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   date    105703 non-null  object \n",
      " 1   open    105703 non-null  float64\n",
      " 2   high    105703 non-null  float64\n",
      " 3   low     105703 non-null  float64\n",
      " 4   close   105703 non-null  float64\n",
      " 5   volume  105703 non-null  int64  \n",
      " 6   tic     105703 non-null  object \n",
      " 7   day     105703 non-null  int64  \n",
      "dtypes: float64(4), int64(2), object(2)\n",
      "memory usage: 6.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe9b1894-7f81-4c66-958f-2932b8cc5ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from stockstats import StockDataFrame as Sdf\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(*, file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    load csv dataset from path\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    # _data = pd.read_csv(f\"{config.DATASET_DIR}/{file_name}\")\n",
    "    _data = pd.read_csv(file_name)\n",
    "    return _data\n",
    "\n",
    "\n",
    "def data_split(df, start, end, target_date_col=\"date\"):\n",
    "    \"\"\"\n",
    "    split the dataset into training or testing using date\n",
    "    :param data: (df) pandas dataframe, start, end\n",
    "    :return: (df) pandas dataframe\n",
    "    \"\"\"\n",
    "    data = df[(df[target_date_col] >= start) & (df[target_date_col] < end)]\n",
    "    data = data.sort_values([target_date_col, \"tic\"], ignore_index=True)\n",
    "    data.index = data[target_date_col].factorize()[0]\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_to_datetime(time):\n",
    "    time_fmt = \"%Y-%m-%dT%H:%M:%S\"\n",
    "    if isinstance(time, str):\n",
    "        return datetime.datetime.strptime(time, time_fmt)\n",
    "\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Provides methods for preprocessing the stock price data\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        use_technical_indicator : boolean\n",
    "            we technical indicator or not\n",
    "        tech_indicator_list : list\n",
    "            a list of technical indicator names (modified from neofinrl_config.py)\n",
    "        use_turbulence : boolean\n",
    "            use turbulence index or not\n",
    "        user_defined_feature:boolean\n",
    "            use user defined features or not\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    preprocess_data()\n",
    "        main method to do the feature engineering\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_technical_indicator=True,\n",
    "        tech_indicator_list=config.INDICATORS,\n",
    "        use_vix=False,\n",
    "        use_turbulence=False,\n",
    "        user_defined_feature=False,\n",
    "    ):\n",
    "        self.use_technical_indicator = use_technical_indicator\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "        self.use_vix = use_vix\n",
    "        self.use_turbulence = use_turbulence\n",
    "        self.user_defined_feature = user_defined_feature\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        \"\"\"main method to do the feature engineering\n",
    "        @:param config: source dataframe\n",
    "        @:return: a DataMatrices object\n",
    "        \"\"\"\n",
    "        # clean data\n",
    "        df = self.clean_data(df)\n",
    "\n",
    "        # add technical indicators using stockstats\n",
    "        if self.use_technical_indicator:\n",
    "            df = self.add_technical_indicator(df)\n",
    "            print(\"Successfully added technical indicators\")\n",
    "\n",
    "        # add vix for multiple stock\n",
    "        if self.use_vix:\n",
    "            df = self.add_vix(df)\n",
    "            print(\"Successfully added vix\")\n",
    "\n",
    "        # add turbulence index for multiple stock\n",
    "        if self.use_turbulence:\n",
    "            df = self.add_turbulence(df)\n",
    "            print(\"Successfully added turbulence index\")\n",
    "\n",
    "        # add user defined feature\n",
    "        if self.user_defined_feature:\n",
    "            df = self.add_user_defined_feature(df)\n",
    "            print(\"Successfully added user defined features\")\n",
    "\n",
    "        # fill the missing values at the beginning and the end\n",
    "        df = df.ffill().bfill()\n",
    "        return df\n",
    "\n",
    "    def clean_data(self, data):\n",
    "        \"\"\"\n",
    "        clean the raw data\n",
    "        deal with missing values\n",
    "        reasons: stocks could be delisted, not incorporated at the time step\n",
    "        :param data: (df) pandas dataframe\n",
    "        :return: (df) pandas dataframe\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        df = df.sort_values([\"date\", \"tic\"], ignore_index=True)\n",
    "        df.index = df.date.factorize()[0]\n",
    "        merged_closes = df.pivot_table(index=\"date\", columns=\"tic\", values=\"close\")\n",
    "        merged_closes = merged_closes.fillna(merged_closes.mean())\n",
    "        # merged_closes = merged_closes.fillna(merged_closes.mean())\n",
    "        tics = merged_closes.columns\n",
    "        df = df[df.tic.isin(tics)]\n",
    "        \n",
    "        # df = data.copy()\n",
    "        # list_ticker = df[\"tic\"].unique().tolist()\n",
    "        # # only apply to daily level data, need to fix for minute level\n",
    "        # list_date = list(pd.date_range(df['date'].min(),df['date'].max()).astype(str))\n",
    "        # combination = list(itertools.product(list_date,list_ticker))\n",
    "\n",
    "        # df_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(df,on=[\"date\",\"tic\"],how=\"left\")\n",
    "        # df_full = df_full[df_full['date'].isin(df['date'])]\n",
    "        # df_full = df_full.sort_values(['date','tic'])\n",
    "        # df_full = df_full.fillna(0)\n",
    "        return df\n",
    "\n",
    "    def add_technical_indicator(self, data):\n",
    "        \"\"\"\n",
    "        calculate technical indicators\n",
    "        use stockstats package to add technical inidactors\n",
    "        :param data: (df) pandas dataframe\n",
    "        :return: (df) pandas dataframe\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        df = df.sort_values(by=[\"tic\", \"date\"])\n",
    "        stock = Sdf.retype(df.copy())\n",
    "        unique_ticker = stock.tic.unique()\n",
    "\n",
    "        for indicator in self.tech_indicator_list:\n",
    "            indicator_df = pd.DataFrame()\n",
    "            for i in range(len(unique_ticker)):\n",
    "                try:\n",
    "                    temp_indicator = stock[stock.tic == unique_ticker[i]][indicator]\n",
    "                    temp_indicator = pd.DataFrame(temp_indicator)\n",
    "                    temp_indicator[\"tic\"] = unique_ticker[i]\n",
    "                    temp_indicator[\"date\"] = df[df.tic == unique_ticker[i]][\n",
    "                        \"date\"\n",
    "                    ].to_list()\n",
    "                    # indicator_df = indicator_df.append(\n",
    "                    #     temp_indicator, ignore_index=True\n",
    "                    # )\n",
    "                    indicator_df = pd.concat(\n",
    "                        [indicator_df, temp_indicator], axis=0, ignore_index=True\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            df = df.merge(\n",
    "                indicator_df[[\"tic\", \"date\", indicator]], on=[\"tic\", \"date\"], how=\"left\"\n",
    "            )\n",
    "        df = df.sort_values(by=[\"date\", \"tic\"])\n",
    "        return df\n",
    "        # df = data.set_index(['date','tic']).sort_index()\n",
    "        # df = df.join(df.groupby(level=0, group_keys=False).apply(lambda x, y: Sdf.retype(x)[y], y=self.tech_indicator_list))\n",
    "        # return df.reset_index()\n",
    "\n",
    "    def add_user_defined_feature(self, data):\n",
    "        \"\"\"\n",
    "         add user defined features\n",
    "        :param data: (df) pandas dataframe\n",
    "        :return: (df) pandas dataframe\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        df[\"daily_return\"] = df.close.pct_change(1)\n",
    "        # df['return_lag_1']=df.close.pct_change(2)\n",
    "        # df['return_lag_2']=df.close.pct_change(3)\n",
    "        # df['return_lag_3']=df.close.pct_change(4)\n",
    "        # df['return_lag_4']=df.close.pct_change(5)\n",
    "        return df\n",
    "\n",
    "    def add_vix(self, data):\n",
    "        \"\"\"\n",
    "        add vix from yahoo finance\n",
    "        :param data: (df) pandas dataframe\n",
    "        :return: (df) pandas dataframe\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        df_vix = YahooDownloader(\n",
    "            start_date=df.date.min(), end_date=df.date.max(), ticker_list=[\"^VIX\"]\n",
    "        ).fetch_data()\n",
    "        vix = df_vix[[\"date\", \"close\"]]\n",
    "        vix.columns = [\"date\", \"vix\"]\n",
    "\n",
    "        df = df.merge(vix, on=\"date\")\n",
    "        df = df.sort_values([\"date\", \"tic\"]).reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def add_turbulence(self, data):\n",
    "        \"\"\"\n",
    "        add turbulence index from a precalcualted dataframe\n",
    "        :param data: (df) pandas dataframe\n",
    "        :return: (df) pandas dataframe\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        turbulence_index = self.calculate_turbulence(df)\n",
    "        df = df.merge(turbulence_index, on=\"date\")\n",
    "        df = df.sort_values([\"date\", \"tic\"]).reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def calculate_turbulence(self, data):\n",
    "        \"\"\"calculate turbulence index based on dow 30\"\"\"\n",
    "        # can add other market assets\n",
    "        df = data.copy()\n",
    "        df_price_pivot = df.pivot(index=\"date\", columns=\"tic\", values=\"close\")\n",
    "        # use returns to calculate turbulence\n",
    "        df_price_pivot = df_price_pivot.pct_change()\n",
    "\n",
    "        unique_date = df.date.unique()\n",
    "        # start after a year\n",
    "        start = 252\n",
    "        turbulence_index = [0] * start\n",
    "        # turbulence_index = [0]\n",
    "        count = 0\n",
    "        for i in range(start, len(unique_date)):\n",
    "            current_price = df_price_pivot[df_price_pivot.index == unique_date[i]]\n",
    "            # use one year rolling window to calcualte covariance\n",
    "            hist_price = df_price_pivot[\n",
    "                (df_price_pivot.index < unique_date[i])\n",
    "                & (df_price_pivot.index >= unique_date[i - 252])\n",
    "            ]\n",
    "            # Drop tickers which has number missing values more than the \"oldest\" ticker\n",
    "            filtered_hist_price = hist_price.iloc[\n",
    "                hist_price.isna().sum().min() :\n",
    "            ].dropna(axis=1)\n",
    "\n",
    "            cov_temp = filtered_hist_price.cov()\n",
    "            current_temp = current_price[[x for x in filtered_hist_price]] - np.mean(\n",
    "                filtered_hist_price, axis=0\n",
    "            )\n",
    "            # cov_temp = hist_price.cov()\n",
    "            # current_temp=(current_price - np.mean(hist_price,axis=0))\n",
    "\n",
    "            temp = current_temp.values.dot(np.linalg.pinv(cov_temp)).dot(\n",
    "                current_temp.values.T\n",
    "            )\n",
    "            if temp > 0:\n",
    "                count += 1\n",
    "                if count > 2:\n",
    "                    turbulence_temp = temp[0][0]\n",
    "                else:\n",
    "                    # avoid large outlier because of the calculation just begins\n",
    "                    turbulence_temp = 0\n",
    "            else:\n",
    "                turbulence_temp = 0\n",
    "            turbulence_index.append(turbulence_temp)\n",
    "        try:\n",
    "            turbulence_index = pd.DataFrame(\n",
    "                {\"date\": df_price_pivot.index, \"turbulence\": turbulence_index}\n",
    "            )\n",
    "        except ValueError:\n",
    "            raise Exception(\"Turbulence information could not be added.\")\n",
    "        return turbulence_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "638a5036-ce5b-45e4-be69-ef5f6a46da2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>88.550003</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>19140</td>\n",
       "      <td>ASIANPAINT.BO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>105.800003</td>\n",
       "      <td>109.599998</td>\n",
       "      <td>103.459999</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>4536215</td>\n",
       "      <td>AXISBANK.BO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>206.050003</td>\n",
       "      <td>210.500000</td>\n",
       "      <td>196.500000</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>52648</td>\n",
       "      <td>BAJAJ-AUTO.BO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>15.140000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>14.975000</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>136590</td>\n",
       "      <td>BAJAJFINSV.BO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>6.660000</td>\n",
       "      <td>6.970000</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>274220</td>\n",
       "      <td>BAJFINANCE.BO</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date        open        high         low       close   volume  \\\n",
       "0  2009-01-02   90.750000   90.750000   88.550003   48.861801    19140   \n",
       "1  2009-01-02  105.800003  109.599998  103.459999   71.914917  4536215   \n",
       "2  2009-01-02  206.050003  210.500000  196.500000  158.413025    52648   \n",
       "3  2009-01-02   15.140000   15.800000   14.975000   13.401811   136590   \n",
       "4  2009-01-02    6.660000    6.970000    6.350000    2.746401   274220   \n",
       "\n",
       "             tic  day  \n",
       "0  ASIANPAINT.BO    4  \n",
       "1    AXISBANK.BO    4  \n",
       "2  BAJAJ-AUTO.BO    4  \n",
       "3  BAJAJFINSV.BO    4  \n",
       "4  BAJFINANCE.BO    4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "febd40c0-9d74-44a1-9fcf-a3b6f7b7bbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\AppData\\Local\\Temp\\ipykernel_14768\\50613336.py:222: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df_price_pivot = df_price_pivot.pct_change()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "from finrl.config import INDICATORS\n",
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                      tech_indicator_list = INDICATORS,\n",
    "                      use_vix=False,\n",
    "                      use_turbulence=True,\n",
    "                      user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083563ce-5668-49e3-abed-4c42a4bfa9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>88.550003</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>19140</td>\n",
       "      <td>ASIANPAINT.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>105.800003</td>\n",
       "      <td>109.599998</td>\n",
       "      <td>103.459999</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>4536215</td>\n",
       "      <td>AXISBANK.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>206.050003</td>\n",
       "      <td>210.500000</td>\n",
       "      <td>196.500000</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>52648</td>\n",
       "      <td>BAJAJ-AUTO.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>15.140000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>14.975000</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>136590</td>\n",
       "      <td>BAJAJFINSV.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>6.660000</td>\n",
       "      <td>6.970000</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>274220</td>\n",
       "      <td>BAJFINANCE.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105698</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>981.000000</td>\n",
       "      <td>992.500000</td>\n",
       "      <td>979.250000</td>\n",
       "      <td>986.799988</td>\n",
       "      <td>26056</td>\n",
       "      <td>SUNPHARMA.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.263414</td>\n",
       "      <td>1019.314408</td>\n",
       "      <td>965.265603</td>\n",
       "      <td>50.085294</td>\n",
       "      <td>14.481255</td>\n",
       "      <td>1.567920</td>\n",
       "      <td>983.446670</td>\n",
       "      <td>985.046100</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105699</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>3208.000000</td>\n",
       "      <td>3227.199951</td>\n",
       "      <td>3197.149902</td>\n",
       "      <td>3175.769043</td>\n",
       "      <td>51644</td>\n",
       "      <td>TCS.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>-15.398183</td>\n",
       "      <td>3235.633708</td>\n",
       "      <td>3045.249324</td>\n",
       "      <td>48.649310</td>\n",
       "      <td>67.966063</td>\n",
       "      <td>0.407494</td>\n",
       "      <td>3131.238102</td>\n",
       "      <td>3257.234477</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105700</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>983.000000</td>\n",
       "      <td>1026.650024</td>\n",
       "      <td>982.950012</td>\n",
       "      <td>986.955139</td>\n",
       "      <td>279514</td>\n",
       "      <td>TECHM.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>-22.941437</td>\n",
       "      <td>1102.074200</td>\n",
       "      <td>929.293964</td>\n",
       "      <td>44.970681</td>\n",
       "      <td>-99.119890</td>\n",
       "      <td>22.233939</td>\n",
       "      <td>1033.226742</td>\n",
       "      <td>1032.633037</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105701</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>2663.500000</td>\n",
       "      <td>2679.300049</td>\n",
       "      <td>2620.050049</td>\n",
       "      <td>2640.399902</td>\n",
       "      <td>32742</td>\n",
       "      <td>TITAN.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>43.161331</td>\n",
       "      <td>2669.834479</td>\n",
       "      <td>2492.325506</td>\n",
       "      <td>60.306098</td>\n",
       "      <td>116.653875</td>\n",
       "      <td>37.463255</td>\n",
       "      <td>2542.391650</td>\n",
       "      <td>2482.099988</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105702</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>7524.950195</td>\n",
       "      <td>7576.549805</td>\n",
       "      <td>7487.549805</td>\n",
       "      <td>7520.226562</td>\n",
       "      <td>8316</td>\n",
       "      <td>ULTRACEMCO.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>28.692164</td>\n",
       "      <td>7774.630825</td>\n",
       "      <td>7303.703648</td>\n",
       "      <td>55.649892</td>\n",
       "      <td>27.765845</td>\n",
       "      <td>3.677975</td>\n",
       "      <td>7441.862826</td>\n",
       "      <td>7301.539185</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105703 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date         open         high          low        close  \\\n",
       "0       2009-01-02    90.750000    90.750000    88.550003    48.861801   \n",
       "1       2009-01-02   105.800003   109.599998   103.459999    71.914917   \n",
       "2       2009-01-02   206.050003   210.500000   196.500000   158.413025   \n",
       "3       2009-01-02    15.140000    15.800000    14.975000    13.401811   \n",
       "4       2009-01-02     6.660000     6.970000     6.350000     2.746401   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "105698  2023-04-28   981.000000   992.500000   979.250000   986.799988   \n",
       "105699  2023-04-28  3208.000000  3227.199951  3197.149902  3175.769043   \n",
       "105700  2023-04-28   983.000000  1026.650024   982.950012   986.955139   \n",
       "105701  2023-04-28  2663.500000  2679.300049  2620.050049  2640.399902   \n",
       "105702  2023-04-28  7524.950195  7576.549805  7487.549805  7520.226562   \n",
       "\n",
       "         volume            tic  day       macd      boll_ub      boll_lb  \\\n",
       "0         19140  ASIANPAINT.BO    4   0.000000    50.523346    48.068260   \n",
       "1       4536215    AXISBANK.BO    4   0.000000    50.523346    48.068260   \n",
       "2         52648  BAJAJ-AUTO.BO    4   0.000000    50.523346    48.068260   \n",
       "3        136590  BAJAJFINSV.BO    4   0.000000    50.523346    48.068260   \n",
       "4        274220  BAJFINANCE.BO    4   0.000000    50.523346    48.068260   \n",
       "...         ...            ...  ...        ...          ...          ...   \n",
       "105698    26056   SUNPHARMA.BO    4  -0.263414  1019.314408   965.265603   \n",
       "105699    51644         TCS.BO    4 -15.398183  3235.633708  3045.249324   \n",
       "105700   279514       TECHM.BO    4 -22.941437  1102.074200   929.293964   \n",
       "105701    32742       TITAN.BO    4  43.161331  2669.834479  2492.325506   \n",
       "105702     8316  ULTRACEMCO.BO    4  28.692164  7774.630825  7303.703648   \n",
       "\n",
       "            rsi_30      cci_30       dx_30  close_30_sma  close_60_sma  \\\n",
       "0       100.000000   66.666667  100.000000     48.861801     48.861801   \n",
       "1       100.000000   66.666667  100.000000     71.914917     71.914917   \n",
       "2       100.000000   66.666667  100.000000    158.413025    158.413025   \n",
       "3       100.000000   66.666667  100.000000     13.401811     13.401811   \n",
       "4       100.000000   66.666667  100.000000      2.746401      2.746401   \n",
       "...            ...         ...         ...           ...           ...   \n",
       "105698   50.085294   14.481255    1.567920    983.446670    985.046100   \n",
       "105699   48.649310   67.966063    0.407494   3131.238102   3257.234477   \n",
       "105700   44.970681  -99.119890   22.233939   1033.226742   1032.633037   \n",
       "105701   60.306098  116.653875   37.463255   2542.391650   2482.099988   \n",
       "105702   55.649892   27.765845    3.677975   7441.862826   7301.539185   \n",
       "\n",
       "        turbulence  \n",
       "0         0.000000  \n",
       "1         0.000000  \n",
       "2         0.000000  \n",
       "3         0.000000  \n",
       "4         0.000000  \n",
       "...            ...  \n",
       "105698   43.069415  \n",
       "105699   43.069415  \n",
       "105700   43.069415  \n",
       "105701   43.069415  \n",
       "105702   43.069415  \n",
       "\n",
       "[105703 rows x 17 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b299a545-307a-4d24-a530-9c7a1e0e84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b45789e-5f8a-4a0d-a1ba-070d4601b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ticker = df[\"tic\"].unique().tolist()\n",
    "# only apply to daily level data, need to fix for minute level\n",
    "list_date = list(pd.date_range(df['date'].min(),df['date'].max()).astype(str))\n",
    "combination = list(itertools.product(list_date,list_ticker))\n",
    "\n",
    "df_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(df,on=[\"date\",\"tic\"],how=\"left\")\n",
    "df_full = df_full[df_full['date'].isin(df['date'])]\n",
    "df_full = df_full.sort_values(['date','tic'])\n",
    "df_full = df_full.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe4ccb1a-fe6a-41e1-95fe-8a3363465c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 105900 entries, 0 to 156899\n",
      "Data columns (total 17 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   date          105900 non-null  object \n",
      " 1   tic           105900 non-null  object \n",
      " 2   open          105900 non-null  float64\n",
      " 3   high          105900 non-null  float64\n",
      " 4   low           105900 non-null  float64\n",
      " 5   close         105900 non-null  float64\n",
      " 6   volume        105900 non-null  float64\n",
      " 7   day           105900 non-null  float64\n",
      " 8   macd          105900 non-null  float64\n",
      " 9   boll_ub       105900 non-null  float64\n",
      " 10  boll_lb       105900 non-null  float64\n",
      " 11  rsi_30        105900 non-null  float64\n",
      " 12  cci_30        105900 non-null  float64\n",
      " 13  dx_30         105900 non-null  float64\n",
      " 14  close_30_sma  105900 non-null  float64\n",
      " 15  close_60_sma  105900 non-null  float64\n",
      " 16  turbulence    105900 non-null  float64\n",
      "dtypes: float64(15), object(2)\n",
      "memory usage: 14.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f2577b7-eabd-4f2c-a467-cb5584c94aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05041644-b750-457e-81c5-29c56fc1d53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>ASIANPAINT.BO</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>88.550003</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>19140.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.06826</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>AXISBANK.BO</td>\n",
       "      <td>105.800003</td>\n",
       "      <td>109.599998</td>\n",
       "      <td>103.459999</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>4536215.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.06826</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>BAJAJ-AUTO.BO</td>\n",
       "      <td>206.050003</td>\n",
       "      <td>210.500000</td>\n",
       "      <td>196.500000</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>52648.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.06826</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>BAJAJFINSV.BO</td>\n",
       "      <td>15.140000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>14.975000</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>136590.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.06826</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>BAJFINANCE.BO</td>\n",
       "      <td>6.660000</td>\n",
       "      <td>6.970000</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>274220.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.06826</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date            tic        open        high         low       close  \\\n",
       "0  2009-01-02  ASIANPAINT.BO   90.750000   90.750000   88.550003   48.861801   \n",
       "1  2009-01-02    AXISBANK.BO  105.800003  109.599998  103.459999   71.914917   \n",
       "2  2009-01-02  BAJAJ-AUTO.BO  206.050003  210.500000  196.500000  158.413025   \n",
       "3  2009-01-02  BAJAJFINSV.BO   15.140000   15.800000   14.975000   13.401811   \n",
       "4  2009-01-02  BAJFINANCE.BO    6.660000    6.970000    6.350000    2.746401   \n",
       "\n",
       "      volume  day  macd    boll_ub   boll_lb  rsi_30     cci_30  dx_30  \\\n",
       "0    19140.0  4.0   0.0  50.523346  48.06826   100.0  66.666667  100.0   \n",
       "1  4536215.0  4.0   0.0  50.523346  48.06826   100.0  66.666667  100.0   \n",
       "2    52648.0  4.0   0.0  50.523346  48.06826   100.0  66.666667  100.0   \n",
       "3   136590.0  4.0   0.0  50.523346  48.06826   100.0  66.666667  100.0   \n",
       "4   274220.0  4.0   0.0  50.523346  48.06826   100.0  66.666667  100.0   \n",
       "\n",
       "   close_30_sma  close_60_sma  turbulence  \n",
       "0     48.861801     48.861801         0.0  \n",
       "1     71.914917     71.914917         0.0  \n",
       "2    158.413025    158.413025         0.0  \n",
       "3     13.401811     13.401811         0.0  \n",
       "4      2.746401      2.746401         0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "067ab770-a250-4ae6-9192-79a47e011ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105900, 17)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d8ef628-f80f-4d6d-9a2f-b626ad01064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# from stable_baselines3.common.logger import Logger, KVWriter, CSVOutputFormat\n",
    "\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        stock_dim: int,\n",
    "        hmax: int,\n",
    "        initial_amount: int,\n",
    "        num_stock_shares: list[int],\n",
    "        buy_cost_pct: list[float],\n",
    "        sell_cost_pct: list[float],\n",
    "        reward_scaling: float,\n",
    "        state_space: int,\n",
    "        action_space: int,\n",
    "        tech_indicator_list: list[str],\n",
    "        turbulence_threshold=None,\n",
    "        risk_indicator_col=\"turbulence\",\n",
    "        make_plots: bool = False,\n",
    "        print_verbosity=10,\n",
    "        day=0,\n",
    "        initial=True,\n",
    "        previous_state=[],\n",
    "        model_name=\"\",\n",
    "        mode=\"\",\n",
    "        iteration=\"\",\n",
    "    ):\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "        self.stock_dim = stock_dim\n",
    "        self.hmax = hmax\n",
    "        self.num_stock_shares = num_stock_shares\n",
    "        self.initial_amount = initial_amount  # get the initial cash\n",
    "        self.buy_cost_pct = buy_cost_pct\n",
    "        self.sell_cost_pct = sell_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.action_space,))\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(self.state_space,)\n",
    "        )\n",
    "        self.data = self.df.loc[self.day, :]\n",
    "        self.terminal = False\n",
    "        self.make_plots = make_plots\n",
    "        self.print_verbosity = print_verbosity\n",
    "        self.turbulence_threshold = turbulence_threshold\n",
    "        self.risk_indicator_col = risk_indicator_col\n",
    "        self.initial = initial\n",
    "        self.previous_state = previous_state\n",
    "        self.model_name = model_name\n",
    "        self.mode = mode\n",
    "        self.iteration = iteration\n",
    "        # initalize state\n",
    "        self.state = self._initiate_state()\n",
    "\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.episode = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [\n",
    "            self.initial_amount\n",
    "            + np.sum(\n",
    "                np.array(self.num_stock_shares)\n",
    "                * np.array(self.state[1 : 1 + self.stock_dim])\n",
    "            )\n",
    "        ]  # the initial total asset is calculated by cash + sum (num_share_stock_i * price_stock_i)\n",
    "        self.rewards_memory = []\n",
    "        self.actions_memory = []\n",
    "        self.state_memory = (\n",
    "            []\n",
    "        )  # we need sometimes to preserve the state in the middle of trading process\n",
    "        self.date_memory = [self._get_date()]\n",
    "        #         self.logger = Logger('results',[CSVOutputFormat])\n",
    "        # self.reset()\n",
    "        self._seed()\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        def _do_sell_normal():\n",
    "            if (\n",
    "                self.state[index + 2 * self.stock_dim + 1] != True\n",
    "            ):  # check if the stock is able to sell, for simlicity we just add it in techical index\n",
    "                # if self.state[index + 1] > 0: # if we use price<0 to denote a stock is unable to trade in that day, the total asset calculation may be wrong for the price is unreasonable\n",
    "                # Sell only if the price is > 0 (no missing data in this particular date)\n",
    "                # perform sell action based on the sign of the action\n",
    "                if self.state[index + self.stock_dim + 1] > 0:\n",
    "                    # Sell only if current asset is > 0\n",
    "                    sell_num_shares = min(\n",
    "                        abs(action), self.state[index + self.stock_dim + 1]\n",
    "                    )\n",
    "                    sell_amount = (\n",
    "                        self.state[index + 1]\n",
    "                        * sell_num_shares\n",
    "                        * (1 - self.sell_cost_pct[index])\n",
    "                    )\n",
    "                    # update balance\n",
    "                    self.state[0] += sell_amount\n",
    "\n",
    "                    self.state[index + self.stock_dim + 1] -= sell_num_shares\n",
    "                    self.cost += (\n",
    "                        self.state[index + 1]\n",
    "                        * sell_num_shares\n",
    "                        * self.sell_cost_pct[index]\n",
    "                    )\n",
    "                    self.trades += 1\n",
    "                else:\n",
    "                    sell_num_shares = 0\n",
    "            else:\n",
    "                sell_num_shares = 0\n",
    "\n",
    "            return sell_num_shares\n",
    "\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.turbulence_threshold is not None:\n",
    "            if self.turbulence >= self.turbulence_threshold:\n",
    "                if self.state[index + 1] > 0:\n",
    "                    # Sell only if the price is > 0 (no missing data in this particular date)\n",
    "                    # if turbulence goes over threshold, just clear out all positions\n",
    "                    if self.state[index + self.stock_dim + 1] > 0:\n",
    "                        # Sell only if current asset is > 0\n",
    "                        sell_num_shares = self.state[index + self.stock_dim + 1]\n",
    "                        sell_amount = (\n",
    "                            self.state[index + 1]\n",
    "                            * sell_num_shares\n",
    "                            * (1 - self.sell_cost_pct[index])\n",
    "                        )\n",
    "                        # update balance\n",
    "                        self.state[0] += sell_amount\n",
    "                        self.state[index + self.stock_dim + 1] = 0\n",
    "                        self.cost += (\n",
    "                            self.state[index + 1]\n",
    "                            * sell_num_shares\n",
    "                            * self.sell_cost_pct[index]\n",
    "                        )\n",
    "                        self.trades += 1\n",
    "                    else:\n",
    "                        sell_num_shares = 0\n",
    "                else:\n",
    "                    sell_num_shares = 0\n",
    "            else:\n",
    "                sell_num_shares = _do_sell_normal()\n",
    "        else:\n",
    "            sell_num_shares = _do_sell_normal()\n",
    "\n",
    "        return sell_num_shares\n",
    "\n",
    "    def _buy_stock(self, index, action):\n",
    "        def _do_buy():\n",
    "            if (\n",
    "                self.state[index + 2 * self.stock_dim + 1] != True\n",
    "            ):  # check if the stock is able to buy\n",
    "                # if self.state[index + 1] >0:\n",
    "                # Buy only if the price is > 0 (no missing data in this particular date)\n",
    "                available_amount = self.state[0] // (\n",
    "                    self.state[index + 1] * (1 + self.buy_cost_pct[index])\n",
    "                )  # when buying stocks, we should consider the cost of trading when calculating available_amount, or we may be have cash<0\n",
    "                # print('available_amount:{}'.format(available_amount))\n",
    "\n",
    "                # update balance\n",
    "                buy_num_shares = min(available_amount, action)\n",
    "                buy_amount = (\n",
    "                    self.state[index + 1]\n",
    "                    * buy_num_shares\n",
    "                    * (1 + self.buy_cost_pct[index])\n",
    "                )\n",
    "                self.state[0] -= buy_amount\n",
    "\n",
    "                self.state[index + self.stock_dim + 1] += buy_num_shares\n",
    "\n",
    "                self.cost += (\n",
    "                    self.state[index + 1] * buy_num_shares * self.buy_cost_pct[index]\n",
    "                )\n",
    "                self.trades += 1\n",
    "            else:\n",
    "                buy_num_shares = 0\n",
    "\n",
    "            return buy_num_shares\n",
    "\n",
    "        # perform buy action based on the sign of the action\n",
    "        if self.turbulence_threshold is None:\n",
    "            buy_num_shares = _do_buy()\n",
    "        else:\n",
    "            if self.turbulence < self.turbulence_threshold:\n",
    "                buy_num_shares = _do_buy()\n",
    "            else:\n",
    "                buy_num_shares = 0\n",
    "                pass\n",
    "\n",
    "        return buy_num_shares\n",
    "\n",
    "    def _make_plot(self):\n",
    "        plt.plot(self.asset_memory, \"r\")\n",
    "        plt.savefig(f\"results/account_value_trade_{self.episode}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.terminal = self.day >= len(self.df.index.unique()) - 1\n",
    "        if self.terminal:\n",
    "            # print(f\"Episode: {self.episode}\")\n",
    "            if self.make_plots:\n",
    "                self._make_plot()\n",
    "            end_total_asset = self.state[0] + sum(\n",
    "                np.array(self.state[1 : (self.stock_dim + 1)])\n",
    "                * np.array(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
    "            )\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            tot_reward = (\n",
    "                self.state[0]\n",
    "                + sum(\n",
    "                    np.array(self.state[1 : (self.stock_dim + 1)])\n",
    "                    * np.array(\n",
    "                        self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)]\n",
    "                    )\n",
    "                )\n",
    "                - self.asset_memory[0]\n",
    "            )  # initial_amount is only cash part of our initial asset\n",
    "            df_total_value.columns = [\"account_value\"]\n",
    "            df_total_value[\"date\"] = self.date_memory\n",
    "            df_total_value[\"daily_return\"] = df_total_value[\"account_value\"].pct_change(\n",
    "                1\n",
    "            )\n",
    "            if df_total_value[\"daily_return\"].std() != 0:\n",
    "                sharpe = (\n",
    "                    (252**0.5)\n",
    "                    * df_total_value[\"daily_return\"].mean()\n",
    "                    / df_total_value[\"daily_return\"].std()\n",
    "                )\n",
    "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            df_rewards.columns = [\"account_rewards\"]\n",
    "            df_rewards[\"date\"] = self.date_memory[:-1]\n",
    "            if self.episode % self.print_verbosity == 0:\n",
    "                print(f\"day: {self.day}, episode: {self.episode}\")\n",
    "                print(f\"begin_total_asset: {self.asset_memory[0]:0.2f}\")\n",
    "                print(f\"end_total_asset: {end_total_asset:0.2f}\")\n",
    "                print(f\"total_reward: {tot_reward:0.2f}\")\n",
    "                print(f\"total_cost: {self.cost:0.2f}\")\n",
    "                print(f\"total_trades: {self.trades}\")\n",
    "                if df_total_value[\"daily_return\"].std() != 0:\n",
    "                    print(f\"Sharpe: {sharpe:0.3f}\")\n",
    "                print(\"=================================\")\n",
    "\n",
    "            if (self.model_name != \"\") and (self.mode != \"\"):\n",
    "                df_actions = self.save_action_memory()\n",
    "                df_actions.to_csv(\n",
    "                    \"results/actions_{}_{}_{}.csv\".format(\n",
    "                        self.mode, self.model_name, self.iteration\n",
    "                    )\n",
    "                )\n",
    "                df_total_value.to_csv(\n",
    "                    \"results/account_value_{}_{}_{}.csv\".format(\n",
    "                        self.mode, self.model_name, self.iteration\n",
    "                    ),\n",
    "                    index=False,\n",
    "                )\n",
    "                df_rewards.to_csv(\n",
    "                    \"results/account_rewards_{}_{}_{}.csv\".format(\n",
    "                        self.mode, self.model_name, self.iteration\n",
    "                    ),\n",
    "                    index=False,\n",
    "                )\n",
    "                plt.plot(self.asset_memory, \"r\")\n",
    "                plt.savefig(\n",
    "                    \"results/account_value_{}_{}_{}.png\".format(\n",
    "                        self.mode, self.model_name, self.iteration\n",
    "                    )\n",
    "                )\n",
    "                plt.close()\n",
    "\n",
    "            # Add outputs to logger interface\n",
    "            # logger.record(\"environment/portfolio_value\", end_total_asset)\n",
    "            # logger.record(\"environment/total_reward\", tot_reward)\n",
    "            # logger.record(\"environment/total_reward_pct\", (tot_reward / (end_total_asset - tot_reward)) * 100)\n",
    "            # logger.record(\"environment/total_cost\", self.cost)\n",
    "            # logger.record(\"environment/total_trades\", self.trades)\n",
    "\n",
    "            return self.state, self.reward, self.terminal, False, {}\n",
    "\n",
    "        else:\n",
    "            actions = actions * self.hmax  # actions initially is scaled between 0 to 1\n",
    "            actions = actions.astype(\n",
    "                int\n",
    "            )  # convert into integer because we can't by fraction of shares\n",
    "            if self.turbulence_threshold is not None:\n",
    "                if self.turbulence >= self.turbulence_threshold:\n",
    "                    actions = np.array([-self.hmax] * self.stock_dim)\n",
    "            begin_total_asset = self.state[0] + sum(\n",
    "                np.array(self.state[1 : (self.stock_dim + 1)])\n",
    "                * np.array(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
    "            )\n",
    "            # print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "\n",
    "            argsort_actions = np.argsort(actions)\n",
    "            sell_index = argsort_actions[: np.where(actions < 0)[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][: np.where(actions > 0)[0].shape[0]]\n",
    "\n",
    "            for index in sell_index:\n",
    "                # print(f\"Num shares before: {self.state[index+self.stock_dim+1]}\")\n",
    "                # print(f'take sell action before : {actions[index]}')\n",
    "                actions[index] = self._sell_stock(index, actions[index]) * (-1)\n",
    "                # print(f'take sell action after : {actions[index]}')\n",
    "                # print(f\"Num shares after: {self.state[index+self.stock_dim+1]}\")\n",
    "\n",
    "            for index in buy_index:\n",
    "                # print('take buy action: {}'.format(actions[index]))\n",
    "                actions[index] = self._buy_stock(index, actions[index])\n",
    "\n",
    "            self.actions_memory.append(actions)\n",
    "\n",
    "            # state: s -> s+1\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day, :]\n",
    "            if self.turbulence_threshold is not None:\n",
    "                if len(self.df.tic.unique()) == 1:\n",
    "                    self.turbulence = self.data[self.risk_indicator_col]\n",
    "                elif len(self.df.tic.unique()) > 1:\n",
    "                    self.turbulence = self.data[self.risk_indicator_col].values[0]\n",
    "            self.state = self._update_state()\n",
    "\n",
    "            end_total_asset = self.state[0] + sum(\n",
    "                np.array(self.state[1 : (self.stock_dim + 1)])\n",
    "                * np.array(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
    "            )\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            self.date_memory.append(self._get_date())\n",
    "            self.reward = end_total_asset - begin_total_asset\n",
    "            self.rewards_memory.append(self.reward)\n",
    "            self.reward = self.reward * self.reward_scaling\n",
    "            self.state_memory.append(\n",
    "                self.state\n",
    "            )  # add current state in state_recorder for each step\n",
    "\n",
    "        return self.state, self.reward, self.terminal, False, {}\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed=None,\n",
    "        options=None,\n",
    "    ):\n",
    "        # initiate state\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day, :]\n",
    "        self.state = self._initiate_state()\n",
    "\n",
    "        if self.initial:\n",
    "            self.asset_memory = [\n",
    "                self.initial_amount\n",
    "                + np.sum(\n",
    "                    np.array(self.num_stock_shares)\n",
    "                    * np.array(self.state[1 : 1 + self.stock_dim])\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            previous_total_asset = self.previous_state[0] + sum(\n",
    "                np.array(self.state[1 : (self.stock_dim + 1)])\n",
    "                * np.array(\n",
    "                    self.previous_state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)]\n",
    "                )\n",
    "            )\n",
    "            self.asset_memory = [previous_total_asset]\n",
    "\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False\n",
    "        # self.iteration=self.iteration\n",
    "        self.rewards_memory = []\n",
    "        self.actions_memory = []\n",
    "        self.date_memory = [self._get_date()]\n",
    "\n",
    "        self.episode += 1\n",
    "\n",
    "        return self.state, {}\n",
    "\n",
    "    def render(self, mode=\"human\", close=False):\n",
    "        return self.state\n",
    "\n",
    "    def _initiate_state(self):\n",
    "        if self.initial:\n",
    "            # For Initial State\n",
    "            if len(self.df.tic.unique()) > 1:\n",
    "                # for multiple stock\n",
    "                state = (\n",
    "                    [self.initial_amount]\n",
    "                    + self.data.close.values.tolist()\n",
    "                    + self.num_stock_shares\n",
    "                    + sum(\n",
    "                        (\n",
    "                            self.data[tech].values.tolist()\n",
    "                            for tech in self.tech_indicator_list\n",
    "                        ),\n",
    "                        [],\n",
    "                    )\n",
    "                )  # append initial stocks_share to initial state, instead of all zero\n",
    "            else:\n",
    "                # for single stock\n",
    "                state = (\n",
    "                    [self.initial_amount]\n",
    "                    + [self.data.close]\n",
    "                    + [0] * self.stock_dim\n",
    "                    + sum(([self.data[tech]] for tech in self.tech_indicator_list), [])\n",
    "                )\n",
    "        else:\n",
    "            # Using Previous State\n",
    "            if len(self.df.tic.unique()) > 1:\n",
    "                # for multiple stock\n",
    "                state = (\n",
    "                    [self.previous_state[0]]\n",
    "                    + self.data.close.values.tolist()\n",
    "                    + self.previous_state[\n",
    "                        (self.stock_dim + 1) : (self.stock_dim * 2 + 1)\n",
    "                    ]\n",
    "                    + sum(\n",
    "                        (\n",
    "                            self.data[tech].values.tolist()\n",
    "                            for tech in self.tech_indicator_list\n",
    "                        ),\n",
    "                        [],\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # for single stock\n",
    "                state = (\n",
    "                    [self.previous_state[0]]\n",
    "                    + [self.data.close]\n",
    "                    + self.previous_state[\n",
    "                        (self.stock_dim + 1) : (self.stock_dim * 2 + 1)\n",
    "                    ]\n",
    "                    + sum(([self.data[tech]] for tech in self.tech_indicator_list), [])\n",
    "                )\n",
    "        return state\n",
    "\n",
    "    def _update_state(self):\n",
    "        if len(self.df.tic.unique()) > 1:\n",
    "            # for multiple stock\n",
    "            state = (\n",
    "                [self.state[0]]\n",
    "                + self.data.close.values.tolist()\n",
    "                + list(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
    "                + sum(\n",
    "                    (\n",
    "                        self.data[tech].values.tolist()\n",
    "                        for tech in self.tech_indicator_list\n",
    "                    ),\n",
    "                    [],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # for single stock\n",
    "            state = (\n",
    "                [self.state[0]]\n",
    "                + [self.data.close]\n",
    "                + list(self.state[(self.stock_dim + 1) : (self.stock_dim * 2 + 1)])\n",
    "                + sum(([self.data[tech]] for tech in self.tech_indicator_list), [])\n",
    "            )\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _get_date(self):\n",
    "        if len(self.df.tic.unique()) > 1:\n",
    "            date = self.data.date.unique()[0]\n",
    "        else:\n",
    "            date = self.data.date\n",
    "        return date\n",
    "\n",
    "    # add save_state_memory to preserve state in the trading process\n",
    "    def save_state_memory(self):\n",
    "        if len(self.df.tic.unique()) > 1:\n",
    "            # date and close price length must match actions length\n",
    "            date_list = self.date_memory[:-1]\n",
    "            df_date = pd.DataFrame(date_list)\n",
    "            df_date.columns = [\"date\"]\n",
    "\n",
    "            state_list = self.state_memory\n",
    "            df_states = pd.DataFrame(\n",
    "                state_list,\n",
    "                columns=[\n",
    "                    \"cash\",\n",
    "                    \"Bitcoin_price\",\n",
    "                    \"Gold_price\",\n",
    "                    \"Bitcoin_num\",\n",
    "                    \"Gold_num\",\n",
    "                    \"Bitcoin_Disable\",\n",
    "                    \"Gold_Disable\",\n",
    "                ],\n",
    "            )\n",
    "            df_states.index = df_date.date\n",
    "            # df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "        else:\n",
    "            date_list = self.date_memory[:-1]\n",
    "            state_list = self.state_memory\n",
    "            df_states = pd.DataFrame({\"date\": date_list, \"states\": state_list})\n",
    "        # print(df_states)\n",
    "        return df_states\n",
    "\n",
    "    def save_asset_memory(self):\n",
    "        date_list = self.date_memory\n",
    "        asset_list = self.asset_memory\n",
    "        # print(len(date_list))\n",
    "        # print(len(asset_list))\n",
    "        df_account_value = pd.DataFrame(\n",
    "            {\"date\": date_list, \"account_value\": asset_list}\n",
    "        )\n",
    "        return df_account_value\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        if len(self.df.tic.unique()) > 1:\n",
    "            # date and close price length must match actions length\n",
    "            date_list = self.date_memory[:-1]\n",
    "            df_date = pd.DataFrame(date_list)\n",
    "            df_date.columns = [\"date\"]\n",
    "\n",
    "            action_list = self.actions_memory\n",
    "            df_actions = pd.DataFrame(action_list)\n",
    "            df_actions.columns = self.data.tic.values\n",
    "            df_actions.index = df_date.date\n",
    "            # df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "        else:\n",
    "            date_list = self.date_memory[:-1]\n",
    "            action_list = self.actions_memory\n",
    "            df_actions = pd.DataFrame({\"date\": date_list, \"actions\": action_list})\n",
    "        return df_actions\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_sb_env(self):\n",
    "        e = DummyVecEnv([lambda: self])\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbbe291-9e9d-4028-8ef4-897e83cf2318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28d66621-48cf-40ea-b98d-5b262d41fa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77550\n",
      "21120\n"
     ]
    }
   ],
   "source": [
    "train = data_split(df, TRAIN_START_DATE,TRAIN_END_DATE)\n",
    "trade = data_split(df, TRADE_START_DATE,TRADE_END_DATE)\n",
    "print(len(train))\n",
    "print(len(trade))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76538de6-2554-4a7f-b69b-7f1303363465",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_data.csv')\n",
    "trade.to_csv('trade_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6cce40-87e6-410c-ac31-d555f17adaa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db88fde3-3088-471d-a275-f30f1c22f7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 30, State Space: 301\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "490ac2e0-422b-4f0d-9d59-c7a13738bb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>ASIANPAINT.BO</td>\n",
       "      <td>173.800003</td>\n",
       "      <td>179.990005</td>\n",
       "      <td>173.800003</td>\n",
       "      <td>113.311302</td>\n",
       "      <td>26700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.051069</td>\n",
       "      <td>115.089549</td>\n",
       "      <td>104.640905</td>\n",
       "      <td>66.436248</td>\n",
       "      <td>113.218646</td>\n",
       "      <td>24.458130</td>\n",
       "      <td>108.824595</td>\n",
       "      <td>104.144379</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>AXISBANK.BO</td>\n",
       "      <td>199.800003</td>\n",
       "      <td>199.800003</td>\n",
       "      <td>197.600006</td>\n",
       "      <td>142.226456</td>\n",
       "      <td>658270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.338474</td>\n",
       "      <td>151.029051</td>\n",
       "      <td>132.164238</td>\n",
       "      <td>52.578761</td>\n",
       "      <td>-1.919545</td>\n",
       "      <td>2.331440</td>\n",
       "      <td>142.339130</td>\n",
       "      <td>140.995590</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>BAJAJ-AUTO.BO</td>\n",
       "      <td>885.000000</td>\n",
       "      <td>886.000000</td>\n",
       "      <td>865.000000</td>\n",
       "      <td>687.819336</td>\n",
       "      <td>71150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.282086</td>\n",
       "      <td>707.370571</td>\n",
       "      <td>641.819761</td>\n",
       "      <td>61.119288</td>\n",
       "      <td>90.337046</td>\n",
       "      <td>15.068862</td>\n",
       "      <td>655.815855</td>\n",
       "      <td>622.730791</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>BAJAJFINSV.BO</td>\n",
       "      <td>34.900002</td>\n",
       "      <td>36.080002</td>\n",
       "      <td>34.799999</td>\n",
       "      <td>31.331820</td>\n",
       "      <td>1119010.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.473058</td>\n",
       "      <td>30.449640</td>\n",
       "      <td>27.363217</td>\n",
       "      <td>60.736230</td>\n",
       "      <td>321.150339</td>\n",
       "      <td>46.461213</td>\n",
       "      <td>28.700716</td>\n",
       "      <td>28.163338</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>BAJFINANCE.BO</td>\n",
       "      <td>33.270000</td>\n",
       "      <td>34.389999</td>\n",
       "      <td>33.270000</td>\n",
       "      <td>16.682077</td>\n",
       "      <td>221680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.568160</td>\n",
       "      <td>16.397749</td>\n",
       "      <td>14.096924</td>\n",
       "      <td>68.269383</td>\n",
       "      <td>212.436642</td>\n",
       "      <td>27.581496</td>\n",
       "      <td>14.887369</td>\n",
       "      <td>14.188843</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date            tic        open        high         low       close  \\\n",
       "0  2010-01-04  ASIANPAINT.BO  173.800003  179.990005  173.800003  113.311302   \n",
       "0  2010-01-04    AXISBANK.BO  199.800003  199.800003  197.600006  142.226456   \n",
       "0  2010-01-04  BAJAJ-AUTO.BO  885.000000  886.000000  865.000000  687.819336   \n",
       "0  2010-01-04  BAJAJFINSV.BO   34.900002   36.080002   34.799999   31.331820   \n",
       "0  2010-01-04  BAJFINANCE.BO   33.270000   34.389999   33.270000   16.682077   \n",
       "\n",
       "      volume  day       macd     boll_ub     boll_lb     rsi_30      cci_30  \\\n",
       "0    26700.0  0.0   2.051069  115.089549  104.640905  66.436248  113.218646   \n",
       "0   658270.0  0.0  -0.338474  151.029051  132.164238  52.578761   -1.919545   \n",
       "0    71150.0  0.0  16.282086  707.370571  641.819761  61.119288   90.337046   \n",
       "0  1119010.0  0.0   0.473058   30.449640   27.363217  60.736230  321.150339   \n",
       "0   221680.0  0.0   0.568160   16.397749   14.096924  68.269383  212.436642   \n",
       "\n",
       "       dx_30  close_30_sma  close_60_sma  turbulence  \n",
       "0  24.458130    108.824595    104.144379         0.0  \n",
       "0   2.331440    142.339130    140.995590         0.0  \n",
       "0  15.068862    655.815855    622.730791         0.0  \n",
       "0  46.461213     28.700716     28.163338         0.0  \n",
       "0  27.581496     14.887369     14.188843         0.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b19bfb4-2266-4356-a0a8-c8ef45fadb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"buy_cost_pct\": 0.001, \n",
    "    \"sell_cost_pct\": 0.001, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\":5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33775ce8-4afc-41e3-b742-f3aff0dadbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl.agents.stablebaselines3.models import DRLEnsembleAgent\n",
    "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
    "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
    "\n",
    "ensemble_agent = DRLEnsembleAgent(df=df,\n",
    "                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
    "                 val_test_period=(TRADE_START_DATE,TRADE_END_DATE),\n",
    "                 rebalance_window=rebalance_window, \n",
    "                 validation_window=validation_window, \n",
    "                 **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b3af798-f792-47af-87e9-b89db8fcffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\n",
    "                    'n_steps': 5,\n",
    "                    'ent_coef': 0.005,\n",
    "                    'learning_rate': 0.0007\n",
    "                    }\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "                    \"ent_coef\":0.01,\n",
    "                    \"n_steps\": 2048,\n",
    "                    \"learning_rate\": 0.00025,\n",
    "                    \"batch_size\": 128\n",
    "                    }\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
    "                      \"buffer_size\": 10_000,\n",
    "                      \"learning_rate\": 0.0005,\n",
    "                      \"batch_size\": 64\n",
    "                    }\n",
    "\n",
    "timesteps_dict = {'a2c' : 10_000, \n",
    "                 'ppo' : 10_000, \n",
    "                 'ddpg' : 10_000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "031defd6-1140-410b-b988-1edbdc7ab52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2020-07-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to tensorboard_log/a2c\\a2c_126_2\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -11.7       |\n",
      "|    reward             | -0.83918446 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 2.71        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 69       |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -87.1    |\n",
      "|    reward             | 1.54583  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 5.85     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 70       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -119     |\n",
      "|    reward             | 2.296505 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 9.59     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -226      |\n",
      "|    reward             | 151.23096 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 67.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 406        |\n",
      "|    reward             | -5.8316936 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 88.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -143       |\n",
      "|    reward             | -1.4232113 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 12         |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 48          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -30.6       |\n",
      "|    reward             | -0.04382071 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 9.04        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 55         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 55.7       |\n",
      "|    reward             | -2.7449331 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.64       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 62         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -108       |\n",
      "|    reward             | 0.23235384 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 7.98       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 68         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 411        |\n",
      "|    reward             | -2.7049763 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 124        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 75          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | 178         |\n",
      "|    reward             | -0.46307257 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 18.7        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 82        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 388       |\n",
      "|    reward             | 1.2824464 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 108       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 89         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 51.9       |\n",
      "|    reward             | 0.13419266 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 8.91       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 95         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 0.534      |\n",
      "|    reward             | -1.1036603 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.06       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 102       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -204      |\n",
      "|    reward             | 1.7214146 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 25.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 109       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | -0.00271  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -9.84     |\n",
      "|    reward             | 3.7555907 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.74      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 116       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 35        |\n",
      "|    reward             | 0.6198071 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.13      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 122      |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -99.5    |\n",
      "|    reward             | 5.203245 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 23.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 129      |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 306      |\n",
      "|    reward             | 4.814341 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 57.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 136      |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -185     |\n",
      "|    reward             | 7.650227 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 24.5     |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2020-07-02 to  2020-09-29\n",
      "A2C Sharpe Ratio:  0.2538570415609139\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_126_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 99          |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 20          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.81946146 |\n",
      "------------------------------------\n",
      "day: 2585, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1898489.84\n",
      "total_reward: 898489.84\n",
      "total_cost: 1802517.36\n",
      "total_trades: 63654\n",
      "Sharpe: 0.316\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018440237 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.00304    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 118         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    reward               | 1.3720264   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 73.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018648814 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.00429     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 172         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    reward               | -0.59584844 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 67.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 97         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 84         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02522846 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | 0.051      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 14.4       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    reward               | 1.5176284  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 62.9       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 97         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 105        |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.0218837  |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.8      |\n",
      "|    explained_variance   | 0.0528     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 19.4       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    reward               | -1.8525114 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 79.3       |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2020-07-02 to  2020-09-29\n",
      "PPO Sharpe Ratio:  0.2538362119059472\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_126_1\n",
      "day: 2585, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4782762.20\n",
      "total_reward: 3782762.20\n",
      "total_cost: 1055.57\n",
      "total_trades: 41308\n",
      "Sharpe: 0.327\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 48         |\n",
      "|    time_elapsed    | 214        |\n",
      "|    total_timesteps | 10344      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -15.8      |\n",
      "|    critic_loss     | 71         |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 7758       |\n",
      "|    reward          | -1.3788683 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2020-07-02 to  2020-09-29\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-09-29\n",
      "======Trading from:  2020-09-29 to  2020-12-30\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2020-09-29\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_189_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.5       |\n",
      "|    explained_variance | -3.58e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -19.6       |\n",
      "|    reward             | -0.08418834 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 0.811       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 47.6      |\n",
      "|    reward             | 4.030175  |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.96      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 300         |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 1500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | -0.0405     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 299         |\n",
      "|    policy_loss        | 233         |\n",
      "|    reward             | -0.26598313 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 30.3        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -279     |\n",
      "|    reward             | 190.1621 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 48.5     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -49.8      |\n",
      "|    reward             | -18.125786 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 26.9       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 41          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -42.4       |\n",
      "|    reward             | -0.26914135 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.41        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 48         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 127        |\n",
      "|    reward             | -1.0741067 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 10.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 55         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 7.08       |\n",
      "|    reward             | -2.2180696 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 4          |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -20       |\n",
      "|    reward             | 4.0596046 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 6.83      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 69       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -651     |\n",
      "|    reward             | 5.160483 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 375      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 76        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -14.6     |\n",
      "|    reward             | 3.0527139 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.59      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 83         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -25.3      |\n",
      "|    reward             | 0.58227944 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.61       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 90        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -71.9     |\n",
      "|    reward             | -1.150264 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 11.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 97        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -375      |\n",
      "|    reward             | -4.008726 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 114       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 103        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -214       |\n",
      "|    reward             | -11.627975 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 57.4       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 72          |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 110         |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0.242       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -9.63       |\n",
      "|    reward             | -0.38184026 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.0665      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 117        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -25.9      |\n",
      "|    reward             | -1.8020637 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.41       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 89.3      |\n",
      "|    reward             | 1.9660234 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.14      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 131        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 211        |\n",
      "|    reward             | 0.14832348 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 29.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 138        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 455        |\n",
      "|    reward             | -2.3344245 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 120        |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2020-09-29 to  2020-12-30\n",
      "A2C Sharpe Ratio:  0.7815263723994448\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_189_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 98         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 20         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.56435114 |\n",
      "-----------------------------------\n",
      "day: 2648, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2259120.39\n",
      "total_reward: 1259120.39\n",
      "total_cost: 1772293.03\n",
      "total_trades: 64570\n",
      "Sharpe: 0.494\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016540788 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.00823    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.5        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    reward               | -1.5054021  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 46.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020881675 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.00508    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    reward               | 1.0760466   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 79.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019875383 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.0196      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.8        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    reward               | 0.7819058   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 94.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018832697 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0282      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 35.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    reward               | -2.9861093  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 222         |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-09-29 to  2020-12-30\n",
      "PPO Sharpe Ratio:  0.5730181402430891\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_189_1\n",
      "day: 2648, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5297865.26\n",
      "total_reward: 4297865.26\n",
      "total_cost: 1268.43\n",
      "total_trades: 58157\n",
      "Sharpe: 0.532\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 50        |\n",
      "|    time_elapsed    | 208       |\n",
      "|    total_timesteps | 10596     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 50.3      |\n",
      "|    critic_loss     | 236       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 7947      |\n",
      "|    reward          | 10.017355 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2020-09-29 to  2020-12-30\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-12-30\n",
      "======Trading from:  2020-12-30 to  2021-04-01\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2020-12-30\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_252_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 70          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -113        |\n",
      "|    reward             | -0.13361724 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 11.4        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -11.2     |\n",
      "|    reward             | 1.1440425 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 8.2       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 341       |\n",
      "|    reward             | 7.511214  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 132       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -494      |\n",
      "|    reward             | 441.3647  |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 171       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 35         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 230        |\n",
      "|    reward             | -17.228497 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 63.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | -1.52      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 202        |\n",
      "|    reward             | -3.1344256 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 41.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 41.5      |\n",
      "|    reward             | -4.655045 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 8.25      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 56         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -1.44e+03  |\n",
      "|    reward             | -5.0266724 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.26e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 694       |\n",
      "|    reward             | 20.948254 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 727       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 70         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 3.56e+03   |\n",
      "|    reward             | 102.493484 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.39e+04   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 70          |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 77          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0.00267     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -42.1       |\n",
      "|    reward             | -0.18829606 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.48        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 70          |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 85          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | -0.102      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 138         |\n",
      "|    reward             | -0.14691146 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 16.1        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 91         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -60.1      |\n",
      "|    reward             | -1.2175494 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 9.26       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 70          |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 99          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 261         |\n",
      "|    reward             | 0.039611485 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 37.6        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 107        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -360       |\n",
      "|    reward             | 0.49942285 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 166        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 114       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0.0762    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -537      |\n",
      "|    reward             | 5.8006816 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 163       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 122         |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | -0.00447    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | -58.6       |\n",
      "|    reward             | -0.44551584 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 2.26        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 69       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 130      |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -35.3    |\n",
      "|    reward             | 3.913071 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.3      |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 69          |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 137         |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 34.2        |\n",
      "|    reward             | 0.035546005 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.64        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 144        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 471        |\n",
      "|    reward             | -3.5920641 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 137        |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2020-12-30 to  2021-04-01\n",
      "A2C Sharpe Ratio:  -0.052015170497686475\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_252_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 93        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 21        |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 1.5897871 |\n",
      "----------------------------------\n",
      "day: 2711, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2391291.83\n",
      "total_reward: 1391291.83\n",
      "total_cost: 1919910.60\n",
      "total_trades: 66458\n",
      "Sharpe: 0.383\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015347354 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.000755    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.4        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    reward               | 0.002496072 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 85          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016342686 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.0131      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    reward               | 0.6162321   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 82.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020495474 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.00829     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 30.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    reward               | 1.6820741   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 163         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 112         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020248022 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0286      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 105         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    reward               | 2.4232426   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 89.8        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-12-30 to  2021-04-01\n",
      "PPO Sharpe Ratio:  -0.03342550204704397\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_252_1\n",
      "day: 2711, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6622487.23\n",
      "total_reward: 5622487.23\n",
      "total_cost: 1035.51\n",
      "total_trades: 45993\n",
      "Sharpe: 0.468\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 49         |\n",
      "|    time_elapsed    | 217        |\n",
      "|    total_timesteps | 10848      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 74.6       |\n",
      "|    critic_loss     | 19.6       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 8136       |\n",
      "|    reward          | -0.7516366 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2020-12-30 to  2021-04-01\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-04-01\n",
      "======Trading from:  2021-04-01 to  2021-07-05\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2021-04-01\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_315_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -130      |\n",
      "|    reward             | 0.6967506 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 9.49      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -136      |\n",
      "|    reward             | 3.1264613 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 14.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 270       |\n",
      "|    reward             | 0.6033396 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 43.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -1.07     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -202      |\n",
      "|    reward             | 180.54482 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 31.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 426       |\n",
      "|    reward             | -12.15714 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 118       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | -0.0018    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -54.3      |\n",
      "|    reward             | -1.5920584 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 5.02       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 49          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | -0.431      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 86.5        |\n",
      "|    reward             | -0.20463969 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 9.03        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -48.2     |\n",
      "|    reward             | 1.2760962 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 5.4       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -283      |\n",
      "|    reward             | 4.2895465 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 66.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 70         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 156        |\n",
      "|    reward             | -2.2100208 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 31.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 76        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0.00751   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 1.13e+03  |\n",
      "|    reward             | -8.480458 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 919       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 71          |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 84          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 139         |\n",
      "|    reward             | -0.51676756 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 11.5        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 91         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -139       |\n",
      "|    reward             | -1.6664141 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 9.04       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 97         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -564       |\n",
      "|    reward             | -0.6658391 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 178        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 104       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -552      |\n",
      "|    reward             | 2.9148982 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 172       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 111       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 36        |\n",
      "|    reward             | 1.8636055 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.86      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 119       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 137       |\n",
      "|    reward             | 2.3819401 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 10.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 126       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 182       |\n",
      "|    reward             | 3.4172032 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 23.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 133       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 29.3      |\n",
      "|    reward             | 3.838476  |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 37.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 140        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 252        |\n",
      "|    reward             | -4.1428037 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 55.6       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-04-01 to  2021-07-05\n",
      "A2C Sharpe Ratio:  0.3009321997643706\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_315_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 93         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 21         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.75566965 |\n",
      "-----------------------------------\n",
      "day: 2774, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2355394.84\n",
      "total_reward: 1355394.84\n",
      "total_cost: 1946194.34\n",
      "total_trades: 67284\n",
      "Sharpe: 0.465\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017954482 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.00262    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.77        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    reward               | 0.9219199   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 58.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013328756 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0282      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    reward               | -0.8077882  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 75.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026988544 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0128      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 181         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    reward               | 0.6866429   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 249         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022290405 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0419      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 115         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    reward               | 1.7608347   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 109         |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-04-01 to  2021-07-05\n",
      "PPO Sharpe Ratio:  0.0991118205867301\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_315_1\n",
      "day: 2774, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6545800.07\n",
      "total_reward: 5545800.07\n",
      "total_cost: 3014.82\n",
      "total_trades: 44313\n",
      "Sharpe: 0.408\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 47         |\n",
      "|    time_elapsed    | 231        |\n",
      "|    total_timesteps | 11100      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 49.2       |\n",
      "|    critic_loss     | 65.7       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 8325       |\n",
      "|    reward          | -1.3931102 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2021-04-01 to  2021-07-05\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-07-05\n",
      "======Trading from:  2021-07-05 to  2021-10-05\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2021-07-05\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_378_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -50.8      |\n",
      "|    reward             | -0.9228248 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.75       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 63        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0.000134  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -70.2     |\n",
      "|    reward             | 1.5674679 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.83      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 63        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 86        |\n",
      "|    reward             | 1.5825938 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.71      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 63       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 182      |\n",
      "|    reward             | 40.63198 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 26.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -57.9     |\n",
      "|    reward             | -7.625594 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 14.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 64          |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 46          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 30          |\n",
      "|    reward             | -0.23046304 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.75        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 64          |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 54          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -7.27       |\n",
      "|    reward             | -0.25640532 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.52        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 61         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 6.91       |\n",
      "|    reward             | -1.0182143 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.4        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -21        |\n",
      "|    reward             | 0.71901095 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.85       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 76         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -48.9      |\n",
      "|    reward             | -5.2427454 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.03       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 65        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 83        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -47.9     |\n",
      "|    reward             | 2.3098576 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.68      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 90        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -17.1     |\n",
      "|    reward             | 1.4998355 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.24      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 97         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 250        |\n",
      "|    reward             | -1.9740245 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 46         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 104       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0.0108    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 255       |\n",
      "|    reward             | 3.4306617 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 55.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 112        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -16.7      |\n",
      "|    reward             | -7.2482038 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.277      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 66         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 120        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 41.9       |\n",
      "|    reward             | -1.9549063 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.23       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 130        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 132        |\n",
      "|    reward             | -1.3455527 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 11.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 139        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 2.13       |\n",
      "|    reward             | -1.7783697 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.4        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 63        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 148       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -473      |\n",
      "|    reward             | 1.1209714 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 142       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 63         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 158        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -225       |\n",
      "|    reward             | -2.8325818 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 29         |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-07-05 to  2021-10-05\n",
      "A2C Sharpe Ratio:  0.6756466646032288\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_378_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 70         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 29         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | -0.2997671 |\n",
      "-----------------------------------\n",
      "day: 2837, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3127310.46\n",
      "total_reward: 2127310.46\n",
      "total_cost: 2047578.14\n",
      "total_trades: 68950\n",
      "Sharpe: 0.539\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017242622 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.000928   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.1        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0217     |\n",
      "|    reward               | -0.77096045 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 88.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016045982 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.00235     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 57.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    reward               | -2.3203535  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 77          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 68          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017375771 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.00915     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 34.9        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    reward               | -13.281004  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 167         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 68         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 150        |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02106142 |\n",
      "|    clip_fraction        | 0.288      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.8      |\n",
      "|    explained_variance   | 0.02       |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 45         |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    reward               | 0.9942647  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 117        |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2021-07-05 to  2021-10-05\n",
      "PPO Sharpe Ratio:  0.34815054249481975\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_378_1\n",
      "day: 2837, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9286908.49\n",
      "total_reward: 8286908.49\n",
      "total_cost: 1170.04\n",
      "total_trades: 34006\n",
      "Sharpe: 0.328\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 52         |\n",
      "|    time_elapsed    | 216        |\n",
      "|    total_timesteps | 11352      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -7.82      |\n",
      "|    critic_loss     | 44.3       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 8514       |\n",
      "|    reward          | -1.0874333 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2021-07-05 to  2021-10-05\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-10-05\n",
      "======Trading from:  2021-10-05 to  2022-01-05\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2021-10-05\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_441_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 50       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -44.6    |\n",
      "|    reward             | 0.759119 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.71     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 50        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -132      |\n",
      "|    reward             | 2.2899146 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 13.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 50       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | -0.0231  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 214      |\n",
      "|    reward             | 4.230251 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 37.5     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 50        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -224      |\n",
      "|    reward             | 318.13892 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 41.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 50        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 24.3      |\n",
      "|    reward             | -7.034887 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 6.57      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 50         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 59         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -10.1      |\n",
      "|    reward             | 0.17312886 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 1.25       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 50         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -154       |\n",
      "|    reward             | 0.75333923 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 14.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 50         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 79         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 799        |\n",
      "|    reward             | -0.5942994 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 371        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 50       |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 89       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 146      |\n",
      "|    reward             | 9.822778 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 21.9     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 50         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 99         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 633        |\n",
      "|    reward             | -1.6995008 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 258        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 50         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 109        |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -209       |\n",
      "|    reward             | -0.1046227 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 33.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 50         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 119        |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -27.5      |\n",
      "|    reward             | -1.2728019 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 0.921      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 50         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 129        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 50.6       |\n",
      "|    reward             | -3.1680524 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 5.86       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 140       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 378       |\n",
      "|    reward             | 6.3831177 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 83.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 150       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -941      |\n",
      "|    reward             | 1.4690456 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 643       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 160       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 271       |\n",
      "|    reward             | 3.1553779 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 57.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 49         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 170        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 142        |\n",
      "|    reward             | -0.8377816 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 19.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 49          |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 180         |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | 110         |\n",
      "|    reward             | -0.99902564 |\n",
      "|    std                | 0.991       |\n",
      "|    value_loss         | 8.95        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 49         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 190        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 171        |\n",
      "|    reward             | -1.4094601 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 17.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 201       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 63.1      |\n",
      "|    reward             | 1.0942011 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 5.93      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2021-10-05 to  2022-01-05\n",
      "A2C Sharpe Ratio:  0.09923624635067949\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_441_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 68        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 29        |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 2.0699482 |\n",
      "----------------------------------\n",
      "day: 2900, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4238367.90\n",
      "total_reward: 3238367.90\n",
      "total_cost: 2301788.49\n",
      "total_trades: 70981\n",
      "Sharpe: 0.301\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 67          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016630484 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.00483    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.1        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    reward               | 1.7243068   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 91.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 66         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 92         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01586835 |\n",
      "|    clip_fraction        | 0.198      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | -0.00486   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 38.4       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    reward               | -0.4986353 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 136        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 66         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 123        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01709083 |\n",
      "|    clip_fraction        | 0.226      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | 0.0139     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 28.5       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    reward               | 1.9481202  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 147        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 66          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029369961 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0225      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.9        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    reward               | -2.1737993  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 89.6        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-10-05 to  2022-01-05\n",
      "PPO Sharpe Ratio:  -0.08887911532678078\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_441_1\n",
      "day: 2900, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6726563.26\n",
      "total_reward: 5726563.26\n",
      "total_cost: 1069.18\n",
      "total_trades: 37645\n",
      "Sharpe: 0.350\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 59        |\n",
      "|    time_elapsed    | 193       |\n",
      "|    total_timesteps | 11604     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -70.1     |\n",
      "|    critic_loss     | 71.7      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 8703      |\n",
      "|    reward          | 7.7817636 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2021-10-05 to  2022-01-05\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-01-05\n",
      "======Trading from:  2022-01-05 to  2022-04-07\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2022-01-05\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_504_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -136      |\n",
      "|    reward             | 1.2743549 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 12.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -122      |\n",
      "|    reward             | 1.9468663 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 10.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 865        |\n",
      "|    reward             | -5.1122627 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 484        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -469      |\n",
      "|    reward             | 11.420904 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 127       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 157       |\n",
      "|    reward             | -17.20538 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 39.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 48.1      |\n",
      "|    reward             | 0.2637639 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 1.3       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 72        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 304       |\n",
      "|    reward             | 4.4275246 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 50.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 82         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 165        |\n",
      "|    reward             | -1.3296072 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 16.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 92         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -48.5      |\n",
      "|    reward             | -1.6405084 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 1.58       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 102        |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 2.98e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 723        |\n",
      "|    reward             | -13.301813 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 312        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 113        |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 938        |\n",
      "|    reward             | -25.357397 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 936        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -147      |\n",
      "|    reward             | 1.0061332 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 17        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 133       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 62.9      |\n",
      "|    reward             | -0.917107 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.43      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 143       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 32.8      |\n",
      "|    reward             | 1.1979263 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.46      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 153        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 231        |\n",
      "|    reward             | -1.7635821 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 37.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 164       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -38.8     |\n",
      "|    reward             | 3.6524782 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 5.87      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 174        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 90.2       |\n",
      "|    reward             | -2.3966467 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 11.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 184       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 33.9      |\n",
      "|    reward             | 2.32034   |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.69      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 195        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -159       |\n",
      "|    reward             | -0.5695567 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 20.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 205       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -94.2     |\n",
      "|    reward             | 0.3941316 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 6.69      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-01-05 to  2022-04-07\n",
      "A2C Sharpe Ratio:  0.2516538790228549\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_504_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 67         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 30         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | -0.2098418 |\n",
      "-----------------------------------\n",
      "day: 2963, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2983066.05\n",
      "total_reward: 1983066.05\n",
      "total_cost: 2013691.14\n",
      "total_trades: 69710\n",
      "Sharpe: 0.393\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 66          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014520185 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.0111     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 69.3        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    reward               | 0.15186028  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 39.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 65         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 93         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01711749 |\n",
      "|    clip_fraction        | 0.219      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | 0.00193    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 16.7       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    reward               | -1.1571838 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 86.7       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 65         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 125        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02120282 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.8      |\n",
      "|    explained_variance   | 0.00201    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 263        |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    reward               | 1.1432272  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 210        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 65          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 157         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018148297 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0138      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.9        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    reward               | 1.4576564   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 57.3        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-01-05 to  2022-04-07\n",
      "PPO Sharpe Ratio:  0.2538381451262466\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_504_1\n",
      "day: 2963, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6443265.56\n",
      "total_reward: 5443265.56\n",
      "total_cost: 999.00\n",
      "total_trades: 56201\n",
      "Sharpe: 0.444\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 36        |\n",
      "|    time_elapsed    | 321       |\n",
      "|    total_timesteps | 11856     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -49.5     |\n",
      "|    critic_loss     | 106       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 8892      |\n",
      "|    reward          | 1.6575832 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2022-01-05 to  2022-04-07\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-04-07\n",
      "======Trading from:  2022-04-07 to  2022-07-08\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2022-04-07\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_567_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -50.7     |\n",
      "|    reward             | 3.1675808 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.77      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -26.3     |\n",
      "|    reward             | 1.8056931 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.02      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 280        |\n",
      "|    reward             | -0.9663746 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 44.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -0.332    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -710      |\n",
      "|    reward             | 196.61179 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 317       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 51         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 200        |\n",
      "|    reward             | -33.948013 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 129        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 48       |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 61       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | -0.00601 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 8.9e+03  |\n",
      "|    reward             | 48.3057  |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 2.3e+05  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 48       |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 72       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -342     |\n",
      "|    reward             | 4.197665 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 67.4     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 82         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -122       |\n",
      "|    reward             | -1.7295644 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 11         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 92         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 84.8       |\n",
      "|    reward             | -1.9617337 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 9.06       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0.00644   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 327       |\n",
      "|    reward             | 2.5366929 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 123       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -176      |\n",
      "|    reward             | 1.3483592 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 60        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -100      |\n",
      "|    reward             | -2.525568 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 26.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 134       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -41.8     |\n",
      "|    reward             | 2.6822622 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.37      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 144       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -18.5     |\n",
      "|    reward             | 1.3945137 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.429     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 155       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 75.8      |\n",
      "|    reward             | 1.1075706 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 23        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 165       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 378       |\n",
      "|    reward             | 1.6066734 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 94.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 175        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -207       |\n",
      "|    reward             | -5.3011217 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 25.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 186       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -158      |\n",
      "|    reward             | 4.5048127 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 32.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 196       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -14.2     |\n",
      "|    reward             | 2.1996515 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.607     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 207       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0.00227   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -23.1     |\n",
      "|    reward             | 3.3774953 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.66      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-04-07 to  2022-07-08\n",
      "A2C Sharpe Ratio:  0.2446361820422425\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_567_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 65          |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 31          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.40761527 |\n",
      "------------------------------------\n",
      "day: 3026, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2939709.57\n",
      "total_reward: 1939709.57\n",
      "total_cost: 2260968.69\n",
      "total_trades: 72532\n",
      "Sharpe: 0.330\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 63          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013891296 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.00231     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13          |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    reward               | -0.9498239  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 57.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 64          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01465505  |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.00693     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 28.7        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    reward               | -0.21817648 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 134         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 64         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 127        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02236784 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | 0.0204     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 150        |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    reward               | -0.6584315 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 158        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 64          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 159         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016802628 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0277      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 112         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    reward               | -0.55627286 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 79.8        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-04-07 to  2022-07-08\n",
      "PPO Sharpe Ratio:  0.1691363242252161\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_567_1\n",
      "day: 3026, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7473734.26\n",
      "total_reward: 6473734.26\n",
      "total_cost: 1066.23\n",
      "total_trades: 45317\n",
      "Sharpe: 0.551\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 38         |\n",
      "|    time_elapsed    | 318        |\n",
      "|    total_timesteps | 12108      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 10.8       |\n",
      "|    critic_loss     | 45         |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 9081       |\n",
      "|    reward          | -2.7150242 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2022-04-07 to  2022-07-08\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-07-08\n",
      "======Trading from:  2022-07-08 to  2022-10-11\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2022-07-08\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_630_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 61        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -116      |\n",
      "|    reward             | 1.2510895 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 11        |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 62       |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -143     |\n",
      "|    reward             | 2.176167 |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 13.3     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 657        |\n",
      "|    reward             | -5.7535453 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 245        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 62       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -198     |\n",
      "|    reward             | 59.1126  |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 29.3     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 39         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -289       |\n",
      "|    reward             | -7.2427278 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 53.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.00564  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 3.15e+03  |\n",
      "|    reward             | 16.575806 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 3.5e+04   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 55         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | -0.0341    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -56.3      |\n",
      "|    reward             | -1.4247812 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.64       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 63         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -71.1      |\n",
      "|    reward             | -1.3021892 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 13.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 71        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 58.3      |\n",
      "|    reward             | -6.496164 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 41.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 62       |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 79       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -172     |\n",
      "|    reward             | 3.16946  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 18.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 62       |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 88       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -124     |\n",
      "|    reward             | 3.33871  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 70.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 62       |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 95       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 146      |\n",
      "|    reward             | -6.02617 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 13.8     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -6.29     |\n",
      "|    reward             | 2.1917374 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.187     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 111       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 81.7      |\n",
      "|    reward             | 1.2479424 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.75      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 119       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 194       |\n",
      "|    reward             | -6.605959 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 50.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 127        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -227       |\n",
      "|    reward             | 0.23068732 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 32.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 62          |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 135         |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 262         |\n",
      "|    reward             | -0.79246473 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 60.4        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 143        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -82.6      |\n",
      "|    reward             | -1.9280306 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 6.26       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 151        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -125       |\n",
      "|    reward             | 0.84678656 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 9.17       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 62          |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 159         |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 46.6        |\n",
      "|    reward             | -0.48554286 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 3.77        |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2022-07-08 to  2022-10-11\n",
      "A2C Sharpe Ratio:  0.3768864732199703\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_630_1\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    fps             | 85       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 2048     |\n",
      "| train/             |          |\n",
      "|    reward          | 1.735179 |\n",
      "---------------------------------\n",
      "day: 3089, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2392872.56\n",
      "total_reward: 1392872.56\n",
      "total_cost: 2325063.16\n",
      "total_trades: 74504\n",
      "Sharpe: 0.339\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016910112 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.000777   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.4        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    reward               | -0.12653768 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 47.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 73         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01643231 |\n",
      "|    clip_fraction        | 0.207      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | -0.00105   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 204        |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    reward               | -4.9413824 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 150        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 98         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02890906 |\n",
      "|    clip_fraction        | 0.252      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | 0.0181     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 118        |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    reward               | 2.3908527  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 130        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018826734 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.00607     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 172         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    reward               | 1.006798    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 53          |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-07-08 to  2022-10-11\n",
      "PPO Sharpe Ratio:  0.09517943995036912\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_630_1\n",
      "day: 3089, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6670629.63\n",
      "total_reward: 5670629.63\n",
      "total_cost: 998.99\n",
      "total_trades: 49323\n",
      "Sharpe: 0.383\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 42        |\n",
      "|    time_elapsed    | 291       |\n",
      "|    total_timesteps | 12360     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 40.3      |\n",
      "|    critic_loss     | 74        |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9270      |\n",
      "|    reward          | 1.5500325 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2022-07-08 to  2022-10-11\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-10-11\n",
      "======Trading from:  2022-10-11 to  2023-01-10\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2022-10-11\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_693_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 49         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 20.3       |\n",
      "|    reward             | -1.1107283 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 48         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -38.3      |\n",
      "|    reward             | 0.22854814 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.5        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -22.5     |\n",
      "|    reward             | 0.8828272 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.29      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 48        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -26.3     |\n",
      "|    reward             | 78.204506 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.27      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 52        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 174       |\n",
      "|    reward             | -3.476319 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 44.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | -0.0149   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 2.64e+03  |\n",
      "|    reward             | 2.1639018 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.79e+04  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 47          |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 73          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -4.97       |\n",
      "|    reward             | -0.26592565 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.3         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 47         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 84         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -66.1      |\n",
      "|    reward             | -4.8382354 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 9.71       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 95       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -257     |\n",
      "|    reward             | 2.534765 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 42.5     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 47        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 106       |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 453       |\n",
      "|    reward             | 0.7487739 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 158       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 46          |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 117         |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -169        |\n",
      "|    reward             | -13.3375435 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 25.4        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 46        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 127       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -28.9     |\n",
      "|    reward             | 13.435318 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 180       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 47       |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 138      |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.9    |\n",
      "|    explained_variance | -0.00249 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 84.6     |\n",
      "|    reward             | 1.092244 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 6.41     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 46        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 149       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -157      |\n",
      "|    reward             | 0.41256   |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 14        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 160        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 752        |\n",
      "|    reward             | -1.8370063 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 335        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 46        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 171       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 414       |\n",
      "|    reward             | 2.9553847 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 88.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 182        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 350        |\n",
      "|    reward             | -3.1225939 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 99.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 46        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 192       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -235      |\n",
      "|    reward             | 3.7853591 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 375       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 203      |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -49      |\n",
      "|    reward             | 1.503663 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 2.11     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 46         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 213        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -111       |\n",
      "|    reward             | -2.4887753 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 14.7       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-10-11 to  2023-01-10\n",
      "A2C Sharpe Ratio:  0.05270120706500692\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_693_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 64        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 31        |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.7303415 |\n",
      "----------------------------------\n",
      "day: 3152, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2032695.35\n",
      "total_reward: 1032695.35\n",
      "total_cost: 2226922.52\n",
      "total_trades: 74593\n",
      "Sharpe: 0.436\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 63          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015090477 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.0123     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.96        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    reward               | 2.615576    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 71.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 63          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030808706 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.00257     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 136         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    reward               | -390.99478  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 98.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 63          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012988253 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0156      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 48          |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    reward               | 0.29042605  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 862         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 63          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 161         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018427536 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0404      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    reward               | -1.2391536  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 148         |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-10-11 to  2023-01-10\n",
      "PPO Sharpe Ratio:  0.05574053999732275\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_693_1\n",
      "day: 3152, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8947631.73\n",
      "total_reward: 7947631.73\n",
      "total_cost: 1784.11\n",
      "total_trades: 47187\n",
      "Sharpe: 0.382\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/              |               |\n",
      "|    episodes        | 4             |\n",
      "|    fps             | 35            |\n",
      "|    time_elapsed    | 352           |\n",
      "|    total_timesteps | 12612         |\n",
      "| train/             |               |\n",
      "|    actor_loss      | -93.3         |\n",
      "|    critic_loss     | 59.9          |\n",
      "|    learning_rate   | 0.0005        |\n",
      "|    n_updates       | 9459          |\n",
      "|    reward          | -0.0063310484 |\n",
      "--------------------------------------\n",
      "======DDPG Validation from:  2022-10-11 to  2023-01-10\n",
      "======Best Model Retraining from:  2010-01-01 to  2023-01-10\n",
      "======Trading from:  2023-01-10 to  2023-04-17\n",
      "Ensemble Strategy took:  101.14680640697479  minutes\n"
     ]
    }
   ],
   "source": [
    "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
    "                                                 PPO_model_kwargs,\n",
    "                                                 DDPG_model_kwargs,\n",
    "                                                 timesteps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c46f5240-ba04-4bc9-ac69-80c4719b7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_trade_date = processed[(processed.date > TRADE_START_DATE)&(processed.date <= TRADE_END_DATE)].date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86c8f748-0961-4fc5-82ba-5268f76f8ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  0.8946650915578\n"
     ]
    }
   ],
   "source": [
    "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
    "\n",
    "df_account_value=pd.DataFrame()\n",
    "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
    "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
    "    df_account_value = pd.concat([df_account_value, temp], ignore_index=True)\n",
    "sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
    "print('Sharpe Ratio: ',sharpe)\n",
    "df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6e160d2-152d-4f42-8ccd-4abb407f869c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_value</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>datadate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>1.879871e+06</td>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>2023-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>1.883552e+06</td>\n",
       "      <td>2023-04-10</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>2023-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>1.898745e+06</td>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>0.008066</td>\n",
       "      <td>2023-04-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>1.896372e+06</td>\n",
       "      <td>2023-04-12</td>\n",
       "      <td>-0.001250</td>\n",
       "      <td>2023-04-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>1.891606e+06</td>\n",
       "      <td>2023-04-13</td>\n",
       "      <td>-0.002513</td>\n",
       "      <td>2023-04-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     account_value        date  daily_return    datadate\n",
       "625   1.879871e+06  2023-04-06      0.001482  2023-04-06\n",
       "626   1.883552e+06  2023-04-10      0.001958  2023-04-10\n",
       "627   1.898745e+06  2023-04-11      0.008066  2023-04-11\n",
       "628   1.896372e+06  2023-04-12     -0.001250  2023-04-12\n",
       "629   1.891606e+06  2023-04-13     -0.002513  2023-04-13"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "973c8fb6-7e44-4e6b-96ed-eec8788c93cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df_account_value.account_value.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b8a4f8b-82b5-4709-b1f6-fdb65983f458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pyfolio\\pos.py:26: UserWarning: Module \"zipline.assets\" not found; mutltipliers will not be applied to position notionals.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return            0.290424\n",
      "Cumulative returns       0.891606\n",
      "Annual volatility      826.306371\n",
      "Sharpe ratio             0.894665\n",
      "Calmar ratio             0.290710\n",
      "Stability                0.111434\n",
      "Max drawdown            -0.999014\n",
      "Omega ratio            405.867147\n",
      "Sortino ratio          817.810965\n",
      "Skew                          NaN\n",
      "Kurtosis                      NaN\n",
      "Tail ratio               1.228954\n",
      "Daily value at risk   -101.171216\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from finrl.plot import backtest_stats\n",
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "83bbeac7-f787-4bc5-b945-56d646ace168",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_a2c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Save trained models\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel_a2c\u001b[49m\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_a2c\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      7\u001b[0m model_ppo\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_ppo\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      8\u001b[0m model_ddpg\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_ddpg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_a2c' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "save_dir = \"saved_strategy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save trained models\n",
    "model_a2c.save(os.path.join(save_dir, \"model_a2c\"))\n",
    "model_ppo.save(os.path.join(save_dir, \"model_ppo\"))\n",
    "model_ddpg.save(os.path.join(save_dir, \"model_ddpg\"))\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"A2C_model_kwargs\": A2C_model_kwargs,\n",
    "    \"PPO_model_kwargs\": PPO_model_kwargs,\n",
    "    \"DDPG_model_kwargs\": DDPG_model_kwargs,\n",
    "    \"timesteps_dict\": timesteps_dict,\n",
    "    # Add any other relevant metadata\n",
    "}\n",
    "with open(os.path.join(save_dir, \"metadata.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "# Save summary DataFrame\n",
    "df_summary.to_csv(os.path.join(save_dir, \"summary.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d4311dc-1307-4f28-8be1-c10e374e426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.to_csv('df_summary')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
