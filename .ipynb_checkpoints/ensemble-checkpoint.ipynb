{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "039f271c-8802-4449-9bc5-1e6f7cf3e9d5",
   "metadata": {},
   "source": [
    "# **_Reinforcement Learning tools for Auto-Stock Trading_**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7410276a-73b2-4d3e-86f4-b04517abcfb5",
   "metadata": {},
   "source": [
    "### 1. Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47834086-2dce-4be5-8161-3be168ccdf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Data Science Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e30a619-597c-4c74-9a55-cc95c742b9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\exchange_calendars\\exchange_calendar.py:2347: FutureWarning: 'T' is deprecated and will be removed in a future version. Please use 'min' instead of 'T'.\n",
      "  align: pd.Timedelta | str = pd.Timedelta(1, \"T\"),\n"
     ]
    }
   ],
   "source": [
    "#Finrl utilities\n",
    "from finrl import config\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import data_split\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c280e0f5-c4bd-4d91-bacb-100c5a429098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing Utilities\n",
    "import datetime\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "206b61a9-a03b-4e63-8237-c7f3b96ebb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make finrl imports accessible\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0645e3f4-6f35-43ca-a4fa-a643601d3e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup libraries\n",
    "from __future__ import annotations\n",
    "#postponed evaluation of type annotations and evaluation available at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f628e44b-b4bf-44e2-9231-81c2d6d06b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other imports will be used wherever applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f09ec60f-d1f4-4b21-97a6-60fe0c7cc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = '2010-01-01'\n",
    "TRAIN_END_DATE = '2020-07-01'\n",
    "TRADE_START_DATE = '2020-07-01'\n",
    "TRADE_END_DATE = '2023-05-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a200604-6d51-4c13-afa2-94ecfb231c15",
   "metadata": {},
   "source": [
    "### 2. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b33b42-1022-48ba-9bb4-724b1fa70c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw=pd.read_csv('datasets/BSE30.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e2e88-14b9-4199-b215-aef6f663bb11",
   "metadata": {},
   "source": [
    "### 3. Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85c5e44b-1124-4426-87de-925b8c8fff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprocessing import FeatureEngineer, load_dataset, data_split, convert_to_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "febd40c0-9d74-44a1-9fcf-a3b6f7b7bbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\Downloads\\BTP-2-4-24\\dataprocessing.py:236: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df_price_pivot = df_price_pivot.pct_change()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "from finrl.config import INDICATORS\n",
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                      tech_indicator_list = INDICATORS,\n",
    "                      use_vix=False,\n",
    "                      use_turbulence=True,\n",
    "                      user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "083563ce-5668-49e3-abed-4c42a4bfa9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>88.550003</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>19140</td>\n",
       "      <td>ASIANPAINT.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>105.800003</td>\n",
       "      <td>109.599998</td>\n",
       "      <td>103.459999</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>4536215</td>\n",
       "      <td>AXISBANK.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>206.050003</td>\n",
       "      <td>210.500000</td>\n",
       "      <td>196.500000</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>52648</td>\n",
       "      <td>BAJAJ-AUTO.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>15.140000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>14.975000</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>136590</td>\n",
       "      <td>BAJAJFINSV.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>6.660000</td>\n",
       "      <td>6.970000</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>274220</td>\n",
       "      <td>BAJFINANCE.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105698</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>981.000000</td>\n",
       "      <td>992.500000</td>\n",
       "      <td>979.250000</td>\n",
       "      <td>986.799988</td>\n",
       "      <td>26056</td>\n",
       "      <td>SUNPHARMA.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.263414</td>\n",
       "      <td>1019.314408</td>\n",
       "      <td>965.265603</td>\n",
       "      <td>50.085294</td>\n",
       "      <td>14.481255</td>\n",
       "      <td>1.567920</td>\n",
       "      <td>983.446670</td>\n",
       "      <td>985.046100</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105699</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>3208.000000</td>\n",
       "      <td>3227.199951</td>\n",
       "      <td>3197.149902</td>\n",
       "      <td>3175.769043</td>\n",
       "      <td>51644</td>\n",
       "      <td>TCS.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>-15.398183</td>\n",
       "      <td>3235.633708</td>\n",
       "      <td>3045.249324</td>\n",
       "      <td>48.649310</td>\n",
       "      <td>67.966063</td>\n",
       "      <td>0.407494</td>\n",
       "      <td>3131.238102</td>\n",
       "      <td>3257.234477</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105700</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>983.000000</td>\n",
       "      <td>1026.650024</td>\n",
       "      <td>982.950012</td>\n",
       "      <td>986.955139</td>\n",
       "      <td>279514</td>\n",
       "      <td>TECHM.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>-22.941437</td>\n",
       "      <td>1102.074200</td>\n",
       "      <td>929.293964</td>\n",
       "      <td>44.970681</td>\n",
       "      <td>-99.119890</td>\n",
       "      <td>22.233939</td>\n",
       "      <td>1033.226742</td>\n",
       "      <td>1032.633037</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105701</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>2663.500000</td>\n",
       "      <td>2679.300049</td>\n",
       "      <td>2620.050049</td>\n",
       "      <td>2640.399902</td>\n",
       "      <td>32742</td>\n",
       "      <td>TITAN.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>43.161331</td>\n",
       "      <td>2669.834479</td>\n",
       "      <td>2492.325506</td>\n",
       "      <td>60.306098</td>\n",
       "      <td>116.653875</td>\n",
       "      <td>37.463255</td>\n",
       "      <td>2542.391650</td>\n",
       "      <td>2482.099988</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105702</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>7524.950195</td>\n",
       "      <td>7576.549805</td>\n",
       "      <td>7487.549805</td>\n",
       "      <td>7520.226562</td>\n",
       "      <td>8316</td>\n",
       "      <td>ULTRACEMCO.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>28.692164</td>\n",
       "      <td>7774.630825</td>\n",
       "      <td>7303.703648</td>\n",
       "      <td>55.649892</td>\n",
       "      <td>27.765845</td>\n",
       "      <td>3.677975</td>\n",
       "      <td>7441.862826</td>\n",
       "      <td>7301.539185</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105703 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date         open         high          low        close  \\\n",
       "0       2009-01-02    90.750000    90.750000    88.550003    48.861801   \n",
       "1       2009-01-02   105.800003   109.599998   103.459999    71.914917   \n",
       "2       2009-01-02   206.050003   210.500000   196.500000   158.413025   \n",
       "3       2009-01-02    15.140000    15.800000    14.975000    13.401811   \n",
       "4       2009-01-02     6.660000     6.970000     6.350000     2.746401   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "105698  2023-04-28   981.000000   992.500000   979.250000   986.799988   \n",
       "105699  2023-04-28  3208.000000  3227.199951  3197.149902  3175.769043   \n",
       "105700  2023-04-28   983.000000  1026.650024   982.950012   986.955139   \n",
       "105701  2023-04-28  2663.500000  2679.300049  2620.050049  2640.399902   \n",
       "105702  2023-04-28  7524.950195  7576.549805  7487.549805  7520.226562   \n",
       "\n",
       "         volume            tic  day       macd      boll_ub      boll_lb  \\\n",
       "0         19140  ASIANPAINT.BO    4   0.000000    50.523346    48.068260   \n",
       "1       4536215    AXISBANK.BO    4   0.000000    50.523346    48.068260   \n",
       "2         52648  BAJAJ-AUTO.BO    4   0.000000    50.523346    48.068260   \n",
       "3        136590  BAJAJFINSV.BO    4   0.000000    50.523346    48.068260   \n",
       "4        274220  BAJFINANCE.BO    4   0.000000    50.523346    48.068260   \n",
       "...         ...            ...  ...        ...          ...          ...   \n",
       "105698    26056   SUNPHARMA.BO    4  -0.263414  1019.314408   965.265603   \n",
       "105699    51644         TCS.BO    4 -15.398183  3235.633708  3045.249324   \n",
       "105700   279514       TECHM.BO    4 -22.941437  1102.074200   929.293964   \n",
       "105701    32742       TITAN.BO    4  43.161331  2669.834479  2492.325506   \n",
       "105702     8316  ULTRACEMCO.BO    4  28.692164  7774.630825  7303.703648   \n",
       "\n",
       "            rsi_30      cci_30       dx_30  close_30_sma  close_60_sma  \\\n",
       "0       100.000000   66.666667  100.000000     48.861801     48.861801   \n",
       "1       100.000000   66.666667  100.000000     71.914917     71.914917   \n",
       "2       100.000000   66.666667  100.000000    158.413025    158.413025   \n",
       "3       100.000000   66.666667  100.000000     13.401811     13.401811   \n",
       "4       100.000000   66.666667  100.000000      2.746401      2.746401   \n",
       "...            ...         ...         ...           ...           ...   \n",
       "105698   50.085294   14.481255    1.567920    983.446670    985.046100   \n",
       "105699   48.649310   67.966063    0.407494   3131.238102   3257.234477   \n",
       "105700   44.970681  -99.119890   22.233939   1033.226742   1032.633037   \n",
       "105701   60.306098  116.653875   37.463255   2542.391650   2482.099988   \n",
       "105702   55.649892   27.765845    3.677975   7441.862826   7301.539185   \n",
       "\n",
       "        turbulence  \n",
       "0         0.000000  \n",
       "1         0.000000  \n",
       "2         0.000000  \n",
       "3         0.000000  \n",
       "4         0.000000  \n",
       "...            ...  \n",
       "105698   43.069415  \n",
       "105699   43.069415  \n",
       "105700   43.069415  \n",
       "105701   43.069415  \n",
       "105702   43.069415  \n",
       "\n",
       "[105703 rows x 17 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b299a545-307a-4d24-a530-9c7a1e0e84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b45789e-5f8a-4a0d-a1ba-070d4601b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ticker = df[\"tic\"].unique().tolist()\n",
    "# only apply to daily level data, need to fix for minute level\n",
    "list_date = list(pd.date_range(df['date'].min(),df['date'].max()).astype(str))\n",
    "combination = list(itertools.product(list_date,list_ticker))\n",
    "\n",
    "df_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(df,on=[\"date\",\"tic\"],how=\"left\")\n",
    "df_full = df_full[df_full['date'].isin(df['date'])]\n",
    "df_full = df_full.sort_values(['date','tic'])\n",
    "df_full = df_full.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f2577b7-eabd-4f2c-a467-cb5584c94aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ef628-f80f-4d6d-9a2f-b626ad01064e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d9d93b0-17f8-4ef6-92f0-3c7b229b2cd1",
   "metadata": {},
   "source": [
    "### 4.Splitting Training and Trading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28d66621-48cf-40ea-b98d-5b262d41fa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77550\n",
      "21120\n"
     ]
    }
   ],
   "source": [
    "train = data_split(df, TRAIN_START_DATE,TRAIN_END_DATE)\n",
    "trade = data_split(df, TRADE_START_DATE,TRADE_END_DATE)\n",
    "print(len(train))\n",
    "print(len(trade))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76538de6-2554-4a7f-b69b-7f1303363465",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_data.csv')\n",
    "trade.to_csv('trade_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa653329-3c36-4bef-803c-af5ab2e5d02d",
   "metadata": {},
   "source": [
    "### 5. Construction of Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fc3c5b8-8ec7-41c4-9f1d-76e4476c8fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TradingEnv import StockTradingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db88fde3-3088-471d-a275-f30f1c22f7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 30, State Space: 301\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "490ac2e0-422b-4f0d-9d59-c7a13738bb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>ASIANPAINT.BO</td>\n",
       "      <td>173.800003</td>\n",
       "      <td>179.990005</td>\n",
       "      <td>173.800003</td>\n",
       "      <td>113.311302</td>\n",
       "      <td>26700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.051069</td>\n",
       "      <td>115.089549</td>\n",
       "      <td>104.640905</td>\n",
       "      <td>66.436248</td>\n",
       "      <td>113.218646</td>\n",
       "      <td>24.458130</td>\n",
       "      <td>108.824595</td>\n",
       "      <td>104.144379</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>AXISBANK.BO</td>\n",
       "      <td>199.800003</td>\n",
       "      <td>199.800003</td>\n",
       "      <td>197.600006</td>\n",
       "      <td>142.226456</td>\n",
       "      <td>658270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.338474</td>\n",
       "      <td>151.029051</td>\n",
       "      <td>132.164238</td>\n",
       "      <td>52.578761</td>\n",
       "      <td>-1.919545</td>\n",
       "      <td>2.331440</td>\n",
       "      <td>142.339130</td>\n",
       "      <td>140.995590</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>BAJAJ-AUTO.BO</td>\n",
       "      <td>885.000000</td>\n",
       "      <td>886.000000</td>\n",
       "      <td>865.000000</td>\n",
       "      <td>687.819336</td>\n",
       "      <td>71150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.282086</td>\n",
       "      <td>707.370571</td>\n",
       "      <td>641.819761</td>\n",
       "      <td>61.119288</td>\n",
       "      <td>90.337046</td>\n",
       "      <td>15.068862</td>\n",
       "      <td>655.815855</td>\n",
       "      <td>622.730791</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>BAJAJFINSV.BO</td>\n",
       "      <td>34.900002</td>\n",
       "      <td>36.080002</td>\n",
       "      <td>34.799999</td>\n",
       "      <td>31.331820</td>\n",
       "      <td>1119010.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.473058</td>\n",
       "      <td>30.449640</td>\n",
       "      <td>27.363217</td>\n",
       "      <td>60.736230</td>\n",
       "      <td>321.150339</td>\n",
       "      <td>46.461213</td>\n",
       "      <td>28.700716</td>\n",
       "      <td>28.163338</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>BAJFINANCE.BO</td>\n",
       "      <td>33.270000</td>\n",
       "      <td>34.389999</td>\n",
       "      <td>33.270000</td>\n",
       "      <td>16.682077</td>\n",
       "      <td>221680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.568160</td>\n",
       "      <td>16.397749</td>\n",
       "      <td>14.096924</td>\n",
       "      <td>68.269383</td>\n",
       "      <td>212.436642</td>\n",
       "      <td>27.581496</td>\n",
       "      <td>14.887369</td>\n",
       "      <td>14.188843</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date            tic        open        high         low       close  \\\n",
       "0  2010-01-04  ASIANPAINT.BO  173.800003  179.990005  173.800003  113.311302   \n",
       "0  2010-01-04    AXISBANK.BO  199.800003  199.800003  197.600006  142.226456   \n",
       "0  2010-01-04  BAJAJ-AUTO.BO  885.000000  886.000000  865.000000  687.819336   \n",
       "0  2010-01-04  BAJAJFINSV.BO   34.900002   36.080002   34.799999   31.331820   \n",
       "0  2010-01-04  BAJFINANCE.BO   33.270000   34.389999   33.270000   16.682077   \n",
       "\n",
       "      volume  day       macd     boll_ub     boll_lb     rsi_30      cci_30  \\\n",
       "0    26700.0  0.0   2.051069  115.089549  104.640905  66.436248  113.218646   \n",
       "0   658270.0  0.0  -0.338474  151.029051  132.164238  52.578761   -1.919545   \n",
       "0    71150.0  0.0  16.282086  707.370571  641.819761  61.119288   90.337046   \n",
       "0  1119010.0  0.0   0.473058   30.449640   27.363217  60.736230  321.150339   \n",
       "0   221680.0  0.0   0.568160   16.397749   14.096924  68.269383  212.436642   \n",
       "\n",
       "       dx_30  close_30_sma  close_60_sma  turbulence  \n",
       "0  24.458130    108.824595    104.144379         0.0  \n",
       "0   2.331440    142.339130    140.995590         0.0  \n",
       "0  15.068862    655.815855    622.730791         0.0  \n",
       "0  46.461213     28.700716     28.163338         0.0  \n",
       "0  27.581496     14.887369     14.188843         0.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b19bfb4-2266-4356-a0a8-c8ef45fadb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"buy_cost_pct\": 0.001, \n",
    "    \"sell_cost_pct\": 0.001, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\":5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33775ce8-4afc-41e3-b742-f3aff0dadbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl.agents.stablebaselines3.models import DRLEnsembleAgent\n",
    "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
    "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
    "\n",
    "ensemble_agent = DRLEnsembleAgent(df=df,\n",
    "                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
    "                 val_test_period=(TRADE_START_DATE,TRADE_END_DATE),\n",
    "                 rebalance_window=rebalance_window, \n",
    "                 validation_window=validation_window, \n",
    "                 **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b3af798-f792-47af-87e9-b89db8fcffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\n",
    "                    'n_steps': 5,\n",
    "                    'ent_coef': 0.005,\n",
    "                    'learning_rate': 0.0007\n",
    "                    }\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "                    \"ent_coef\":0.01,\n",
    "                    \"n_steps\": 2048,\n",
    "                    \"learning_rate\": 0.00025,\n",
    "                    \"batch_size\": 128\n",
    "                    }\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
    "                      \"buffer_size\": 10_000,\n",
    "                      \"learning_rate\": 0.0005,\n",
    "                      \"batch_size\": 64\n",
    "                    }\n",
    "\n",
    "timesteps_dict = {'a2c' : 10_000, \n",
    "                 'ppo' : 10_000, \n",
    "                 'ddpg' : 10_000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "031defd6-1140-410b-b988-1edbdc7ab52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2020-07-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to tensorboard_log/a2c\\a2c_126_2\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 55         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0.0318     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -72.7      |\n",
      "|    reward             | -0.2836429 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.79       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 55         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 8.66       |\n",
      "|    reward             | 0.71366155 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.44       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 348      |\n",
      "|    reward             | 7.499145 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 145      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 55         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 36         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -718       |\n",
      "|    reward             | 115.294876 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 380        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 526       |\n",
      "|    reward             | -42.26177 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 601       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 55         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 53         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -170       |\n",
      "|    reward             | -1.9063131 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 15.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 27.3      |\n",
      "|    reward             | 1.2643732 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 10.6      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 55          |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 72          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 213         |\n",
      "|    reward             | -0.16960628 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 58.8        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -624      |\n",
      "|    reward             | 3.7619452 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 254       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 90        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 117       |\n",
      "|    reward             | 3.0696223 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 32.6      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 99       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | 0.000117 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -4.31    |\n",
      "|    reward             | 1.696195 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.3      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 108       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -28.5     |\n",
      "|    reward             | 0.5941854 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.7       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 117       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 177       |\n",
      "|    reward             | -5.621383 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 33.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 55          |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 126         |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -283        |\n",
      "|    reward             | -0.28846124 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 88.6        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 135       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -723      |\n",
      "|    reward             | 0.7434546 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 271       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 55          |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 144         |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -122        |\n",
      "|    reward             | -0.17724358 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 11          |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 55          |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 153         |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 67.4        |\n",
      "|    reward             | -0.07286738 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 7.33        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 162       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -140      |\n",
      "|    reward             | 10.635272 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 29.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 171       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 211       |\n",
      "|    reward             | 12.211534 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 30.5      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 181      |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -230     |\n",
      "|    reward             | 8.08215  |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 32.1     |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2020-07-02 to  2020-09-29\n",
      "A2C Sharpe Ratio:  0.11406474299467247\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_126_2\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 78         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 26         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | -1.0559512 |\n",
      "-----------------------------------\n",
      "day: 2585, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1774987.96\n",
      "total_reward: 774987.96\n",
      "total_cost: 1660675.66\n",
      "total_trades: 63070\n",
      "Sharpe: 0.386\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015467463 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.00598     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    reward               | 4.804846    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 35          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01514141  |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0191      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 22.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    reward               | -0.46414262 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 54.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019670766 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0343      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 116         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    reward               | 1.5517982   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 76.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033595134 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.048       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    reward               | -1.5878446  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 81.2        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-07-02 to  2020-09-29\n",
      "PPO Sharpe Ratio:  0.21516728172867136\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_126_2\n",
      "day: 2585, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4631660.09\n",
      "total_reward: 3631660.09\n",
      "total_cost: 1044.00\n",
      "total_trades: 38720\n",
      "Sharpe: 0.368\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 39        |\n",
      "|    time_elapsed    | 260       |\n",
      "|    total_timesteps | 10344     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -51.5     |\n",
      "|    critic_loss     | 33.2      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 7758      |\n",
      "|    reward          | 1.4182893 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2020-07-02 to  2020-09-29\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-09-29\n",
      "======Trading from:  2020-09-29 to  2020-12-30\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2020-09-29\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_189_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 53         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | -0.18      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -123       |\n",
      "|    reward             | -1.4913697 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 9.27       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -0.0875   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 88.3      |\n",
      "|    reward             | 1.5181862 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 5.94      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 54         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 598        |\n",
      "|    reward             | -1.5162566 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 190        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -156      |\n",
      "|    reward             | 757.69794 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 38.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 54         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 45         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 262        |\n",
      "|    reward             | -60.129467 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 354        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -116      |\n",
      "|    reward             | 2.5667593 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 8.49      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 53.6      |\n",
      "|    reward             | 1.2329332 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.75      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 54         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 73         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 65.2       |\n",
      "|    reward             | -2.8188615 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 11.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 82        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 13.4      |\n",
      "|    reward             | 8.674923  |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 21.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 91       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -695     |\n",
      "|    reward             | 5.230303 |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 486      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 100       |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -6.54     |\n",
      "|    reward             | 1.8048289 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 0.459     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 109       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -57.5     |\n",
      "|    reward             | 0.7935943 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 3.75      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 118       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 15.1      |\n",
      "|    reward             | 1.2544382 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.15      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 54         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 128        |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -240       |\n",
      "|    reward             | -2.7277973 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 56.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 54         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 137        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -994       |\n",
      "|    reward             | -15.473264 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 590        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 54         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 146        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | -2.83e-05  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 46.5       |\n",
      "|    reward             | 0.30530384 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.69       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 54         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 155        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -37.2      |\n",
      "|    reward             | -2.6557252 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.05       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 164      |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 61.6     |\n",
      "|    reward             | 1.61098  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 2.13     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 54          |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 173         |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 35.5        |\n",
      "|    reward             | 0.087889716 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 4.23        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 182       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 114       |\n",
      "|    reward             | 2.0199902 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 10.6      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2020-09-29 to  2020-12-30\n",
      "A2C Sharpe Ratio:  0.5462932723595365\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_189_2\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 76        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 26        |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.7358204 |\n",
      "----------------------------------\n",
      "day: 2648, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1729261.00\n",
      "total_reward: 729261.00\n",
      "total_cost: 1634967.10\n",
      "total_trades: 63262\n",
      "Sharpe: 0.343\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019831859 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.00679    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    reward               | -0.09860015 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 38.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020324439 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.00653     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    reward               | 1.10575     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 59.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 80         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 102        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02612715 |\n",
      "|    clip_fraction        | 0.285      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.8      |\n",
      "|    explained_variance   | 0.0853     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 38.1       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    reward               | 0.4477226  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 37.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017770149 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.0476      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23.8        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    reward               | -3.3698742  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 139         |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-09-29 to  2020-12-30\n",
      "PPO Sharpe Ratio:  0.7111803491800973\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_189_2\n",
      "day: 2648, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4875530.51\n",
      "total_reward: 3875530.51\n",
      "total_cost: 1566.52\n",
      "total_trades: 23800\n",
      "Sharpe: 0.454\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 49        |\n",
      "|    time_elapsed    | 213       |\n",
      "|    total_timesteps | 10596     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -20.3     |\n",
      "|    critic_loss     | 66.2      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 7947      |\n",
      "|    reward          | 13.871818 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2020-09-29 to  2020-12-30\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-12-30\n",
      "======Trading from:  2020-12-30 to  2021-04-01\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2020-12-30\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_252_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 66          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -80.7       |\n",
      "|    reward             | -0.30962017 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 4.31        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -2.3      |\n",
      "|    reward             | 1.0452757 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.755     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 192        |\n",
      "|    reward             | -4.3610754 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 24.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -889      |\n",
      "|    reward             | 72.725586 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 463       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 359        |\n",
      "|    reward             | -26.352432 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 122        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -0.169    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 69.6      |\n",
      "|    reward             | -4.114417 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 8.51      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 12        |\n",
      "|    reward             | -1.74577  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.08      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 58         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -355       |\n",
      "|    reward             | -0.6379254 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 91.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 66        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -112      |\n",
      "|    reward             | 4.5545015 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 25.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 73        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -124      |\n",
      "|    reward             | -5.501853 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 25        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 81         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 35.3       |\n",
      "|    reward             | -1.8669009 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.83       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 88         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0.0347     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 198        |\n",
      "|    reward             | -3.8051705 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 25.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 96         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -283       |\n",
      "|    reward             | 0.99005055 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 44.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -227      |\n",
      "|    reward             | 10.696322 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 32.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 111        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -206       |\n",
      "|    reward             | -0.7368431 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 35.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 118        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0.00113    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -282       |\n",
      "|    reward             | -0.3565193 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 59.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 125        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -25.8      |\n",
      "|    reward             | -1.6450503 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.18       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 67          |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 133         |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.8       |\n",
      "|    explained_variance | -0.00666    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | 179         |\n",
      "|    reward             | -0.23383628 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 26.4        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 140       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 160       |\n",
      "|    reward             | 3.2529793 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 18.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 147       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 143       |\n",
      "|    reward             | 0.6950233 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 21.4      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2020-12-30 to  2021-04-01\n",
      "A2C Sharpe Ratio:  -0.006886856917852125\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_252_2\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 100       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 20        |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | -1.294167 |\n",
      "----------------------------------\n",
      "day: 2711, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3042691.20\n",
      "total_reward: 2042691.20\n",
      "total_cost: 1923320.98\n",
      "total_trades: 66332\n",
      "Sharpe: 0.333\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 98            |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 41            |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.017720722   |\n",
      "|    clip_fraction        | 0.191         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | -0.00551      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 14.5          |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.022        |\n",
      "|    reward               | -0.0077406573 |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 47.2          |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019812107 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | -0.00491    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 254         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    reward               | -1.137241   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 100         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021426562 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.013       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 263         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    reward               | 1.499932    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 133         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025093421 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.0194      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 46.6        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00759    |\n",
      "|    reward               | 3.747242    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 251         |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-12-30 to  2021-04-01\n",
      "PPO Sharpe Ratio:  -0.08573402429264845\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_252_2\n",
      "day: 2711, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7154263.69\n",
      "total_reward: 6154263.69\n",
      "total_cost: 1034.78\n",
      "total_trades: 40597\n",
      "Sharpe: 0.380\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 50         |\n",
      "|    time_elapsed    | 216        |\n",
      "|    total_timesteps | 10848      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -65        |\n",
      "|    critic_loss     | 42.8       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 8136       |\n",
      "|    reward          | -0.2907477 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2020-12-30 to  2021-04-01\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-04-01\n",
      "======Trading from:  2021-04-01 to  2021-07-05\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2021-04-01\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_315_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 66          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -21.7       |\n",
      "|    reward             | -0.07426238 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.72        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 68         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -58        |\n",
      "|    reward             | 0.79760975 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.52       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 65       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 258      |\n",
      "|    reward             | 5.108707 |\n",
      "|    std                | 0.999    |\n",
      "|    value_loss         | 82.8     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -494      |\n",
      "|    reward             | 3.7966483 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 172       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 757        |\n",
      "|    reward             | -43.211327 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 737        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 63         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 12.5       |\n",
      "|    reward             | 0.22655925 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 6.65       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 63         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 55         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 27.8       |\n",
      "|    reward             | 0.19328615 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 0.625      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 62       |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 64       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -360     |\n",
      "|    reward             | 2.210253 |\n",
      "|    std                | 0.993    |\n",
      "|    value_loss         | 75.7     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 72        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -160      |\n",
      "|    reward             | 1.0765895 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 19.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 31.2      |\n",
      "|    reward             | 0.8812732 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 1.48      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 87        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 363       |\n",
      "|    reward             | -8.365561 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 78.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 63        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 94        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 40.8      |\n",
      "|    reward             | 1.0036873 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 3.08      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 63        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 102       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0.0145    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 10        |\n",
      "|    reward             | 1.0053391 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 2.68      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 63        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 110       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -805      |\n",
      "|    reward             | -5.797137 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 432       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 63        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 117       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 172       |\n",
      "|    reward             | -8.977072 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 44.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -849      |\n",
      "|    reward             | 10.874769 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 476       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 64          |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 132         |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0.0698      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 91.8        |\n",
      "|    reward             | -0.52453494 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 4.96        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 64       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 139      |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 59.4     |\n",
      "|    reward             | 2.924116 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 2.79     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 146        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -267       |\n",
      "|    reward             | -1.7296058 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 55.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 154        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 247        |\n",
      "|    reward             | -5.9805484 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 33.1       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-04-01 to  2021-07-05\n",
      "A2C Sharpe Ratio:  0.06660081332959643\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_315_2\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 88         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 23         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.39228547 |\n",
      "-----------------------------------\n",
      "day: 2774, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3272022.49\n",
      "total_reward: 2272022.49\n",
      "total_cost: 2000151.40\n",
      "total_trades: 68027\n",
      "Sharpe: 0.419\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018684208 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.00532     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.9        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    reward               | 1.75977     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 94.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019063659 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | -0.00865    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 42          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    reward               | -0.95667434 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 150         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019987151 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | -0.00849    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 44.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    reward               | -0.19267583 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 98.1        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 88        |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 115       |\n",
      "|    total_timesteps      | 10240     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0177778 |\n",
      "|    clip_fraction        | 0.205     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -42.8     |\n",
      "|    explained_variance   | 0.00872   |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | 120       |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -0.0145   |\n",
      "|    reward               | 1.7352338 |\n",
      "|    std                  | 1.01      |\n",
      "|    value_loss           | 101       |\n",
      "---------------------------------------\n",
      "======PPO Validation from:  2021-04-01 to  2021-07-05\n",
      "PPO Sharpe Ratio:  0.17512185837113156\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_315_2\n",
      "day: 2774, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6623027.61\n",
      "total_reward: 5623027.61\n",
      "total_cost: 999.00\n",
      "total_trades: 63693\n",
      "Sharpe: 0.341\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 48         |\n",
      "|    time_elapsed    | 229        |\n",
      "|    total_timesteps | 11100      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -73.8      |\n",
      "|    critic_loss     | 95.4       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 8325       |\n",
      "|    reward          | -3.6211135 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2021-04-01 to  2021-07-05\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-07-05\n",
      "======Trading from:  2021-07-05 to  2021-10-05\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2021-07-05\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_378_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -114       |\n",
      "|    reward             | 0.08958644 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 12.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -6.42     |\n",
      "|    reward             | 1.1582463 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.93      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 448       |\n",
      "|    reward             | 11.762014 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 207       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 64       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -520     |\n",
      "|    reward             | 724.5202 |\n",
      "|    std                | 0.999    |\n",
      "|    value_loss         | 216      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 62.9       |\n",
      "|    reward             | -15.607009 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 53.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 55.9      |\n",
      "|    reward             | -2.215917 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 4.29      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 54         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0.0964     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 125        |\n",
      "|    reward             | -6.4168844 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 8.07       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -135      |\n",
      "|    reward             | 2.1097589 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 11.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 70        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -265      |\n",
      "|    reward             | 6.4834547 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 71        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 77         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -22.7      |\n",
      "|    reward             | -15.268183 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 8.33       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 85        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -18.6     |\n",
      "|    reward             | -10.37186 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 77.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 93        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -205      |\n",
      "|    reward             | 1.7861346 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 27.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 101        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 211        |\n",
      "|    reward             | 0.53737485 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 37         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 108       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 306       |\n",
      "|    reward             | 4.5466957 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 71.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 116       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -133      |\n",
      "|    reward             | 1.7445685 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 16.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 124        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -222       |\n",
      "|    reward             | -2.5355158 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 56.1       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 64          |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 132         |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | -27.6       |\n",
      "|    reward             | -0.50081855 |\n",
      "|    std                | 0.993       |\n",
      "|    value_loss         | 9.69        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 140        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -114       |\n",
      "|    reward             | -2.4446135 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 8.7        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 64          |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 148         |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -389        |\n",
      "|    reward             | -0.11168294 |\n",
      "|    std                | 0.992       |\n",
      "|    value_loss         | 97.6        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 155        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -16.5      |\n",
      "|    reward             | -0.9435085 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 1.63       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-07-05 to  2021-10-05\n",
      "A2C Sharpe Ratio:  0.2876438612993931\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_378_2\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 93          |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 22          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.011977726 |\n",
      "------------------------------------\n",
      "day: 2837, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2601995.89\n",
      "total_reward: 1601995.89\n",
      "total_cost: 2025393.91\n",
      "total_trades: 68707\n",
      "Sharpe: 0.430\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.016970117  |\n",
      "|    clip_fraction        | 0.19         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -0.0011      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 15.6         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0172      |\n",
      "|    reward               | -0.015341467 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 63.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015279269 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0047      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23.7        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    reward               | -2.0359447  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 71.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019787505 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0168      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.26        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    reward               | -5.737983   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 37.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016693046 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0894      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.27        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    reward               | 0.28941965  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 29.8        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-07-05 to  2021-10-05\n",
      "PPO Sharpe Ratio:  0.5441406911097518\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_378_2\n",
      "day: 2837, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6358826.76\n",
      "total_reward: 5358826.76\n",
      "total_cost: 1011.62\n",
      "total_trades: 42487\n",
      "Sharpe: 0.452\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 4           |\n",
      "|    fps             | 48          |\n",
      "|    time_elapsed    | 235         |\n",
      "|    total_timesteps | 11352       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | 44.4        |\n",
      "|    critic_loss     | 111         |\n",
      "|    learning_rate   | 0.0005      |\n",
      "|    n_updates       | 8514        |\n",
      "|    reward          | -0.52093184 |\n",
      "------------------------------------\n",
      "======DDPG Validation from:  2021-07-05 to  2021-10-05\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-10-05\n",
      "======Trading from:  2021-10-05 to  2022-01-05\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2021-10-05\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_441_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -112       |\n",
      "|    reward             | 0.29221085 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 6.68       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -79.7     |\n",
      "|    reward             | 0.9637775 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 5         |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 49.6       |\n",
      "|    reward             | -1.0022963 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 10.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 167       |\n",
      "|    reward             | 404.11685 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 22.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 99.2       |\n",
      "|    reward             | -32.646458 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 92.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -70.5     |\n",
      "|    reward             | 1.0081937 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 3.96      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 64           |\n",
      "|    iterations         | 700          |\n",
      "|    time_elapsed       | 54           |\n",
      "|    total_timesteps    | 3500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -42.3        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 699          |\n",
      "|    policy_loss        | -67.2        |\n",
      "|    reward             | -0.068191424 |\n",
      "|    std                | 0.991        |\n",
      "|    value_loss         | 3.15         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0.0463    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -38.2     |\n",
      "|    reward             | 2.3273296 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 3.88      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 70         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -15.8      |\n",
      "|    reward             | -3.8720508 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 5.55       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 64          |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 77          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -24.5       |\n",
      "|    reward             | -0.93281543 |\n",
      "|    std                | 0.99        |\n",
      "|    value_loss         | 6.95        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 85        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -359      |\n",
      "|    reward             | -9.045644 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 134       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 93         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -4.15      |\n",
      "|    reward             | -2.5545368 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 0.503      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 100        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 20.2       |\n",
      "|    reward             | -1.9887943 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 1.82       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 64       |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 108      |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 253      |\n",
      "|    reward             | 1.086206 |\n",
      "|    std                | 0.99     |\n",
      "|    value_loss         | 30.8     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 116       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -903      |\n",
      "|    reward             | 15.145094 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 1.12e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 64        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -1.04e+03 |\n",
      "|    reward             | 25.930286 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 835       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 132        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 2.42e+03   |\n",
      "|    reward             | -1.8161721 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 3.77e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 140        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | -0.0109    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 269        |\n",
      "|    reward             | -1.2385741 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 45.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 64          |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 147         |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.2       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 53.6        |\n",
      "|    reward             | -0.04253159 |\n",
      "|    std                | 0.988       |\n",
      "|    value_loss         | 2.36        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 64       |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 155      |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -2.57    |\n",
      "|    reward             | 5.483636 |\n",
      "|    std                | 0.987    |\n",
      "|    value_loss         | 1.3      |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2021-10-05 to  2022-01-05\n",
      "A2C Sharpe Ratio:  0.008166926193272762\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_441_2\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 91         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 22         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.23241335 |\n",
      "-----------------------------------\n",
      "day: 2900, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2835808.32\n",
      "total_reward: 1835808.32\n",
      "total_cost: 1849307.09\n",
      "total_trades: 68376\n",
      "Sharpe: 0.330\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0146736745 |\n",
      "|    clip_fraction        | 0.201        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | -0.00312     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.77         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0234      |\n",
      "|    reward               | -0.13624087  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 40.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018624302 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.00214     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    reward               | -0.97834826 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 57.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014289261 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.00784     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 36.7        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    reward               | 1.9558306   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 178         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 87        |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 116       |\n",
      "|    total_timesteps      | 10240     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0294759 |\n",
      "|    clip_fraction        | 0.289     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -42.7     |\n",
      "|    explained_variance   | 0.0127    |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | 15.2      |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -0.0126   |\n",
      "|    reward               | -2.538213 |\n",
      "|    std                  | 1.01      |\n",
      "|    value_loss           | 90.2      |\n",
      "---------------------------------------\n",
      "======PPO Validation from:  2021-10-05 to  2022-01-05\n",
      "PPO Sharpe Ratio:  0.05613531846793805\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_441_2\n",
      "day: 2900, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7542039.86\n",
      "total_reward: 6542039.86\n",
      "total_cost: 999.00\n",
      "total_trades: 52116\n",
      "Sharpe: 0.358\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 48        |\n",
      "|    time_elapsed    | 241       |\n",
      "|    total_timesteps | 11604     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 22.9      |\n",
      "|    critic_loss     | 88.3      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 8703      |\n",
      "|    reward          | 5.5042963 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2021-10-05 to  2022-01-05\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-01-05\n",
      "======Trading from:  2022-01-05 to  2022-04-07\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2022-01-05\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_504_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 57         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | -2.23      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -88.5      |\n",
      "|    reward             | 0.17671154 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 6.05       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -40.9    |\n",
      "|    reward             | 0.963694 |\n",
      "|    std                | 0.996    |\n",
      "|    value_loss         | 1.54     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 81.9       |\n",
      "|    reward             | 0.81543535 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 4.06       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 60       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -187     |\n",
      "|    reward             | 28.15505 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 25.3     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -74.5     |\n",
      "|    reward             | -6.225252 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 8.92      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0.0283     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -45.1      |\n",
      "|    reward             | 0.31427768 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 1.79       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 61        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | -0.0368   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 63.9      |\n",
      "|    reward             | 1.7260412 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 3.01      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 61          |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 65          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 71.4        |\n",
      "|    reward             | -0.47406557 |\n",
      "|    std                | 0.993       |\n",
      "|    value_loss         | 4.58        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 61          |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 73          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.3       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -30.4       |\n",
      "|    reward             | -0.71137786 |\n",
      "|    std                | 0.993       |\n",
      "|    value_loss         | 0.742       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 61        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | -0.0967   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 131       |\n",
      "|    reward             | -5.487768 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 15        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 89         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0.0625     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 310        |\n",
      "|    reward             | -14.448683 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 140        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 97         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0.00101    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -30.1      |\n",
      "|    reward             | 0.62530565 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 1          |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 105        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -393       |\n",
      "|    reward             | -1.3408586 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 85.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 61        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 178       |\n",
      "|    reward             | 4.5558634 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 32.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 121        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -58.4      |\n",
      "|    reward             | -5.6081285 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 11.2       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 61       |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 129      |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -797     |\n",
      "|    reward             | 11.93438 |\n",
      "|    std                | 0.992    |\n",
      "|    value_loss         | 364      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 61        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 137       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 1.13e+03  |\n",
      "|    reward             | 12.985769 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 916       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 61       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 145      |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 88.5     |\n",
      "|    reward             | 1.201531 |\n",
      "|    std                | 0.993    |\n",
      "|    value_loss         | 5.16     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 61        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 153       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -279      |\n",
      "|    reward             | 2.6602204 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 55.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 161        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -278       |\n",
      "|    reward             | -2.7745178 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 71.1       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-01-05 to  2022-04-07\n",
      "A2C Sharpe Ratio:  0.24138820199205732\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_504_2\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 88         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 23         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | -0.8344172 |\n",
      "-----------------------------------\n",
      "day: 2963, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2680630.67\n",
      "total_reward: 1680630.67\n",
      "total_cost: 2110638.21\n",
      "total_trades: 71030\n",
      "Sharpe: 0.416\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015723433 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.0207      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    reward               | 0.7030176   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 48.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018919218 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | -0.0023     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    reward               | -1.1758727  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 74.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017154908 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0698      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 27          |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    reward               | 0.18627387  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 88.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 117        |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01883312 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.8      |\n",
      "|    explained_variance   | 0.0342     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 144        |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.015     |\n",
      "|    reward               | 2.5669053  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 64.1       |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2022-01-05 to  2022-04-07\n",
      "PPO Sharpe Ratio:  0.1974788687492428\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_504_2\n",
      "day: 2963, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11758611.48\n",
      "total_reward: 10758611.48\n",
      "total_cost: 999.00\n",
      "total_trades: 41417\n",
      "Sharpe: 0.524\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 47       |\n",
      "|    time_elapsed    | 250      |\n",
      "|    total_timesteps | 11856    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -60.5    |\n",
      "|    critic_loss     | 58.7     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 8892     |\n",
      "|    reward          | 9.610302 |\n",
      "---------------------------------\n",
      "======DDPG Validation from:  2022-01-05 to  2022-04-07\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-04-07\n",
      "======Trading from:  2022-04-07 to  2022-07-08\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2022-04-07\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_567_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -92.2      |\n",
      "|    reward             | 0.51049435 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.22       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -41.2      |\n",
      "|    reward             | 0.73400676 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.26       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | -0.212     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 154        |\n",
      "|    reward             | -0.6144098 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 14.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -181      |\n",
      "|    reward             | 261.72137 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 28.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -107      |\n",
      "|    reward             | -6.648151 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 11.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -0.0165   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 4.2e+03   |\n",
      "|    reward             | 17.904318 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 8.57e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 62       |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 56       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -214     |\n",
      "|    reward             | 4.320806 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 22.3     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 64         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 12.6       |\n",
      "|    reward             | -1.7816259 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 7.56       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 72         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 195        |\n",
      "|    reward             | -1.2216614 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 26.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 80        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 170       |\n",
      "|    reward             | 3.9385214 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 32        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 88        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 525       |\n",
      "|    reward             | 2.5161576 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 193       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 96         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 52.3       |\n",
      "|    reward             | -30.211584 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 55.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 104        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -105       |\n",
      "|    reward             | 0.22505625 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 9.41       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 112       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -31.9     |\n",
      "|    reward             | 0.5795111 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.645     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 120        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 36.1       |\n",
      "|    reward             | -4.7929335 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.47       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 128        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -126       |\n",
      "|    reward             | -3.3116603 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 19.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 136       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 397       |\n",
      "|    reward             | -0.41768  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 157       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 144        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -159       |\n",
      "|    reward             | 0.19549645 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 35.7       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 62          |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 152         |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 91.4        |\n",
      "|    reward             | -0.07404768 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 13.4        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 160        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 269        |\n",
      "|    reward             | -3.0629227 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 43.5       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-04-07 to  2022-07-08\n",
      "A2C Sharpe Ratio:  0.24919483247926985\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_567_2\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 87         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 23         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | -0.3522712 |\n",
      "-----------------------------------\n",
      "day: 3026, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2941840.68\n",
      "total_reward: 1941840.68\n",
      "total_cost: 2402476.03\n",
      "total_trades: 73505\n",
      "Sharpe: 0.320\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 47           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0141270105 |\n",
      "|    clip_fraction        | 0.22         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | 0.00365      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 15.5         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.018       |\n",
      "|    reward               | -0.5418157   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 97.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020846764 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.00786     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 30.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    reward               | -0.0893679  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 162         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 96         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02042262 |\n",
      "|    clip_fraction        | 0.232      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.6      |\n",
      "|    explained_variance   | 0.0493     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 32.9       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.00929   |\n",
      "|    reward               | -1.1479701 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 103        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 120         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023918986 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.00362     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.2        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    reward               | 0.48714033  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 59.8        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-04-07 to  2022-07-08\n",
      "PPO Sharpe Ratio:  0.2085667271600255\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_567_2\n",
      "day: 3026, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7253697.01\n",
      "total_reward: 6253697.01\n",
      "total_cost: 1092.70\n",
      "total_trades: 42303\n",
      "Sharpe: 0.530\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 46         |\n",
      "|    time_elapsed    | 258        |\n",
      "|    total_timesteps | 12108      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -55.4      |\n",
      "|    critic_loss     | 47.9       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 9081       |\n",
      "|    reward          | -4.3261495 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2022-04-07 to  2022-07-08\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-07-08\n",
      "======Trading from:  2022-07-08 to  2022-10-11\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2022-07-08\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_630_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 59          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | -0.125      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -93.3       |\n",
      "|    reward             | 0.116110526 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 6.74        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -25       |\n",
      "|    reward             | 1.1133811 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 7.18      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 379       |\n",
      "|    reward             | 10.124326 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 154       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | -0.0217  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -449     |\n",
      "|    reward             | 625.3437 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 169      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 42.5       |\n",
      "|    reward             | -14.030238 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 33         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.00124  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 1.47e+04  |\n",
      "|    reward             | 28.762705 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 6.42e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0.294     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 4.46      |\n",
      "|    reward             | 2.8567295 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 0.139     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 64.4      |\n",
      "|    reward             | 7.3210382 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 7.24      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 75         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -374       |\n",
      "|    reward             | -4.9409037 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 112        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 84        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -379      |\n",
      "|    reward             | -16.62485 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 99.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 92        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -742      |\n",
      "|    reward             | 2.6158855 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 412       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 100        |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 1.25e+03   |\n",
      "|    reward             | -31.439732 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 1.25e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 109      |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | 9.66e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 78.7     |\n",
      "|    reward             | 2.678681 |\n",
      "|    std                | 0.996    |\n",
      "|    value_loss         | 3.53     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 117        |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 190        |\n",
      "|    reward             | -0.3186318 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 58.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 126        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | -0.0105    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 623        |\n",
      "|    reward             | -12.052382 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 281        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 134      |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 348      |\n",
      "|    reward             | 9.081514 |\n",
      "|    std                | 0.996    |\n",
      "|    value_loss         | 256      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 143        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -457       |\n",
      "|    reward             | 0.97957045 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 324        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 151       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -0.000163 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 554       |\n",
      "|    reward             | 1.6901181 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 354       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 160        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0.00762    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -84.4      |\n",
      "|    reward             | 0.58117175 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 4.36       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 168      |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -197     |\n",
      "|    reward             | 6.256515 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 25.5     |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-07-08 to  2022-10-11\n",
      "A2C Sharpe Ratio:  -0.10122092516220454\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_630_2\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 86         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 23         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.14928535 |\n",
      "-----------------------------------\n",
      "day: 3089, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3042470.37\n",
      "total_reward: 2042470.37\n",
      "total_cost: 2355877.57\n",
      "total_trades: 74572\n",
      "Sharpe: 0.568\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024986312 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.0169      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    reward               | -0.4075529  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 69.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017242976 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.000754    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 56.4        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    reward               | -6.0462327  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 179         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 97          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024335595 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0317      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 103         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00876    |\n",
      "|    reward               | 0.13231933  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017935827 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0396      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12          |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00609    |\n",
      "|    reward               | 1.3090798   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 30.8        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-07-08 to  2022-10-11\n",
      "PPO Sharpe Ratio:  0.08888541938646742\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_630_2\n",
      "day: 3089, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6948686.00\n",
      "total_reward: 5948686.00\n",
      "total_cost: 999.00\n",
      "total_trades: 33920\n",
      "Sharpe: 0.312\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 46        |\n",
      "|    time_elapsed    | 266       |\n",
      "|    total_timesteps | 12360     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 6.93      |\n",
      "|    critic_loss     | 85.6      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9270      |\n",
      "|    reward          | 7.6386557 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2022-07-08 to  2022-10-11\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-10-11\n",
      "======Trading from:  2022-10-11 to  2023-01-10\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2022-10-11\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_693_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 61          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.4       |\n",
      "|    explained_variance | 0.00254     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -56.1       |\n",
      "|    reward             | -0.32299224 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 2.07        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -21.7      |\n",
      "|    reward             | 0.33584356 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 2.75       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 61        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 365       |\n",
      "|    reward             | 6.3689265 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 110       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 61       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 22.4     |\n",
      "|    reward             | 678.5918 |\n",
      "|    std                | 0.991    |\n",
      "|    value_loss         | 11.5     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 40         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 155        |\n",
      "|    reward             | -41.032413 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 159        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 61       |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.3    |\n",
      "|    explained_variance | -0.00665 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 8.96e+03 |\n",
      "|    reward             | 64.14664 |\n",
      "|    std                | 0.99     |\n",
      "|    value_loss         | 3.97e+05 |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 61           |\n",
      "|    iterations         | 700          |\n",
      "|    time_elapsed       | 57           |\n",
      "|    total_timesteps    | 3500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -42.3        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 699          |\n",
      "|    policy_loss        | -16.6        |\n",
      "|    reward             | -0.026873928 |\n",
      "|    std                | 0.992        |\n",
      "|    value_loss         | 0.59         |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 60       |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 65       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.3    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 81.5     |\n",
      "|    reward             | -4.96701 |\n",
      "|    std                | 0.992    |\n",
      "|    value_loss         | 5.52     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -72.9     |\n",
      "|    reward             | 3.1371887 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 5.91      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 82        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0.00195   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 229       |\n",
      "|    reward             | 17.750845 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 41.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 90         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 54.7       |\n",
      "|    reward             | -17.953651 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 3.77       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 99        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 661       |\n",
      "|    reward             | 10.158611 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 542       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 107       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 107       |\n",
      "|    reward             | 1.3620051 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 7.83      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 60          |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 115         |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.4       |\n",
      "|    explained_variance | -3.46       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -13.5       |\n",
      "|    reward             | -0.56362695 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 1.65        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 905       |\n",
      "|    reward             | -2.401951 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 565       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 132       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | -0.000301 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -25.9     |\n",
      "|    reward             | -16.9065  |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 52.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 140        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -81.2      |\n",
      "|    reward             | -27.679794 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 246        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 148       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -44.2     |\n",
      "|    reward             | 13.555995 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 549       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 157       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | -0.000107 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -46.1     |\n",
      "|    reward             | 1.3639776 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 1.64      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 165        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -158       |\n",
      "|    reward             | -1.6331007 |\n",
      "|    std                | 0.984      |\n",
      "|    value_loss         | 30.2       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-10-11 to  2023-01-10\n",
      "A2C Sharpe Ratio:  -0.0943664423779587\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_693_2\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 84        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 24        |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | -0.464039 |\n",
      "----------------------------------\n",
      "day: 3152, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2370760.18\n",
      "total_reward: 1370760.18\n",
      "total_cost: 2243860.49\n",
      "total_trades: 74677\n",
      "Sharpe: 0.522\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016496658 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.00302     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.89        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    reward               | 0.71008605  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 36.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 82         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 74         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02426703 |\n",
      "|    clip_fraction        | 0.248      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | 0.00772    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 34.1       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    reward               | -157.3697  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 128        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.012284005  |\n",
      "|    clip_fraction        | 0.18         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.8        |\n",
      "|    explained_variance   | 0.0263       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 87.5         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0133      |\n",
      "|    reward               | -0.087677754 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 182          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 80         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 127        |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02276186 |\n",
      "|    clip_fraction        | 0.219      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.9      |\n",
      "|    explained_variance   | 0.0764     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 171        |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    reward               | -1.8862967 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 42.7       |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2022-10-11 to  2023-01-10\n",
      "PPO Sharpe Ratio:  -0.00948740869300398\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_693_2\n",
      "day: 3152, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11693256.40\n",
      "total_reward: 10693256.40\n",
      "total_cost: 999.00\n",
      "total_trades: 44039\n",
      "Sharpe: 0.404\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 46        |\n",
      "|    time_elapsed    | 273       |\n",
      "|    total_timesteps | 12612     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.91     |\n",
      "|    critic_loss     | 123       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9459      |\n",
      "|    reward          | -2.192436 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2022-10-11 to  2023-01-10\n",
      "======Best Model Retraining from:  2010-01-01 to  2023-01-10\n",
      "======Trading from:  2023-01-10 to  2023-04-17\n",
      "Ensemble Strategy took:  94.8919874072075  minutes\n"
     ]
    }
   ],
   "source": [
    "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
    "                                                 PPO_model_kwargs,\n",
    "                                                 DDPG_model_kwargs,\n",
    "                                                 timesteps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c46f5240-ba04-4bc9-ac69-80c4719b7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_trade_date = processed[(processed.date > TRADE_START_DATE)&(processed.date <= TRADE_END_DATE)].date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86c8f748-0961-4fc5-82ba-5268f76f8ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  0.7938978448725033\n"
     ]
    }
   ],
   "source": [
    "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
    "\n",
    "df_account_value=pd.DataFrame()\n",
    "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
    "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
    "    df_account_value = pd.concat([df_account_value, temp], ignore_index=True)\n",
    "sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
    "print('Sharpe Ratio: ',sharpe)\n",
    "df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6e160d2-152d-4f42-8ccd-4abb407f869c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_value</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>datadate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>1.537268e+06</td>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>2023-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>1.536244e+06</td>\n",
       "      <td>2023-04-10</td>\n",
       "      <td>-0.000666</td>\n",
       "      <td>2023-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>1.540735e+06</td>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>2023-04-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>1.542060e+06</td>\n",
       "      <td>2023-04-12</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>2023-04-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>1.545561e+06</td>\n",
       "      <td>2023-04-13</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>2023-04-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     account_value        date  daily_return    datadate\n",
       "625   1.537268e+06  2023-04-06      0.001009  2023-04-06\n",
       "626   1.536244e+06  2023-04-10     -0.000666  2023-04-10\n",
       "627   1.540735e+06  2023-04-11      0.002923  2023-04-11\n",
       "628   1.542060e+06  2023-04-12      0.000860  2023-04-12\n",
       "629   1.545561e+06  2023-04-13      0.002271  2023-04-13"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "973c8fb6-7e44-4e6b-96ed-eec8788c93cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df_account_value.account_value.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b8a4f8b-82b5-4709-b1f6-fdb65983f458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pyfolio\\pos.py:26: UserWarning: Module \"zipline.assets\" not found; mutltipliers will not be applied to position notionals.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return          0.190240\n",
      "Cumulative returns     0.545561\n",
      "Annual volatility      6.464552\n",
      "Sharpe ratio           0.793898\n",
      "Calmar ratio           0.210522\n",
      "Stability              0.203475\n",
      "Max drawdown          -0.903656\n",
      "Omega ratio            3.925517\n",
      "Sortino ratio          6.479652\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.043498\n",
      "Daily value at risk   -0.794091\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from finrl.plot import backtest_stats\n",
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7c4b99-a6e4-4092-9d77-3ed9712c5300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
