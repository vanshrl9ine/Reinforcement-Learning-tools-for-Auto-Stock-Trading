{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc709d59-3ab6-401e-a803-4ff31993ffb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\exchange_calendars\\exchange_calendar.py:2347: FutureWarning: 'T' is deprecated and will be removed in a future version. Please use 'min' instead of 'T'.\n",
      "  align: pd.Timedelta | str = pd.Timedelta(1, \"T\"),\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "\n",
    "from finrl import config\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import data_split\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09ec60f-d1f4-4b21-97a6-60fe0c7cc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = '2010-01-01'\n",
    "TRAIN_END_DATE = '2020-07-01'\n",
    "TRADE_START_DATE = '2020-07-01'\n",
    "TRADE_END_DATE = '2023-05-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b33b42-1022-48ba-9bb4-724b1fa70c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw=pd.read_csv('datasets/BSE30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85c5e44b-1124-4426-87de-925b8c8fff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprocessing import FeatureEngineer, load_dataset, data_split, convert_to_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "febd40c0-9d74-44a1-9fcf-a3b6f7b7bbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\Downloads\\BTP-2-4-24\\dataprocessing.py:236: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  df_price_pivot = df_price_pivot.pct_change()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "from finrl.config import INDICATORS\n",
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                      tech_indicator_list = INDICATORS,\n",
    "                      use_vix=False,\n",
    "                      use_turbulence=True,\n",
    "                      user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "083563ce-5668-49e3-abed-4c42a4bfa9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>88.550003</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>19140</td>\n",
       "      <td>ASIANPAINT.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>48.861801</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>105.800003</td>\n",
       "      <td>109.599998</td>\n",
       "      <td>103.459999</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>4536215</td>\n",
       "      <td>AXISBANK.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>71.914917</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>206.050003</td>\n",
       "      <td>210.500000</td>\n",
       "      <td>196.500000</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>52648</td>\n",
       "      <td>BAJAJ-AUTO.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>158.413025</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>15.140000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>14.975000</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>136590</td>\n",
       "      <td>BAJAJFINSV.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>13.401811</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>6.660000</td>\n",
       "      <td>6.970000</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>274220</td>\n",
       "      <td>BAJFINANCE.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.523346</td>\n",
       "      <td>48.068260</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>2.746401</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105698</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>981.000000</td>\n",
       "      <td>992.500000</td>\n",
       "      <td>979.250000</td>\n",
       "      <td>986.799988</td>\n",
       "      <td>26056</td>\n",
       "      <td>SUNPHARMA.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.263414</td>\n",
       "      <td>1019.314408</td>\n",
       "      <td>965.265603</td>\n",
       "      <td>50.085294</td>\n",
       "      <td>14.481255</td>\n",
       "      <td>1.567920</td>\n",
       "      <td>983.446670</td>\n",
       "      <td>985.046100</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105699</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>3208.000000</td>\n",
       "      <td>3227.199951</td>\n",
       "      <td>3197.149902</td>\n",
       "      <td>3175.769043</td>\n",
       "      <td>51644</td>\n",
       "      <td>TCS.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>-15.398183</td>\n",
       "      <td>3235.633708</td>\n",
       "      <td>3045.249324</td>\n",
       "      <td>48.649310</td>\n",
       "      <td>67.966063</td>\n",
       "      <td>0.407494</td>\n",
       "      <td>3131.238102</td>\n",
       "      <td>3257.234477</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105700</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>983.000000</td>\n",
       "      <td>1026.650024</td>\n",
       "      <td>982.950012</td>\n",
       "      <td>986.955139</td>\n",
       "      <td>279514</td>\n",
       "      <td>TECHM.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>-22.941437</td>\n",
       "      <td>1102.074200</td>\n",
       "      <td>929.293964</td>\n",
       "      <td>44.970681</td>\n",
       "      <td>-99.119890</td>\n",
       "      <td>22.233939</td>\n",
       "      <td>1033.226742</td>\n",
       "      <td>1032.633037</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105701</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>2663.500000</td>\n",
       "      <td>2679.300049</td>\n",
       "      <td>2620.050049</td>\n",
       "      <td>2640.399902</td>\n",
       "      <td>32742</td>\n",
       "      <td>TITAN.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>43.161331</td>\n",
       "      <td>2669.834479</td>\n",
       "      <td>2492.325506</td>\n",
       "      <td>60.306098</td>\n",
       "      <td>116.653875</td>\n",
       "      <td>37.463255</td>\n",
       "      <td>2542.391650</td>\n",
       "      <td>2482.099988</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105702</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>7524.950195</td>\n",
       "      <td>7576.549805</td>\n",
       "      <td>7487.549805</td>\n",
       "      <td>7520.226562</td>\n",
       "      <td>8316</td>\n",
       "      <td>ULTRACEMCO.BO</td>\n",
       "      <td>4</td>\n",
       "      <td>28.692164</td>\n",
       "      <td>7774.630825</td>\n",
       "      <td>7303.703648</td>\n",
       "      <td>55.649892</td>\n",
       "      <td>27.765845</td>\n",
       "      <td>3.677975</td>\n",
       "      <td>7441.862826</td>\n",
       "      <td>7301.539185</td>\n",
       "      <td>43.069415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105703 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date         open         high          low        close  \\\n",
       "0       2009-01-02    90.750000    90.750000    88.550003    48.861801   \n",
       "1       2009-01-02   105.800003   109.599998   103.459999    71.914917   \n",
       "2       2009-01-02   206.050003   210.500000   196.500000   158.413025   \n",
       "3       2009-01-02    15.140000    15.800000    14.975000    13.401811   \n",
       "4       2009-01-02     6.660000     6.970000     6.350000     2.746401   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "105698  2023-04-28   981.000000   992.500000   979.250000   986.799988   \n",
       "105699  2023-04-28  3208.000000  3227.199951  3197.149902  3175.769043   \n",
       "105700  2023-04-28   983.000000  1026.650024   982.950012   986.955139   \n",
       "105701  2023-04-28  2663.500000  2679.300049  2620.050049  2640.399902   \n",
       "105702  2023-04-28  7524.950195  7576.549805  7487.549805  7520.226562   \n",
       "\n",
       "         volume            tic  day       macd      boll_ub      boll_lb  \\\n",
       "0         19140  ASIANPAINT.BO    4   0.000000    50.523346    48.068260   \n",
       "1       4536215    AXISBANK.BO    4   0.000000    50.523346    48.068260   \n",
       "2         52648  BAJAJ-AUTO.BO    4   0.000000    50.523346    48.068260   \n",
       "3        136590  BAJAJFINSV.BO    4   0.000000    50.523346    48.068260   \n",
       "4        274220  BAJFINANCE.BO    4   0.000000    50.523346    48.068260   \n",
       "...         ...            ...  ...        ...          ...          ...   \n",
       "105698    26056   SUNPHARMA.BO    4  -0.263414  1019.314408   965.265603   \n",
       "105699    51644         TCS.BO    4 -15.398183  3235.633708  3045.249324   \n",
       "105700   279514       TECHM.BO    4 -22.941437  1102.074200   929.293964   \n",
       "105701    32742       TITAN.BO    4  43.161331  2669.834479  2492.325506   \n",
       "105702     8316  ULTRACEMCO.BO    4  28.692164  7774.630825  7303.703648   \n",
       "\n",
       "            rsi_30      cci_30       dx_30  close_30_sma  close_60_sma  \\\n",
       "0       100.000000   66.666667  100.000000     48.861801     48.861801   \n",
       "1       100.000000   66.666667  100.000000     71.914917     71.914917   \n",
       "2       100.000000   66.666667  100.000000    158.413025    158.413025   \n",
       "3       100.000000   66.666667  100.000000     13.401811     13.401811   \n",
       "4       100.000000   66.666667  100.000000      2.746401      2.746401   \n",
       "...            ...         ...         ...           ...           ...   \n",
       "105698   50.085294   14.481255    1.567920    983.446670    985.046100   \n",
       "105699   48.649310   67.966063    0.407494   3131.238102   3257.234477   \n",
       "105700   44.970681  -99.119890   22.233939   1033.226742   1032.633037   \n",
       "105701   60.306098  116.653875   37.463255   2542.391650   2482.099988   \n",
       "105702   55.649892   27.765845    3.677975   7441.862826   7301.539185   \n",
       "\n",
       "        turbulence  \n",
       "0         0.000000  \n",
       "1         0.000000  \n",
       "2         0.000000  \n",
       "3         0.000000  \n",
       "4         0.000000  \n",
       "...            ...  \n",
       "105698   43.069415  \n",
       "105699   43.069415  \n",
       "105700   43.069415  \n",
       "105701   43.069415  \n",
       "105702   43.069415  \n",
       "\n",
       "[105703 rows x 17 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b299a545-307a-4d24-a530-9c7a1e0e84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b45789e-5f8a-4a0d-a1ba-070d4601b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ticker = df[\"tic\"].unique().tolist()\n",
    "# only apply to daily level data, need to fix for minute level\n",
    "list_date = list(pd.date_range(df['date'].min(),df['date'].max()).astype(str))\n",
    "combination = list(itertools.product(list_date,list_ticker))\n",
    "\n",
    "df_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(df,on=[\"date\",\"tic\"],how=\"left\")\n",
    "df_full = df_full[df_full['date'].isin(df['date'])]\n",
    "df_full = df_full.sort_values(['date','tic'])\n",
    "df_full = df_full.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f2577b7-eabd-4f2c-a467-cb5584c94aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d8ef628-f80f-4d6d-9a2f-b626ad01064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TradingEnv import StockTradingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28d66621-48cf-40ea-b98d-5b262d41fa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77550\n",
      "21120\n"
     ]
    }
   ],
   "source": [
    "train = data_split(df, TRAIN_START_DATE,TRAIN_END_DATE)\n",
    "trade = data_split(df, TRADE_START_DATE,TRADE_END_DATE)\n",
    "print(len(train))\n",
    "print(len(trade))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76538de6-2554-4a7f-b69b-7f1303363465",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_data.csv')\n",
    "trade.to_csv('trade_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db88fde3-3088-471d-a275-f30f1c22f7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 30, State Space: 301\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "490ac2e0-422b-4f0d-9d59-c7a13738bb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>ASIANPAINT.BO</td>\n",
       "      <td>173.800003</td>\n",
       "      <td>179.990005</td>\n",
       "      <td>173.800003</td>\n",
       "      <td>113.311302</td>\n",
       "      <td>26700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.051069</td>\n",
       "      <td>115.089549</td>\n",
       "      <td>104.640905</td>\n",
       "      <td>66.436248</td>\n",
       "      <td>113.218646</td>\n",
       "      <td>24.458130</td>\n",
       "      <td>108.824595</td>\n",
       "      <td>104.144379</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>AXISBANK.BO</td>\n",
       "      <td>199.800003</td>\n",
       "      <td>199.800003</td>\n",
       "      <td>197.600006</td>\n",
       "      <td>142.226456</td>\n",
       "      <td>658270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.338474</td>\n",
       "      <td>151.029051</td>\n",
       "      <td>132.164238</td>\n",
       "      <td>52.578761</td>\n",
       "      <td>-1.919545</td>\n",
       "      <td>2.331440</td>\n",
       "      <td>142.339130</td>\n",
       "      <td>140.995590</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>BAJAJ-AUTO.BO</td>\n",
       "      <td>885.000000</td>\n",
       "      <td>886.000000</td>\n",
       "      <td>865.000000</td>\n",
       "      <td>687.819336</td>\n",
       "      <td>71150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.282086</td>\n",
       "      <td>707.370571</td>\n",
       "      <td>641.819761</td>\n",
       "      <td>61.119288</td>\n",
       "      <td>90.337046</td>\n",
       "      <td>15.068862</td>\n",
       "      <td>655.815855</td>\n",
       "      <td>622.730791</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>BAJAJFINSV.BO</td>\n",
       "      <td>34.900002</td>\n",
       "      <td>36.080002</td>\n",
       "      <td>34.799999</td>\n",
       "      <td>31.331820</td>\n",
       "      <td>1119010.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.473058</td>\n",
       "      <td>30.449640</td>\n",
       "      <td>27.363217</td>\n",
       "      <td>60.736230</td>\n",
       "      <td>321.150339</td>\n",
       "      <td>46.461213</td>\n",
       "      <td>28.700716</td>\n",
       "      <td>28.163338</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>BAJFINANCE.BO</td>\n",
       "      <td>33.270000</td>\n",
       "      <td>34.389999</td>\n",
       "      <td>33.270000</td>\n",
       "      <td>16.682077</td>\n",
       "      <td>221680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.568160</td>\n",
       "      <td>16.397749</td>\n",
       "      <td>14.096924</td>\n",
       "      <td>68.269383</td>\n",
       "      <td>212.436642</td>\n",
       "      <td>27.581496</td>\n",
       "      <td>14.887369</td>\n",
       "      <td>14.188843</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date            tic        open        high         low       close  \\\n",
       "0  2010-01-04  ASIANPAINT.BO  173.800003  179.990005  173.800003  113.311302   \n",
       "0  2010-01-04    AXISBANK.BO  199.800003  199.800003  197.600006  142.226456   \n",
       "0  2010-01-04  BAJAJ-AUTO.BO  885.000000  886.000000  865.000000  687.819336   \n",
       "0  2010-01-04  BAJAJFINSV.BO   34.900002   36.080002   34.799999   31.331820   \n",
       "0  2010-01-04  BAJFINANCE.BO   33.270000   34.389999   33.270000   16.682077   \n",
       "\n",
       "      volume  day       macd     boll_ub     boll_lb     rsi_30      cci_30  \\\n",
       "0    26700.0  0.0   2.051069  115.089549  104.640905  66.436248  113.218646   \n",
       "0   658270.0  0.0  -0.338474  151.029051  132.164238  52.578761   -1.919545   \n",
       "0    71150.0  0.0  16.282086  707.370571  641.819761  61.119288   90.337046   \n",
       "0  1119010.0  0.0   0.473058   30.449640   27.363217  60.736230  321.150339   \n",
       "0   221680.0  0.0   0.568160   16.397749   14.096924  68.269383  212.436642   \n",
       "\n",
       "       dx_30  close_30_sma  close_60_sma  turbulence  \n",
       "0  24.458130    108.824595    104.144379         0.0  \n",
       "0   2.331440    142.339130    140.995590         0.0  \n",
       "0  15.068862    655.815855    622.730791         0.0  \n",
       "0  46.461213     28.700716     28.163338         0.0  \n",
       "0  27.581496     14.887369     14.188843         0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b19bfb4-2266-4356-a0a8-c8ef45fadb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"buy_cost_pct\": 0.001, \n",
    "    \"sell_cost_pct\": 0.001, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\":5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33775ce8-4afc-41e3-b742-f3aff0dadbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl.agents.stablebaselines3.models import DRLEnsembleAgent\n",
    "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
    "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
    "\n",
    "ensemble_agent = DRLEnsembleAgent(df=df,\n",
    "                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
    "                 val_test_period=(TRADE_START_DATE,TRADE_END_DATE),\n",
    "                 rebalance_window=rebalance_window, \n",
    "                 validation_window=validation_window, \n",
    "                 **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b3af798-f792-47af-87e9-b89db8fcffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\n",
    "                    'n_steps': 5,\n",
    "                    'ent_coef': 0.005,\n",
    "                    'learning_rate': 0.0007\n",
    "                    }\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "                    \"ent_coef\":0.01,\n",
    "                    \"n_steps\": 2048,\n",
    "                    \"learning_rate\": 0.00025,\n",
    "                    \"batch_size\": 128\n",
    "                    }\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
    "                      \"buffer_size\": 10_000,\n",
    "                      \"learning_rate\": 0.0005,\n",
    "                      \"batch_size\": 64\n",
    "                    }\n",
    "\n",
    "timesteps_dict = {'a2c' : 10_000, \n",
    "                 'ppo' : 10_000, \n",
    "                 'ddpg' : 10_000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "031defd6-1140-410b-b988-1edbdc7ab52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2020-07-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to tensorboard_log/a2c\\a2c_126_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -0.00221  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -36.8     |\n",
      "|    reward             | 1.6071275 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.1       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 57         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -55        |\n",
      "|    reward             | 0.14525886 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.3        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 57        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 199       |\n",
      "|    reward             | 5.3355803 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 43.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 58       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -252     |\n",
      "|    reward             | 331.8605 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 46.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 58        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 23.8      |\n",
      "|    reward             | -7.077574 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 10.6      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 57          |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 51          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.8       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -108        |\n",
      "|    reward             | -0.46563184 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 9.29        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 57        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 60        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -328      |\n",
      "|    reward             | 1.8439615 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 144       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 57         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 1.03e+03   |\n",
      "|    reward             | -17.913773 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 605        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 57         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 78         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -1.36e+03  |\n",
      "|    reward             | -1.1555426 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.42e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 57         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 86         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 1.24e+03   |\n",
      "|    reward             | -25.171038 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.07e+03   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 57          |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 95          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -60         |\n",
      "|    reward             | 0.025169967 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 3.38        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 57       |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 103      |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 140      |\n",
      "|    reward             | 7.612689 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 11.7     |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 57           |\n",
      "|    iterations         | 1300         |\n",
      "|    time_elapsed       | 112          |\n",
      "|    total_timesteps    | 6500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -42.8        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1299         |\n",
      "|    policy_loss        | 413          |\n",
      "|    reward             | -0.027389308 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 148          |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 57        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 121       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -121      |\n",
      "|    reward             | 3.4979272 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 133       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 57        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 129       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 602       |\n",
      "|    reward             | 0.9905888 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 550       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 57        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 138       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 32.7      |\n",
      "|    reward             | 5.0694947 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.45      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 57        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 147       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 262       |\n",
      "|    reward             | 2.1003406 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 46.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 57        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 155       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -96       |\n",
      "|    reward             | 14.481445 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 52.7      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 57       |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 164      |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43      |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 717      |\n",
      "|    reward             | 5.897514 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 289      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 57        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 173       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -94.9     |\n",
      "|    reward             | 14.431122 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 41.4      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2020-07-02 to  2020-09-29\n",
      "A2C Sharpe Ratio:  0.25165097218950483\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_126_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 75        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 27        |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.0837311 |\n",
      "----------------------------------\n",
      "day: 2585, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1934596.33\n",
      "total_reward: 934596.33\n",
      "total_cost: 1591972.43\n",
      "total_trades: 61990\n",
      "Sharpe: 0.398\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012355056 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.0113     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.67        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    reward               | 4.312203    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 43.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024267972 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | -0.00716    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.5        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    reward               | -0.6842561  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 63.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 73         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 110        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01630703 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | 0.00889    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 352        |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    reward               | 1.6862917  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 165        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018567793 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0348      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.5        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    reward               | -2.0040514  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 86.7        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-07-02 to  2020-09-29\n",
      "PPO Sharpe Ratio:  0.24022660467018858\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_126_1\n",
      "day: 2585, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4593291.00\n",
      "total_reward: 3593291.00\n",
      "total_cost: 998.98\n",
      "total_trades: 41303\n",
      "Sharpe: 0.428\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 40         |\n",
      "|    time_elapsed    | 253        |\n",
      "|    total_timesteps | 10344      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 77.3       |\n",
      "|    critic_loss     | 61.8       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 7758       |\n",
      "|    reward          | 0.44953245 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2020-07-02 to  2020-09-29\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-09-29\n",
      "======Trading from:  2020-09-29 to  2020-12-30\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2020-09-29\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_189_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -88.8      |\n",
      "|    reward             | 0.31931126 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.38       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 4.14      |\n",
      "|    reward             | 1.7949817 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.61      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -0.353    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 17.2      |\n",
      "|    reward             | -1.342904 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.24      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 55         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 36         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -179       |\n",
      "|    reward             | 0.15376103 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 34.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 55         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 45         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 283        |\n",
      "|    reward             | -14.932652 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 153        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -113      |\n",
      "|    reward             | 0.7861252 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 6.98      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 62       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -25.2    |\n",
      "|    reward             | 0.405645 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 0.732    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 72       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -417     |\n",
      "|    reward             | -6.53529 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 140      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 80        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -178      |\n",
      "|    reward             | 3.8364367 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 38.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 89        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -697      |\n",
      "|    reward             | 11.905766 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 258       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 98       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 36.8     |\n",
      "|    reward             | 6.995706 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 2.6      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 107       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 70.2      |\n",
      "|    reward             | 2.0116763 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.28      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 55          |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 116         |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 61.5        |\n",
      "|    reward             | -0.42780867 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 10.5        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 55         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 125        |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -151       |\n",
      "|    reward             | -1.4655076 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 21.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 55         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 134        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 1.03e+03   |\n",
      "|    reward             | -6.0998225 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 581        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 55         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 143        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -29.8      |\n",
      "|    reward             | -0.6796274 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.527      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 55         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 152        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -76.7      |\n",
      "|    reward             | 0.87708515 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.1        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 161       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -44.9     |\n",
      "|    reward             | 1.3424544 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.32      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 55          |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 170         |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 443         |\n",
      "|    reward             | -0.62525636 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 131         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 179       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 422       |\n",
      "|    reward             | 1.6110531 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 138       |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2020-09-29 to  2020-12-30\n",
      "A2C Sharpe Ratio:  0.5383936159551725\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_189_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 76        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 26        |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.2654576 |\n",
      "----------------------------------\n",
      "day: 2648, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2527889.11\n",
      "total_reward: 1527889.11\n",
      "total_cost: 1851222.20\n",
      "total_trades: 65054\n",
      "Sharpe: 0.490\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 73         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 55         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01610582 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.6      |\n",
      "|    explained_variance   | -0.00144   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 11.8       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0202    |\n",
      "|    reward               | -1.5425512 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 113        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020490434 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.00428    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.7        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    reward               | 1.6994146   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 78.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 110         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024162743 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0208      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    reward               | 0.7190209   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 96.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 138         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016934806 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.00887     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 27.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00698    |\n",
      "|    reward               | -0.8442609  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 207         |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-09-29 to  2020-12-30\n",
      "PPO Sharpe Ratio:  0.42692932346651585\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_189_1\n",
      "day: 2648, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6479485.58\n",
      "total_reward: 5479485.58\n",
      "total_cost: 1078.23\n",
      "total_trades: 26455\n",
      "Sharpe: 0.326\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 38        |\n",
      "|    time_elapsed    | 275       |\n",
      "|    total_timesteps | 10596     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 18.2      |\n",
      "|    critic_loss     | 59.7      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 7947      |\n",
      "|    reward          | 13.643758 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2020-09-29 to  2020-12-30\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-12-30\n",
      "======Trading from:  2020-12-30 to  2021-04-01\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2020-12-30\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_252_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 55         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -61.6      |\n",
      "|    reward             | 0.20128012 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 2.2        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -58.7     |\n",
      "|    reward             | 1.1881437 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 3.11      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -2.92    |\n",
      "|    reward             | 1.483107 |\n",
      "|    std                | 0.992    |\n",
      "|    value_loss         | 0.889    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 55        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -24.9     |\n",
      "|    reward             | 215.75838 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 1.98      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 54         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 45         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0.0337     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 54.5       |\n",
      "|    reward             | -17.082232 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 24         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 55         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 54         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 171        |\n",
      "|    reward             | -2.1828513 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 20.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 54         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 63         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | -0.618     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -90.2      |\n",
      "|    reward             | -2.6856427 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 6.06       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 54         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 73         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0.00694    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -198       |\n",
      "|    reward             | 0.14137769 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 25         |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 54          |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 82          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | 31.1        |\n",
      "|    reward             | -0.26174554 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 3.36        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 92        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 156       |\n",
      "|    reward             | 5.0133023 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 67.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 54         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 101        |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -26.6      |\n",
      "|    reward             | -2.2449012 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 2.04       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 110       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 181       |\n",
      "|    reward             | 2.3346345 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 21        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 120       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -90.2     |\n",
      "|    reward             | 0.6710161 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 7.26      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 129       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -182      |\n",
      "|    reward             | 6.677651  |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 29.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 138       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.00315  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 34.8      |\n",
      "|    reward             | 12.084345 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 25.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 148       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -376      |\n",
      "|    reward             | 18.510084 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 132       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 53          |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 157         |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 154         |\n",
      "|    reward             | -0.49017942 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 14.5        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 53          |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 167         |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -43         |\n",
      "|    reward             | -0.72321004 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 3.25        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 176       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 378       |\n",
      "|    reward             | 1.9582368 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 92.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 185       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.0324   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 73        |\n",
      "|    reward             | -5.271722 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 4.16      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2020-12-30 to  2021-04-01\n",
      "A2C Sharpe Ratio:  -0.09996188541661505\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_252_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 70         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 28         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | -0.5379672 |\n",
      "-----------------------------------\n",
      "day: 2711, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2128138.15\n",
      "total_reward: 1128138.15\n",
      "total_cost: 1754832.18\n",
      "total_trades: 65155\n",
      "Sharpe: 0.476\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015290607 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.00345    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.2        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    reward               | -0.2601853  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 46.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01337533  |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.013       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    reward               | -0.25621098 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 66.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018844545 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0346      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 120         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | 1.0998102   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 88.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 146         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019185692 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.037       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20.5        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    reward               | 1.910749    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 115         |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-12-30 to  2021-04-01\n",
      "PPO Sharpe Ratio:  0.14149659022250993\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_252_1\n",
      "day: 2711, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7200485.68\n",
      "total_reward: 6200485.68\n",
      "total_cost: 999.00\n",
      "total_trades: 37890\n",
      "Sharpe: 0.575\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 38        |\n",
      "|    time_elapsed    | 281       |\n",
      "|    total_timesteps | 10848     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.06     |\n",
      "|    critic_loss     | 87.2      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 8136      |\n",
      "|    reward          | 2.0607502 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2020-12-30 to  2021-04-01\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-04-01\n",
      "======Trading from:  2021-04-01 to  2021-07-05\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2021-04-01\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_315_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -144      |\n",
      "|    reward             | 1.4038739 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 13.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -53.3     |\n",
      "|    reward             | 0.7292529 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.35      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 53         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 168        |\n",
      "|    reward             | -1.5137783 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 15.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -162      |\n",
      "|    reward             | 174.23396 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 18.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 54         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0.0329     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -108       |\n",
      "|    reward             | -3.3737407 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 13.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0.000326  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -145      |\n",
      "|    reward             | -2.869551 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 16.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 54         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 64         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 25.6       |\n",
      "|    reward             | -1.0992576 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.21       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -173      |\n",
      "|    reward             | 1.7963065 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 23.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 83       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -70.3    |\n",
      "|    reward             | 2.022036 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 6.47     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 92        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 177       |\n",
      "|    reward             | 1.1577439 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 24.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 53         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 102        |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 393        |\n",
      "|    reward             | -1.5068198 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 107        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 53          |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 111         |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 73.2        |\n",
      "|    reward             | -0.14891662 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 7.84        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 53         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 120        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 272        |\n",
      "|    reward             | -1.0222063 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 62.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 53         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 129        |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -86        |\n",
      "|    reward             | -2.2732213 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.17       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 139      |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 183      |\n",
      "|    reward             | -7.79608 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 48.6     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 149       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -556      |\n",
      "|    reward             | 5.2707963 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 165       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 159       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 67.4      |\n",
      "|    reward             | 1.4770734 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.67      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 168       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 62.1      |\n",
      "|    reward             | 3.3130126 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 2.69      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 178       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -184      |\n",
      "|    reward             | -2.509172 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 31.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 53         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 188        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 286        |\n",
      "|    reward             | -6.5254173 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 46.3       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-04-01 to  2021-07-05\n",
      "A2C Sharpe Ratio:  0.1642183241359983\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_315_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 64         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 31         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.32397297 |\n",
      "-----------------------------------\n",
      "day: 2774, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2570575.15\n",
      "total_reward: 1570575.15\n",
      "total_cost: 1744849.56\n",
      "total_trades: 66065\n",
      "Sharpe: 0.321\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 64          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015526863 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.021      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.98        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    reward               | 0.8818422   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 44.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 64          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016384866 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | -0.0103     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    reward               | -0.63090545 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 61.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 66          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 124         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017682938 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | -0.000863   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.14        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    reward               | 5.60085     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 37.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 67          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 152         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013286924 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0164      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 142         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    reward               | 1.3425184   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 119         |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-04-01 to  2021-07-05\n",
      "PPO Sharpe Ratio:  0.34447069579066986\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_315_1\n",
      "day: 2774, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6707497.86\n",
      "total_reward: 5707497.86\n",
      "total_cost: 999.00\n",
      "total_trades: 38764\n",
      "Sharpe: 0.336\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 39         |\n",
      "|    time_elapsed    | 280        |\n",
      "|    total_timesteps | 11100      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 86.2       |\n",
      "|    critic_loss     | 32.7       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 8325       |\n",
      "|    reward          | -2.6122735 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2021-04-01 to  2021-07-05\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-07-05\n",
      "======Trading from:  2021-07-05 to  2021-10-05\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2021-07-05\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_378_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 53          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0.0103      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -29.3       |\n",
      "|    reward             | 0.058217335 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.28        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 238       |\n",
      "|    reward             | 7.5050287 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 32        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 52         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 395        |\n",
      "|    reward             | -1.4632522 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 85.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0.0165    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -178      |\n",
      "|    reward             | 281.35574 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 25.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 52         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 116        |\n",
      "|    reward             | -33.957726 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 104        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 52         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 56         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 57.9       |\n",
      "|    reward             | -1.2291294 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.79       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 52         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 66         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -24.4      |\n",
      "|    reward             | -0.9854935 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.69       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 156       |\n",
      "|    reward             | -2.964042 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 21.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 84        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 29.1      |\n",
      "|    reward             | 2.4049675 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 10.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 53         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 94         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -279       |\n",
      "|    reward             | -14.230361 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 63         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 52         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 103        |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -121       |\n",
      "|    reward             | 0.47079644 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 34.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 4.17e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -146      |\n",
      "|    reward             | 4.7409687 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 14.4      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 53           |\n",
      "|    iterations         | 1300         |\n",
      "|    time_elapsed       | 122          |\n",
      "|    total_timesteps    | 6500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -42.8        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1299         |\n",
      "|    policy_loss        | -55.7        |\n",
      "|    reward             | -0.062335297 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 3.53         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 132       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 64.4      |\n",
      "|    reward             | 3.3580866 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 9.7       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 52       |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 141      |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -14.8    |\n",
      "|    reward             | 2.953166 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 27.4     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 52         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 150        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 81.4       |\n",
      "|    reward             | -3.8894873 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 49.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 53         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 160        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 208        |\n",
      "|    reward             | -5.6713133 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 129        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 169       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -69.6     |\n",
      "|    reward             | -4.300124 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.98      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 178       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -138      |\n",
      "|    reward             | 0.7192536 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 23.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 53         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 188        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -227       |\n",
      "|    reward             | -4.7377806 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 30.7       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-07-05 to  2021-10-05\n",
      "A2C Sharpe Ratio:  0.264048973469542\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_378_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 72         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 28         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.95428765 |\n",
      "-----------------------------------\n",
      "day: 2837, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1659705.39\n",
      "total_reward: 659705.39\n",
      "total_cost: 1469825.93\n",
      "total_trades: 64997\n",
      "Sharpe: 0.330\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013219982 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.0234     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14          |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    reward               | 0.09665438  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 24.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018991511 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.00643    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.07        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    reward               | -2.3766773  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 34.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015833737 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | -0.00234    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 38.1        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    reward               | -6.5223923  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 83.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 145         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027942881 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.000706    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.82        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    reward               | 2.835009    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 35.8        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-07-05 to  2021-10-05\n",
      "PPO Sharpe Ratio:  0.1950993407907368\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_378_1\n",
      "day: 2837, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9097251.56\n",
      "total_reward: 8097251.56\n",
      "total_cost: 1025.86\n",
      "total_trades: 39665\n",
      "Sharpe: 0.481\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 4           |\n",
      "|    fps             | 38          |\n",
      "|    time_elapsed    | 296         |\n",
      "|    total_timesteps | 11352       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | -80.4       |\n",
      "|    critic_loss     | 176         |\n",
      "|    learning_rate   | 0.0005      |\n",
      "|    n_updates       | 8514        |\n",
      "|    reward          | -0.01029816 |\n",
      "------------------------------------\n",
      "======DDPG Validation from:  2021-07-05 to  2021-10-05\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-10-05\n",
      "======Trading from:  2021-10-05 to  2022-01-05\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2021-10-05\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_441_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 52         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -64.7      |\n",
      "|    reward             | 0.13314308 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 2.6        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 52       |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -82.2    |\n",
      "|    reward             | 7.457152 |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 4.82     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 137       |\n",
      "|    reward             | 3.7932038 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 9.8       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -416      |\n",
      "|    reward             | 128.92102 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 108       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 130       |\n",
      "|    reward             | -11.26742 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 13.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -106      |\n",
      "|    reward             | 2.4049053 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 8.74      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 52         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 67         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -173       |\n",
      "|    reward             | 0.03935103 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 23.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 76        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -262      |\n",
      "|    reward             | 3.1875095 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 52.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 52         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 85         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 270        |\n",
      "|    reward             | -2.2342532 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 44.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 95        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -44.7     |\n",
      "|    reward             | 7.2079215 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 13.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 105       |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 142       |\n",
      "|    reward             | 3.5932188 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 113       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 114       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -47.9     |\n",
      "|    reward             | -2.055782 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 3.27      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 2.5       |\n",
      "|    reward             | 1.0124912 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 0.444     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 52         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 134        |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 594        |\n",
      "|    reward             | 0.71026564 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 209        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 144       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -317      |\n",
      "|    reward             | 1.1507658 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 109       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 51          |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 153         |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -416        |\n",
      "|    reward             | -0.58687437 |\n",
      "|    std                | 0.989       |\n",
      "|    value_loss         | 139         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 163       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 771       |\n",
      "|    reward             | 1.9749451 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 349       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 172       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 158       |\n",
      "|    reward             | -1.675435 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 17.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 52         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 182        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 164        |\n",
      "|    reward             | 0.27479658 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 18.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 192       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -260      |\n",
      "|    reward             | 1.9560362 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 40.9      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2021-10-05 to  2022-01-05\n",
      "A2C Sharpe Ratio:  -0.27452168615950523\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_441_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 72         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 28         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | -0.1315423 |\n",
      "-----------------------------------\n",
      "day: 2900, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3618599.32\n",
      "total_reward: 2618599.32\n",
      "total_cost: 2184109.18\n",
      "total_trades: 69832\n",
      "Sharpe: 0.502\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 71          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017439302 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.00218    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 44.8        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    reward               | 0.8859843   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 72.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017341387 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.00755     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    reward               | -0.2419695  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 110         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 69         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 118        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02417427 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | 0.0173     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 89.1       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    reward               | 1.7909513  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 102        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018333765 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.0187      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.2        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    reward               | -1.6260868  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 72.7        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-10-05 to  2022-01-05\n",
      "PPO Sharpe Ratio:  -0.012199382677831373\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_441_1\n",
      "day: 2900, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7189037.87\n",
      "total_reward: 6189037.87\n",
      "total_cost: 1179.76\n",
      "total_trades: 52132\n",
      "Sharpe: 0.491\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 299      |\n",
      "|    total_timesteps | 11604    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 76.2     |\n",
      "|    critic_loss     | 54.5     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 8703     |\n",
      "|    reward          | 8.77406  |\n",
      "---------------------------------\n",
      "======DDPG Validation from:  2021-10-05 to  2022-01-05\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-01-05\n",
      "======Trading from:  2022-01-05 to  2022-04-07\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2022-01-05\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_504_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 50          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -31.1       |\n",
      "|    reward             | -0.10632927 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.655       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 50         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 255        |\n",
      "|    reward             | -2.9159446 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 42.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 50         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 157        |\n",
      "|    reward             | -1.8209449 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 16.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 50        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -281      |\n",
      "|    reward             | 27.342943 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 61.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 51        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 206       |\n",
      "|    reward             | -9.772083 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 38.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 51          |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 58          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0.0289      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 47.5        |\n",
      "|    reward             | -0.36486137 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.88        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 51         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 68         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 74.4       |\n",
      "|    reward             | 0.88911223 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.3        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 51          |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 78          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 156         |\n",
      "|    reward             | -0.58061254 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 23.8        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 51         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -141       |\n",
      "|    reward             | -1.1065967 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 19.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 51         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 97         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -153       |\n",
      "|    reward             | -3.8366256 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 24.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 51         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 106        |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 1.33e+03   |\n",
      "|    reward             | -11.417377 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.37e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 51        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -101      |\n",
      "|    reward             | 1.4397159 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 7.43      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 51         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 125        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -140       |\n",
      "|    reward             | -1.8904514 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 12.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 51        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 135       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -7.51e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 234       |\n",
      "|    reward             | 1.7288307 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 43.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 51        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 144       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 485       |\n",
      "|    reward             | -8.436545 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 244       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 51        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 154       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -527      |\n",
      "|    reward             | 3.0824893 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 169       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 51        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 164       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 213       |\n",
      "|    reward             | -4.203855 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 82.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 173      |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 70.5     |\n",
      "|    reward             | 2.171893 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 4.94     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 51        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 183       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -112      |\n",
      "|    reward             | 0.3225206 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 9.27      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 51        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 193       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -201      |\n",
      "|    reward             | 1.4026031 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 29.1      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-01-05 to  2022-04-07\n",
      "A2C Sharpe Ratio:  0.2537826638361802\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_504_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 71         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 28         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | -0.8885333 |\n",
      "-----------------------------------\n",
      "day: 2963, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2427866.26\n",
      "total_reward: 1427866.26\n",
      "total_cost: 1932455.77\n",
      "total_trades: 69929\n",
      "Sharpe: 0.425\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015162752 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.0136     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 71.2        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    reward               | 0.2335662   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 39          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016922098 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.0138      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.48        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    reward               | -1.3905387  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 48.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 69         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 117        |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02538218 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | 0.0245     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 15         |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    reward               | 0.10133444 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 106        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013759941 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0296      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 94.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    reward               | 1.39261     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 47.5        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-01-05 to  2022-04-07\n",
      "PPO Sharpe Ratio:  0.2520880210322136\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_504_1\n",
      "day: 2963, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10223385.04\n",
      "total_reward: 9223385.04\n",
      "total_cost: 1052.26\n",
      "total_trades: 50284\n",
      "Sharpe: 0.444\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 37       |\n",
      "|    time_elapsed    | 315      |\n",
      "|    total_timesteps | 11856    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -89      |\n",
      "|    critic_loss     | 35       |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 8892     |\n",
      "|    reward          | 7.009075 |\n",
      "---------------------------------\n",
      "======DDPG Validation from:  2022-01-05 to  2022-04-07\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-04-07\n",
      "======Trading from:  2022-04-07 to  2022-07-08\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2022-04-07\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_567_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -98.1     |\n",
      "|    reward             | 0.3647933 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 5.64      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -92.8     |\n",
      "|    reward             | 1.6072967 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 8.67      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 49       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 48.6     |\n",
      "|    reward             | 2.095341 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 3.47     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -268      |\n",
      "|    reward             | 313.07605 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 44.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 49         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 50.2       |\n",
      "|    reward             | -6.0566297 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 6.37       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 60        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | -0.0179   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 4.85e+03  |\n",
      "|    reward             | 15.371579 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 7.49e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 49        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 70        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -200      |\n",
      "|    reward             | 1.6204175 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 24.4      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 51          |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 78          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -102        |\n",
      "|    reward             | -0.29403204 |\n",
      "|    std                | 0.993       |\n",
      "|    value_loss         | 11.6        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 253       |\n",
      "|    reward             | 1.3172455 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 44.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 53        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 94        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 74.5      |\n",
      "|    reward             | 4.886173  |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 9.65      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 102      |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -408     |\n",
      "|    reward             | 8.910319 |\n",
      "|    std                | 0.994    |\n",
      "|    value_loss         | 241      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 54         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 109        |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 169        |\n",
      "|    reward             | -16.985594 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 129        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 55         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 117        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | -1.16e-05  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -62.6      |\n",
      "|    reward             | -0.4902708 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 4.94       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 55         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 125        |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 76.8       |\n",
      "|    reward             | 0.61431843 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 4.31       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 56        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 132       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 24.9      |\n",
      "|    reward             | -5.913922 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 2.09      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 56         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 140        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -125       |\n",
      "|    reward             | -3.5460484 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 23         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 57         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 148        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 354        |\n",
      "|    reward             | -0.4648253 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 142        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 57        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 155       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -205      |\n",
      "|    reward             | 2.4466069 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 40.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 58        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 163       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 54.9      |\n",
      "|    reward             | 0.0324868 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 7.79      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 58        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 171       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 239       |\n",
      "|    reward             | -2.747151 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 32.3      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-04-07 to  2022-07-08\n",
      "A2C Sharpe Ratio:  0.25385533065385657\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_567_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 87        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 23        |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | -0.489376 |\n",
      "----------------------------------\n",
      "day: 3026, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2053399.30\n",
      "total_reward: 1053399.30\n",
      "total_cost: 2094090.82\n",
      "total_trades: 71528\n",
      "Sharpe: 0.370\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014895197 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.0117     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 50.2        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    reward               | -0.48745963 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 42.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013284797 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.000943    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.9        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    reward               | 0.64421296  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 77.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015090117 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0193      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 94.9        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    reward               | 0.45047775  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 134         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 117         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021443352 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.00243     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    reward               | -0.53110385 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 45.1        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-04-07 to  2022-07-08\n",
      "PPO Sharpe Ratio:  0.224105472818995\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_567_1\n",
      "day: 3026, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 14574238.07\n",
      "total_reward: 13574238.07\n",
      "total_cost: 1136.30\n",
      "total_trades: 39274\n",
      "Sharpe: 0.596\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 47        |\n",
      "|    time_elapsed    | 254       |\n",
      "|    total_timesteps | 12108     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 14.6      |\n",
      "|    critic_loss     | 61        |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9081      |\n",
      "|    reward          | -6.484133 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2022-04-07 to  2022-07-08\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-07-08\n",
      "======Trading from:  2022-07-08 to  2022-10-11\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2022-07-08\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_630_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -126       |\n",
      "|    reward             | 0.70934504 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 9.88       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -6.77      |\n",
      "|    reward             | 0.10460223 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.472      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 66        |\n",
      "|    reward             | -0.379389 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.04      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 62       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -132     |\n",
      "|    reward             | 5.536847 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 16.6     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 62         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 40         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -141       |\n",
      "|    reward             | -5.1451693 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 13.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 62        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 48        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -0.0228   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 2.03e+03  |\n",
      "|    reward             | 7.7953806 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.02e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 56         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | -0.107     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -72.7      |\n",
      "|    reward             | -1.7565569 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.31       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 64         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -58.6      |\n",
      "|    reward             | -1.5039443 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 12         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 72         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 63.4       |\n",
      "|    reward             | -7.0596037 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 46.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 61        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 80        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -183      |\n",
      "|    reward             | 3.3611846 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 20.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 61        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 88        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -121      |\n",
      "|    reward             | 3.4671195 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 76.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 61        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 96        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 163       |\n",
      "|    reward             | -6.396907 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 15.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 61        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 105       |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0.0259    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 57.1      |\n",
      "|    reward             | 2.5626996 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.48      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 61        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -11.2     |\n",
      "|    reward             | 1.1175597 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.744     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 61        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 122       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 622       |\n",
      "|    reward             | -4.921722 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 223       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 131        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 32.4       |\n",
      "|    reward             | -2.4317217 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.93       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 140       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 20.1      |\n",
      "|    reward             | 1.9905592 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.44      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 149        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 144        |\n",
      "|    reward             | -0.6511567 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 14.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 157       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -94.9     |\n",
      "|    reward             | 2.0091643 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 7.17      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 59       |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 166      |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 104      |\n",
      "|    reward             | 0.828916 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 11.2     |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-07-08 to  2022-10-11\n",
      "A2C Sharpe Ratio:  0.009803349891213458\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_630_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 84         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 24         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | -0.9878568 |\n",
      "-----------------------------------\n",
      "day: 3089, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3350610.73\n",
      "total_reward: 2350610.73\n",
      "total_cost: 2421559.40\n",
      "total_trades: 74243\n",
      "Sharpe: 0.507\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014817291 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.00313    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    reward               | 0.19743651  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 62.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023359489 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.00105     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 39.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    reward               | -4.7655506  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 195         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 82         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 99         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03871855 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | 0.015      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 17.4       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    reward               | 1.6556473  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 128        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 125         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019540418 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | -0.00307    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 15.2        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    reward               | 0.5070266   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 44          |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-07-08 to  2022-10-11\n",
      "PPO Sharpe Ratio:  0.18463641996388666\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_630_1\n",
      "day: 3089, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6912126.12\n",
      "total_reward: 5912126.12\n",
      "total_cost: 999.00\n",
      "total_trades: 55479\n",
      "Sharpe: 0.562\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 47        |\n",
      "|    time_elapsed    | 260       |\n",
      "|    total_timesteps | 12360     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 42.2      |\n",
      "|    critic_loss     | 85.2      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9270      |\n",
      "|    reward          | 1.6574949 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2022-07-08 to  2022-10-11\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-10-11\n",
      "======Trading from:  2022-10-11 to  2023-01-10\n",
      "============================================\n",
      "turbulence_threshold:  180.74924550975138\n",
      "======Model training from:  2010-01-01 to  2022-10-11\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_693_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 57          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -63.9       |\n",
      "|    reward             | -0.46497214 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 3.08        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 57        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 43.1      |\n",
      "|    reward             | 0.5410882 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.41      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 57         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 248        |\n",
      "|    reward             | 0.66870284 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 41.8       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 58       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | -0.00574 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -209     |\n",
      "|    reward             | 297.82   |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 40.7     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 58        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 301       |\n",
      "|    reward             | -33.07429 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 162       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 59        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -0.0136   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 6.58e+03  |\n",
      "|    reward             | 30.549044 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.11e+05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 59         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -90.9      |\n",
      "|    reward             | 0.32218453 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.05       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 59         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 67         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 70.7       |\n",
      "|    reward             | -3.8685527 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.38       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -21.2     |\n",
      "|    reward             | 2.6191115 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 2.94      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 82        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | -1.91e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 174       |\n",
      "|    reward             | 13.685346 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 25.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 91         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -2.23      |\n",
      "|    reward             | -14.121894 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 1.79       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 99        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 162       |\n",
      "|    reward             | 5.6672845 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 155       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 60         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 107        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 49.2       |\n",
      "|    reward             | 0.39571047 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 2.35       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 60          |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 115         |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -17.5       |\n",
      "|    reward             | -0.10758309 |\n",
      "|    std                | 0.992       |\n",
      "|    value_loss         | 1.92        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 60       |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 123      |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 376      |\n",
      "|    reward             | 3.298996 |\n",
      "|    std                | 0.991    |\n",
      "|    value_loss         | 104      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 131       |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 241       |\n",
      "|    reward             | -0.530397 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 63.4      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 60       |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 139      |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 93.5     |\n",
      "|    reward             | 0.52031  |\n",
      "|    std                | 0.991    |\n",
      "|    value_loss         | 7.89     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 60        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 147       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 222       |\n",
      "|    reward             | 6.8597293 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 114       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 155        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0.0185     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -2.64      |\n",
      "|    reward             | -0.7711496 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 0.237      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 163        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -27.1      |\n",
      "|    reward             | 0.03383513 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 2.23       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-10-11 to  2023-01-10\n",
      "A2C Sharpe Ratio:  0.031160302431972348\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_693_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 85         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 24         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | -1.6032629 |\n",
      "-----------------------------------\n",
      "day: 3152, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2878380.92\n",
      "total_reward: 1878380.92\n",
      "total_cost: 2514443.65\n",
      "total_trades: 76432\n",
      "Sharpe: 0.464\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013405504 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.0176     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 49.3        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    reward               | 0.58901966  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 68.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020882202 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | -0.0136     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 64.9        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    reward               | -214.8593   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 186         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022998441 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0045      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 250         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    reward               | -0.35048348 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 364         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022165548 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0158      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.49        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    reward               | -1.6585596  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 74.6        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-10-11 to  2023-01-10\n",
      "PPO Sharpe Ratio:  -0.22161539783122114\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_693_1\n",
      "day: 3152, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8914808.97\n",
      "total_reward: 7914808.97\n",
      "total_cost: 999.00\n",
      "total_trades: 50322\n",
      "Sharpe: 0.658\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 46         |\n",
      "|    time_elapsed    | 270        |\n",
      "|    total_timesteps | 12612      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 3.43       |\n",
      "|    critic_loss     | 38.7       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 9459       |\n",
      "|    reward          | -2.9277647 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2022-10-11 to  2023-01-10\n",
      "======Best Model Retraining from:  2010-01-01 to  2023-01-10\n",
      "======Trading from:  2023-01-10 to  2023-04-17\n",
      "Ensemble Strategy took:  107.30549987951915  minutes\n"
     ]
    }
   ],
   "source": [
    "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
    "                                                 PPO_model_kwargs,\n",
    "                                                 DDPG_model_kwargs,\n",
    "                                                 timesteps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c46f5240-ba04-4bc9-ac69-80c4719b7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_trade_date = processed[(processed.date > TRADE_START_DATE)&(processed.date <= TRADE_END_DATE)].date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86c8f748-0961-4fc5-82ba-5268f76f8ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  0.6568886906896435\n"
     ]
    }
   ],
   "source": [
    "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
    "\n",
    "df_account_value=pd.DataFrame()\n",
    "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
    "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
    "    df_account_value = pd.concat([df_account_value, temp], ignore_index=True)\n",
    "sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
    "print('Sharpe Ratio: ',sharpe)\n",
    "df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6e160d2-152d-4f42-8ccd-4abb407f869c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_value</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>datadate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>1.651971e+06</td>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>-0.000992</td>\n",
       "      <td>2023-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>1.655904e+06</td>\n",
       "      <td>2023-04-10</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>2023-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>1.664557e+06</td>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>0.005226</td>\n",
       "      <td>2023-04-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>1.653694e+06</td>\n",
       "      <td>2023-04-12</td>\n",
       "      <td>-0.006527</td>\n",
       "      <td>2023-04-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>1.646168e+06</td>\n",
       "      <td>2023-04-13</td>\n",
       "      <td>-0.004551</td>\n",
       "      <td>2023-04-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     account_value        date  daily_return    datadate\n",
       "625   1.651971e+06  2023-04-06     -0.000992  2023-04-06\n",
       "626   1.655904e+06  2023-04-10      0.002381  2023-04-10\n",
       "627   1.664557e+06  2023-04-11      0.005226  2023-04-11\n",
       "628   1.653694e+06  2023-04-12     -0.006527  2023-04-12\n",
       "629   1.646168e+06  2023-04-13     -0.004551  2023-04-13"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "973c8fb6-7e44-4e6b-96ed-eec8788c93cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGsCAYAAAD+L/ysAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABk2klEQVR4nO3dd1zTd/4H8FcSIOy9leHALUOsiOPUiqL1vNppex3W1vZqtVePjl/tXbW99mqntb3z6nW4uuzULuuiVWuLUlHcAycgBARk7+T7+yPJl3yBYIJAvpDX8/HIoyT55ptPUgmvfMb7oxAEQQARERGRjClt3QAiIiKiq2FgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2etxgWX37t2YOXMmQkNDoVAosGnTJqvPIQgCXn/9dQwYMABqtRq9evXCv/71r45vLBEREVnEwdYN6GhVVVWIiYnB/fffj5tvvrld53jsscewbds2vP766xg+fDhKSkpQUlLSwS0lIiIiSyl68uaHCoUCGzduxKxZs8Tb6urq8Pe//x2ffvopSktLMWzYMLzyyiuYOHEiAODEiROIjo7G0aNHMXDgQNs0nIiIiCR63JDQ1SxcuBBpaWnYsGEDDh8+jNtuuw3Tpk1DVlYWAOC7775D37598f3336NPnz6IjIzEvHnz2MNCRERkQ3YVWLKzs7FmzRp88cUXGD9+PPr164cnnngC48aNw5o1awAA586dw8WLF/HFF19g/fr1WLt2LTIyMnDrrbfauPVERET2q8fNYWnLkSNHoNVqMWDAAMntdXV18PPzAwDodDrU1dVh/fr14nEffPAB4uPjcerUKQ4TERER2YBdBZbKykqoVCpkZGRApVJJ7nN3dwcAhISEwMHBQRJqBg8eDEDfQ8PAQkRE1PXsKrDExcVBq9WisLAQ48ePb/WYsWPHorGxEWfPnkW/fv0AAKdPnwYAREREdFlbiYiIqEmPWyVUWVmJM2fOANAHlOXLl2PSpEnw9fVFeHg47r77bvz666944403EBcXh8uXLyM1NRXR0dGYMWMGdDodrrvuOri7u2PFihXQ6XRYsGABPD09sW3bNhu/OiIiIvvU4wLLzp07MWnSpBa3z5kzB2vXrkVDQwNefPFFrF+/HpcuXYK/vz9Gjx6N559/HsOHDwcA5OXl4dFHH8W2bdvg5uaG6dOn44033oCvr29XvxwiIiJCDwwsRERE1PPY1bJmIiIi6p4YWIiIiEj2esQqIZ1Oh7y8PHh4eEChUNi6OURERGQBQRBQUVGB0NBQKJVt96H0iMCSl5eHsLAwWzeDiIiI2iEnJwe9e/du85geEVg8PDwA6F+wp6enjVtDREREligvL0dYWJj4d7wtPSKwGIeBPD09GViIiIi6GUumc3DSLREREckeAwsRERHJHgMLERERyR4DCxEREckeAwsRERHJHgMLERERyR4DCxEREckeAwsRERHJHgMLERERyR4DCxEREckeAwsRERHJHgMLERERyR4DCxEREbXqSlU9Vv58BqXV9bZuSs/YrZmIiIg63qOfHsSeM0U4qanAv++Ms2lb2MNCREREEhW1Ddh4MBd7zhQBAL47lAetTrBpmxhYiIiISOLpr4/gb58dkty2O+uyjVqjx8BCREREorKaBmw7phGvJw8NAgCs2nnWVk0CwDksREREZOL7w3lo0AqICnTHlkV/QGFFLX46WYiD2aXIKalGmK+rTdrFwEJEREQAgLpGLf6degYAcMeocKiUCoR4uWDF7DiMjPRBkKezzdrGwEJEREQAgH3nSqApr4W/uxp3JYSLt8+IDrFhq/Q4h4WIiIgAADtP6SfWTh4UCGdHlY1bI8XAQkRERACA387qlzFPGBhg45a0xMBCREREAIBLV2oAAAODPWzckpYYWIioWxAEAf/YdATPbDyCRq3O1s0h6nGq6hpRUdcIADadXGsOJ90SUbdwvqgKH+3NFq+/cOMwqJQKG7aISB4EQcCl0hqEerlAeQ2/E4UVdQAANycV3NXyiwfsYSGibuFQbqn48yf7svHtoUu2awyRjPxwJB/jXvkZ7/5yrt3nqG3Q4uEPMwDIs3cFYGAhom7iUE6Z5Prh3DIzRxLZl5TP9SX0X/7xZLse//n+HAx6dgtOFVQAAPw91B3Wto7EwEJEsiYIAlbvOY+1v10AAIyK9AUA5JRU27BVRB0nr7QG5y5Xtvvxrk5Ny4+tnd/13aE8PPXlYcltRZV17W5LZ2JgISJZSztXjH9+f1y8fu+YCABANgML9QBanYApy3fh+jd24XJF+4KCYLKJclah5cGnUavDa1tPtbg9zMc2pfevhoGFiGTtl6wi8ecFk/phaKgXACCnpAaCYNvt7omu1fmiKlTVawEA6edLrH58SVU9ymoaxOunNBXizxW1DTiUU4oGrQ4nNeUtfl8O5pQiu6Qans4O2PjIGLx1RyxmxYbixVnD2vlqOpf8pgETEZnYYwgsCyb1Q8qUgWjU6aBQADUNWhRV1iNApuPtRJY4ltc0F+tg9hWrS+A3H0oyDpVeKq3BHe+mIaekBrFh3sjMKcXfkgbgsaQoAPp5K8ahoJgwb8SF+yAu3Ac3xva6lpfTqdjDQkSydaWqHkcNH+hzEiOhUiqgdlAh2LCK4VJpjS2bR3aitkGLI7llndKjd8Rk8vjvF6zvYTnbPLBc0QeWpd8cRU6J/vcjM6cUAPDmjtNiL87/fdU0b2VwiKfVz2sLDCxEJFu/nS2GIAADgzwQaLLU0rjsUlNWa6umkR15OzULM/+zByt2ZHXoeQVBwK7Tl8Xrhy+VoaSq3uzxjVodFn99BF9m5Iq3nb1cBQDwcnEEoB8SWvnzGew4UdjqORZ/rQ8qwSa/T2G+8pyz0hwDCxHJ1p4z+uGgsf39JbcHi4GFPSzU+f63W1/f5K3ULPR7ZjO+yeyYGkAHskuRVVgJF0cVenm7QBCA3SYBprmfT13Gp+nZeOKLQ6hr1M97MQ4JTTTs/XMot0ycSPuHAQHwcXUUfwaAc0VVqG/Uwc2kMNwkGe4b1BoGFiKSrT1n9B/e46OaBRYvQ2Apl+fyS+pZogLdxZ+1OgFLvjnWIef91RDIJw8OxKy4UADAurQLLYaequsb8fG+izieVy7etuDjg6isaxRXBU1sJXTcGBOK7/86HpsWjMWa+66Dk4MSggAUlNeKK5I+fGAUest0VVBzDCxEJEvZxdXIKamBo0qBUX18JfcZA8vGg7moMaywIOosV6qlwzQ19VoIgoAP9pzHvHW/Y9+54nad96RGH0Cie3thTmIkXBxVOJhdKimK2KDV4ZZ30vD3jUfx5o7T4u07ThRg2NKtuFhcDSeVEpMGBkqCVXRvL0wdGoRe3i6IDfOGSqlAL28XAMCF4ipxZdHwXl7tarstMLAQkSz9YuhdiQv3kXRfA01DQgXldXhlS/uqexJZQqcTUFSpDyxPTB0AAKjX6jDhtZ14afMJ7DhRiPvW/N6uCbkn8/VLkAcFeyLQ01ncIbmgvGlu1s8nC3Eiv7zFY50cmv583zA8GN6uTlj/wChMGxqM/941At8uHAcPZ0fJY0IMQd8YiBxVCnHuS3fAwEJEsmRczjyu2fwVQLrXydrfLqCBuzdTJymtaYBWpw8jf5nQD0ND9StqskuqxdtrGrTQlFs3AbymXosLxfoJs4NC9EHF0xAeKmobxeM+35/b4rHP/2koXr0lGgAQ6KHGo5P1S5VDvFyw6p543DC89aXRoYYeltV7zgMAAtzVUCi6zwaiDCxEJDu1DVr8dlbfzT4uqmVgiQ3zRnTvpq5s4+Rcoo5mnOvh4+oIR5USz/9paKvHHb3UshekLeeKKqETAG9XRwS462sJeTjrexIravXDNReLq5B6sgAA8EeT+ixBns6YFdcLWxaNx09PTES/AHdYwhhYig0rkUIM17sLBhYikhVBEPD8d8dQVtOAAA81olsZY3dxUuHbheNwb6K+TP/3h/K7uplkJ4z76hgLFI6M9BVX3Jg6ckm6GWd1faM4R6U1F4v19VL6+LuJvRyeYmDR97BsOpgHQdCv8Fkwqb/42AAPJwD6oSR3teX1X//QLPz/fcZgix8rBwwsRCQrXx24hE/TcwAAr9wyHA4q8x9TM2P0Kyu2HdNYvekbkSWM80n83ZsqKkeY1C1J7OsHANh+vADPbDwiLktO+ewQpq34xewyZeNwUKSfm3ibcc5JuaGHJddQBG5UpA8GBXsgoY8vgjzVGBTcvkJvIyN9MXtkGJxUSnzxcCJGhPu06zy2wtL8RCQrB7KvAADuHBWG6wcFtXlsfLgPnByUqKhrRH5ZbbcpgEXdR+4Vfa2f3j6tD5/Mn9gPaeeKcSK/HCfyy3H0Uhki/dyw5ZgGAPDeL+ckPTK1DVo8/91xbDmq7xWUBBa1tIel0DAcFejhDIVCgY/mJUCpUEClbP+8k2U3D8fSPw2Bq1P3+/PPHhYikpU8Q7n92DDvqx6rVCoQZvhDwt2bqTMY9+YxrVVy+8gwKBTAjOEhGBkp7aU4nFuGbcc14vWjl8qQVVCB7ccLIAgC3tt9Dp+mZ+NKtb4XJdK/6bzNJ90aA0uAp753x1GlvKawAuh/Z7pjWAHaEVh2796NmTNnIjQ0FAqFAps2bWrz+J07d0KhULS4aDQayXErV65EZGQknJ2dkZCQgPT0dGubRkTtkFNSjXnr9uOtHVniqgdrXCyuwpJvjqLQylUS5uSX6s8T4mXZhMBwQ68KAwt1BmMPS5hv07/H4b298MtTk/DG7TGt/vH/6kBTJdwr1Q2Y8uZuPLh+P/o9sxlvbD8tOdZ0wqxx0q1xSOiy2MPCDT6BdgSWqqoqxMTEYOXKlVY97tSpU8jPzxcvgYGB4n2fffYZUlJSsHTpUhw4cAAxMTFITk5GYWHreyEQUcfQ6gQ8/FEGdpwowJs7TuOz33OsevyerCJMeG0n1qddxKLPMjukTcYellALVzAwsNC1qKprxB3vpuHt1Nb3CTJuJhjWrBpsbx9XODuqAADLb4/BmH5+4n2t1U0BgNa+DxhrrwCmc1ga0ajVobiqaUiI2hFYpk+fjhdffBE33XSTVY8LDAxEcHCweFEqm556+fLlePDBBzF37lwMGTIEq1atgqurK1avXm1t84jICunnS3DMpNy3sRS+JQrLa3H3B/vE67+dLRb3N2mv8toGVNTpu8NDvS37kA5jYCErpZ0txr9T9T2Ke84UYe+5ErydmoXiSulWDw1aHfING2y2NT/q5hG98cmDo1vcvigpSvw5wEONxyZHtTjG0WRSuemy5uKqeggCoFQAvm5O1r3AHqrL5rDExsYiJCQEU6ZMwa+//ireXl9fj4yMDCQlJTU1SqlEUlIS0tLSWj1XXV0dysvLJRcist7mI8aJf/oP4/0XrlhcsXNTKxvA7T5tWT0UrU7AhaIqbD9egAtF+tUSFbUNeMOwaZu3q6PF4+x9/PWTFk+a+VZL1Nyd7+3FG9tP45P0bJw1bB7YqBPwTWae5LitxzTQ6gT4uzuJtVLa0rzM/fWDArF05hC4Oanw6i3R+NuUATi0dCqmDQ0GAPw5IVxyvIfJsuZCwz5Z/u7qa5630lN0+sybkJAQrFq1CiNHjkRdXR3ef/99TJw4Efv27cOIESNQVFQErVaLoCDpaoCgoCCcPNl6ye1ly5bh+eef7+ymE/V4u7P0PSpPTRuExzYcRGFFHXKv1Jj9Npl7pRpPfnEY1fWNqNc2BZu+AW44d7kK3x7Kw5Qhba/sST9fgmc3HcWpAn1Z8uG9vPD1I2Nw1/v7xJLhf4iyfPfY+AgfKBTA2ctVKKyoZfc5tUlT1jTXavfpy2JIAIAvM3Jx/7g+AIB3d5/FWzv0w0R/ToiA0oLQsPz2GEx5c7d4PdjLGXPH9sF9YyLFWiteLo5YPjsGM06EYOpQ6e+KsUz+lap6pJ3Th3/Tqs72rtN7WAYOHIi//OUviI+Px5gxY7B69WqMGTMGb775ZrvPuXjxYpSVlYmXnBzrxt2JCCiurBOLV43t7y+OpR9tVgDL1L9+OIG0c8U4lFsmjtN//+g4LL89FgDw04mCNuuhFJbX4vb/pYlhBdAX3Pp8f44YVhYlReHVW6Mtfh3erk4YbKhLsfdcicWPI/u0/2LTv5HfL5TgjGG3YwA4nl+O43nlSD9fgpc2n0RVvRYjwr3xwNg+Fp07KsgDy2+PEa/7u+l7ZZqXv3d1csDMmFCoHVSS23t5u2BUpC8adQJe2qz/wm7c/4dstKx51KhROHPmDADA398fKpUKBQUFkmMKCgoQHBzc6uPVajU8PT0lFyKyXE29FjP/vQcA0C/ADV4ujhgWqu/OPprXemCprm/Ez6ekE+EVCv0qh+G9vOChdkBVvVYSRpo7ZLILbYxJaf0Vhm+yTyYPxKKkAeJkRkvFGJZAm/7xIWrNEZN/g6XVDWJQHhikD+yvbDmJ2/+nn44wfVgwvpo/Bl6ulm8QOGVIEEK8nDFlSJBFvTKmFAoF/tpsnoulk8/tgU0CS2ZmJkJC9PsiODk5IT4+HqmpqeL9Op0OqampSExMtEXziHq8N7adQp6ha9xY7XKoYfzddBKuqWc3HUNtg7T3RBD0ZfJVSgViw70BAAeyS80+r7H3ZtLAAGxaMBYjDI8xLt803S/FGn6GSYll1fXtejzZj1zDKjRT/QLc8ETyQADALpPKtFOHBlm9OaCHsyN+eWoS3r0nvl3tiwqS7gvEHpYmVgeWyspKZGZmIjMzEwBw/vx5ZGZmIjs7G4B+uObee+8Vj1+xYgW++eYbnDlzBkePHsWiRYvw008/YcGCBeIxKSkpeO+997Bu3TqcOHEC8+fPR1VVFebOnXuNL4/IvgmCgA/2nMf9a3/H94ebJhRuMCxf7h/ojkVTBgAAhhh2jD2tadlDkldag68O6HeN/c+f4+BkWNlguo9JnCH47L9gfljmmKH3ZsKAACgUCgwIalrSGe7rigiTqp/W8DZ8AzYW4yJqTlNWi68ycsW6KqadHw+M64uJAwPg0qxnb+KAQLSHg0rZ7l2QAz3UYsVboPttUNiZrJ50u3//fkyaNEm8npKSAgCYM2cO1q5di/z8fDG8APpVQI8//jguXboEV1dXREdHY8eOHZJzzJ49G5cvX8aSJUug0WgQGxuLLVu2tJiIS0TWef+X8/jX5hMAgIyLVzBtaDDqtTpUGpYOb1owVgwdxsmqRVX1EARB8oH7uyGEDO/lhT9GhyLSzw1PfHEIKYawA+j3VHk7NQu/nilu8Xgj4462xt6cuHBvMTw1X2FhDW9XfQ/LFfawkBkvfH8cPxxp2iTzprjeYgi/bWRvOKqU6O3jgizDsGL6M5PhY4PlxAqFAv0C3ZGZUwqAPSymrA4sEydObHPZ49q1ayXXn3rqKTz11FNXPe/ChQuxcOFCa5tDRG0wrgICgLKaBqSfLxHHxNUOSrg5NX2j9HPXfzjXN+pQVa+FpqwWn+zLxoJJ/fDJPv2XkOsifQEAw3p5YcuiP0iea0SEN1wcVSiqrMObO7KQMmUAzl2uxCMfH8CCSf0xuq8fNOW1UCiAwSH6eWfjTVYD9fZt/zdJH0MPSyl7WMgM07ACAI9NjkK/QDdMGxos1kLxcW0KKIE2XJ0zYUAAMnNKoVAAEX7cH8uoe24oQEQWKanS9zgEeKhxuaIOb6VmYd95fW+Jv7ta0gvi6uQAF0cVahq0KK6sw1NfHsKB7FKs/vW8eMz4AdLt6U2pHVQYH+WPbccL8HZqFu4eHY6nvjyMk5oKPPrpQaydex0Afd0UY69OqLcLnByUqG/UYVx/8+e+GmMPS2kNe1iopZr6lgUNe/m44JGJ/SW3LZk5BDf/9zc8PKFvVzWtVYuSojAy0geNWoHL9E0wsBD1YMWV+j/gU4YE4ZN92WJYAZp6VEz5ujnhUmkNiqvqW0yenTY0GJMGtj2m/8ot0dh2fDsA4JSmAucNReGApsm8xtVIRjv+NgGnCyokvS3WEntYqtjD0t2cLqjA018dxp9iQvHh3ouYMCAQf58xuEOLpbW2eqy18w/r5YUTL0yDreu0KRSKa/p96Km4WzNRN3SptAa/XyjB6TaWEAuCIPawjO3XsvfCr5XxeX9DiCmurEdwsy7x+8ZGXrVdPm5OYhXPf/1wAsVVTT0ee7L0hbCiAqWrIML9XJF0lWJzV2PsYamoa0SDoQ5MdX0jfjicL14neXr0k4M4kF2K5747jrOXq7D61/P42jC3pKNkFep/TwYEuSMu3BtPGlYEtUalVLR7wix1LgYWom5m+fbTGPvyT7htVRpueOsXpJ0tbvW4yrpG1Bv+WI/u69vi/sZWdmIz7llSUF6Lwgrp7svRvS2bFDvAsCzzZLPVRmnn9O1sa0+W9jJWCAWAbw3l1f+x6SgWfHIA//7pTIc/H3Uc4+aCptLPd2wBQOOy+WGhXtj4yFgsmNT/Ko8gOWJgIepGzl6ulOwq26gTMGdNOl74/niLngRj74qrkwp+reyDUl7TcvjE11CZ81heubizbFSgO2aPDLN4b58BJrvPAk29Nka9fTp+maZp9/7jXxxCflkNvj6g3+vo7dQsHMy+0uHPSR2jtX9X5moBtVep4d+6NQXgSH4YWIi6ia3HNJj8xi4A+h6TE/+chusHBaK+UYcP9pwXlwcbFRnmrxh7TVbdPQJ3jmrabM24aaApY7gwLqmM9HPF9pQJeMWKUvlJg4Mwe2QY5iRGIHPJFNwS31tyf2+fzln1cJvJ83yVIR1SmP/RAWjKanHbqt+w1mQSMXWMBq0OGRdLoG2l1+5qjzNdin5zXC8A+iGc+saOG8orMwYWFwaW7oyBhaibeP7bY+LP46MC4OKkwgdzRmKeYbO25dtOifv7AE09LMbelWnDQrDs5uH4av4Y3DyiF565YXCL5zDuJ2Q8T3uGb5wdVXjl1mg8f+MweLs6SQKKo0qBQI+r73rbHq/dFiOGM+MybONQmKa8Fg9/lIHfL1zBc98d75Tnt2frfruAW95Jwwd7zom3ldU04J2dZ3GlyvzKrdwrNWLIuTcxAi/dPBwezg5o0AqSCdvXqsyw3N2bgaVbY2AhkiFBEJB2thhbjuZDEATU1GvFUvoAxB2RFQoFHp86EAODPHClugEPrt+Pilr9h7NxomFAsyGZ+AgfLL89ttU6EzcMD5EM4Vg6b6UtvU0qdYZ6u1i9v4o1jAHL+F49+8chYvVSY68R0BTmqGMY9+PZfrxpT7jHPz+EV7acxBNfHDL7uHxDmfx+AW74543D4OyoEgNtcVVdh7WvjENCPQIDC5EMvffLOdz53l48/NEBfLT3Ig4Y5mCoHZTY/NfxkpL2Lk4qfP6XRIT5uiD3Sg2WfnMMjVodPky7CKAp3FjC2VGFW+PDxOtxYT7X/Fp6mcxZiQ+/9vO1JczkudzVDhgc7IngViqFHjOzwSO1T36ZPnhk5pSipl6LOavTseOEPrykniw0+7giQ3D0N5ljZewl68gigMb6PN4uXV+5ljoOAwuRzFTWNeI/JitbXt92WqwjMT4qAENCW+5O7uXqiBWzY6FUAF8fvIQvM3KRX1YLH1dHzDLMC7DUrLhQ8WfjhobXopdJD8v1g9u3N4ulwk2GsIaEeEKpVLRYng0A244VoKiy477B27u8Un2PVoNWwKbMS5INBAEgp6RpJdCKHacx8bWfkXulGkWG1Tv+JsOExmqzHdkLZuxh8eSQULfGwEIkM99kXkJ5bSMi/Fzh5qRCWU2D2NXe1gqb+AhfJPTxAwD883v9PI2JAwOhdlCZfUxrBgV74rmZQ/DSTcMl33zby03tgFtG9Mb4KH+renvaw7h1AAD0DdBPKm5tL5YP917EyBd3oLahZQVUso5WJ6CgvGm4cvHXR1ocYzq3asWOLFworsbTXx0Rh338TWoCGQNLW3NfrGXsrfHmkFC3xsBCJDMbDctx70oIx6g++j/Ae87oi65dbRLsHwboq2NWG0qRTxzYvmqZ943tgz8nhF/9QAu9cXsMPnwgwerwZK0JA5pe7/BW5t+Mj5IW0Psm81Kntqen2336MhZ+cqDVmj6PTY5CkqFHzbhizXQfuj1niqApMwQWk2Bs3HCwpIM2stTqBFTU6jf75Cqh7o2BhUhGsoursf/iFSgVwI2xvTC6r5/k/rCr1DCZOlTag5E02L52PFcqFfjhr+OQMmUAbh+pn4szfXgIAOD2kb3RL0BaZXfTwbwub2NPkXHxCu5bk44fj2oAAM6OTX9Orov0wezrwhBgGOoxFm4rNwQHo40H9cvP/SRzWDp2I8vzRU1l+RlYujfuJUQkIxsP6r/xj+3vjyBPZyQNCcKyH0+K91+thkm/AHf8/YbB+NfmE1h283C4qe3vV3xoqBeGmuxXlDQ4ED89PgGRfm544QfpkuaM7CuobdDC2bFze356ou3HC2DasfLH6FBMHhQIV7WD2NMVYNi4z1g12XToCID4eNN9rYxDQkWVdWjU6uCgav/36sq6RvzlwwwA+pVIjtdwLrI9+/s0I5IpQRDEb5yzYvUTZfsFuCMq0B1Zhkm3lmw1/+Af+uKu0eEWV6bt6RQKBfo261kxqm/UIf18iTiURpY7qdHPS3kyeSCmDQtGX3+3FnvwGHtYCivqkFdag4WfHAAAeKgd4O7sgHzD8nPJkJAhsPySVYQhS7diy2Pjzf7/a8uVqnrc8e5enL1chUAPNVbdHW/9iyRZYdy0E41aHXKvVENnZSVK6hyfpmdj37liaMpqsfjrw9h2TIOb/vsbLhRXw8VRhWnDgsVjX75lOO5NjMCmBWMt7jFhWGmdAi1rwBh7tTrKpdIaVNU1QqcT8PnvOTh3ueVOwT2BcSLt6L5+6Bfg3uqGgQGGILL9eAHGvPwTThcYgre/K75ZOBbero5wdlRKqi77mEzArW/UiVssWKq6vhH/+SkLT355GKcKKuDt6ohV98QjKsjj6g8mWeOnmp2Y//EBbD9egPFR/vjwgQRbN8euHc4tFVdSOKmUqNfq8Gl6U1n9G4aHSIJJfIQv4iNabl5IHeOHI/n4541D4eF87fMb8kprMOm1nQjyUuPhCf3w941HAQApUwbgjlFhCPRouWKpOyqurENBuX5eysBg80EgwExVY5VCgUAPZ+x6YhIq6xvF2isAMLTZsv16K3fbfubrI9iU2TQ36c3ZsRjRyfV/qGuwh8UOCIIgLov9JavI6v0+qGMdudRUtKz5h7G3qyOWzBzS1U2yG6adAF4ujujl7YL6Rh2O5HZMIblDOaWo1+qQU1IjhhVAv8P2bavSOnR/HFs6mF0KQD8vxL2NXr8gz5aBJdTLGX+fof837uXqKKnTA+iLF66de5143bSGy9XUNmglYcXfXY2JHO7rMdjDYgcuNyuQdaW6vkPqa9DVfX84D1kFlXjwD33FD/asgqYhAicHJf75p6F42tDj8sZtMVzJ0IlMBy0UCiA2zBuXSmtwKLcMY/r7m32cpc62MvwzZUgQDmZfwcXiamzKvAQvF0cM6+XV4g91d7LtuH5lkGndm9b08nbBXyb0haujAx4Y3wcNjTrJkI85EwcG4oM5I/HAuv3ItiKwHLgo3ZXbx9Wx1aEq6p4YWHq4qrpGbGo2Rl9cycDSFXQ6Af/35WFU1WuRerIA3y0cB4VCIU5WfPWWaEwbHgxPZ0colQpkFVRg4sDOrQRLTRTQ75X0w5F8HDLZZ+hanL0s3bDPyUGJ12+Nwfq0C3hj+2k89eVhAPpeht8WT+6Q5+xqn/2ejc/36yeHj7xKYFEoFFg83WSTTSs+doxVi7OLqyEIQovgcaawAjklNZg0qOl35vcL0sDyyKR+lj8hyR4DSw+m1Qm44e1fcLFY+g1FX12SE9A626XSGlQZCrgdvVSOPWeKMCzUC8cu6QPL4BBPeBrmTRhrhlDnMv2bp1AoMDJSP7dhyzENLhZXIcLPzcwjLWPsYfnvXSOgVCjg4+oIL1fHFlsc5JXVolGrg0qpQHltY7fqVdt5qqns/qR2Fia0RJivKxyUClTUNSK7pFry/6a2QYuk5bsBAN8uHIvo3t4AgN8vlADQr1yKC/NGYj+/Fuel7otzWHqoX88Uod8zm1uEFUDfw0Kd63heObYe00hue2h9BuJe2I6KukZ4OjtgcAhDY1dr/i19RLgPBofoJ3lOeG2nuImftQRBwEd7L4q7Fg8O8cS0YcFIMBT+iwps+f/6yS8PI3HZTxj54nbsMNnlWK7qGrXIKakWQ9mauddJCr51NGdHlRgof262geK3JvNUdpzQ39eo1YmbhE4eHIgx/f05HNTDMLD0QBW1Dbjr/X2S24aGeop7qnTkpmLUUnFlHW5+51e8+MMJAMAgwyqKGpN9awaFeF5TQSy6dgroA8w/ZjQNWZwuaN8S5O3HC/CPTfpJtqP7+kqW6QKtTz7dePASNOW1aNAKmLd+PxqtXA3T1f71wwmMf/Vn8T3q347aKNa63jDcs7PZZoo/n2oKMPvOFQMAjueXo7peC09nBwxoJSBS98dPzB6o+U6pf50chR/+Oh6TDft6FHOX2k7129li1DY0/fGZOiSoxR42D0/o29XNIrScdAvoqwpfZ/gmX1XX2PJBFvjtbLH4s2TOhvhcCoyMaHtpbWU7n7urrE+7KP7s5KBEaBdMGjZu5nk4t0yyD9HRvKZVXRkXr6CwvBbv7DwLQD+vRqlkz0pPxDksPdBPhu7TuHBvDA7xxEN/0P9x9HXTf8srNulh0eoEfH0gF2P7+3fJB5A9MP3j5efmhKlDg/HgH/oiM6cU/QLc4ebkAC/uGmsbZv6OGQvttTewHDRM2n3rjljEhHm3esyzfxyC1BMF6Bfojsc2ZLa4X5B5tQEHpULc5LB/gDtUXRAKBgZ7QKVUoKSqHpryWoR4uWDnqULklOiH7iL9XHGhuBqjXkoVHzNtaLC501E3x8DSwwiCgF8NO/s+OXWgZKmmsYhTXmnTOP1z3x7Dh3svYmx/P3w8b3TXNrYH0ukE7DJ0V79z1wgkDw0Wv+2Nj2I9CHlp+oNrXHJ+tcCi1Qm4b006vFwc8Z8/j8DmI/kI9XbBccM3/rYKlMWEeSMmzNtsT4rM8wqcHVWorGtEYl8/PJE8oMueMyrQHSc1FXh962n8OSEM9635HYB+pdU9iZF44fum/aFevnk4bhvZu0vaRl2PQ0I9TF5ZLQrK6+CgVCCu2YfnMEMFycycUrFE/4d79d28v54pBl27jOwryCurhbvaAZMGBbJrWmZMS/Obzsd0U+s3PzyWV46VP59BXaO2+UMBAFmFFfglqwjfH87H3nPFeOTjA5i18lc0aAX08nZB76vspg3ow9Hdo8MxKNgDu5+cdG0vqIvUNWrFoPXO3SO6tPJyjGEF0FcHcnHLO2ni7feP64OY3k2bXEb4ueKOUeGcaNuDsYelhzEWThoc4gkXJ+kOtENDveDsqMSV6gacK6qU7DcT7NkzSobbUnltA5784hAA/Q7B3AFYfsz9LTNuhfBFRq5424JJ/Vscd7miaf7Xj0fyJfclDw22+I/li7OGA4BkXoYg4zGhK1UNAACVUiEuxe8qC6/vj1+yLiOvrGmn51vje2Pe+L4or20Qb+NnWM/HHpYepEGrw5pfzwPQz19pzslBiVjD+PrvF67gQlFVi2Oo/T7/PQcXiqsR4KFGypSBtm4OXYVptHBrtlnk8bzyVh+Te6VpONW0BLyjSoGbR/Syvg0mAUe+ccVYu0m/k3JX9xqG+bripycm4oVZw8TbRhuWi3d1eCLbYg9LD5J6ogAHskvh7KjEXQkRrR4zMsIXe8+ViAWWjEy/qVBLh3NL8c7Os0iZMsDsrq+fpmcDAP6WNADhfq5d2TyyUGurhAC02AVbMBMfcq801TUqq9H/zvzzxqG4YXjINVePfvH747hcWYd37xlp8a7cXcXYw+LrZpuA4Oyowj2jIxAV6I6Mi1cwMyZEvO+O68Kw4fccPD6VXxJ6Onn9VtA1ST+vHw66ZURvszuoGgsx7b9wRbJzbHW9Fg1aHRxZG6SFoso6/Ok/vwIA6hp1WH3fdS2OqWvUimXZk4cGdWn7yHLmRmzc1dLhO+MqlOYuXWl5+/BeXh2y1YWxx+aXrMuYNizkKkd3LWMPi68F+wB1ptF9/cTeFaPn/jQUC6/vj94+/JLQ0/GvUw+SYajyOKqP+QlxIyJ8oFAA2SXVOJgt3XfD+I3R1hq1OpRWd11xuyO5Zfgm85LZ+18yFIAD9EvGy6pbvk8aw/i6s6PS5h/qZBnTCbiuzYaEcq60vuFebrPAMmN4iFgWvt3taBaijDshX01tgxaZOaVdMvfllKYCADq1sm17OTuqGFbsBANLD5FfVoNjl66+tNLT2REBhg+dfeelw0JyCSwLPzmIuBe2I7uVbQU6w8z/7MFjGzJbTKIEgMKKWmxsFmY2H215XF6pPrCEerlwlYKMmV8lJA0spdUNqGhlmPSSSUmAkRE+WHnXiA6vR9I8sBy9VIZzrewC/fjnhzBr5a/49lBei/s6UklVPdb+dgEAMH0Ya5yQ7TCw9BD/23UOjToBo/r4Isy37W8bwV6tz6Yvl0lg2XJMA0EAPk6/ePWDr5FxeTcAfL4/p8X9248XQBD0NTSenj4IAPD1gaaVJI1aHV7behKvbj0JAAjx5koFOTM/JNRydLz5Fhb1jTpoyvXB9H/3xGPt/aM6pk3Nrh/MuSJWoy6pqsdN//0V17+xS7Lf0PG8cvxgCNiPbchE8pu70e+ZzeIqtY70ZUYOquu1GBLiiRnD5TVURfaFgaWHMO6t8dD4q5d8D2q2/K9vgH7fEzn0sNQ36lr9ubMUVTUtU824eAVanbR7PdWwsVry0CDcGBsKlVKB3y9cEfcveX3baaz8+az4rTjEi9WCuwvToOCqbrkEvaJWWuAtv6wGggCoHZSYOiSo1ZDTrnY0S1ENWkFcXp1dUo0Grf7f5Lz1+3HfmnTUN+rwY7NevlMFFdDq9I/r6OHUz/fr23JvYgR7D8mmGFi6uZ2nCvHyjyfFXZmvi7x6QSfTegXBns4INFTAlUNgKaxoqrVQXtP5e6sUlDUFlvLaRhwz2aNEpxPE1VR/iApAiJcL7rguDACw7MeT0OoEbPg9W3K+UDO9VyQP0lVCTdccTIZ1jL8PzSvSGifc9vLpvGE/4wal3x/WD/OY1n0BgJ2nLiP1RAHSzpov9HimsH0bOLYmu7gaZwor4aBU4IZo9q6QbXGVUDdW36gTy1QD+p4SS/aoMR0SCvdzhbeLfpKorQLLucuVcFM7IMjTGQXlTYElp6Tz57BoTJ4PAFbsyEJcmDdCvF3w2taTqKhthEqpEHdcfiwpChsPXkJmTin+t/ssSptNwB1gZnUWyYSZoGH6OxHi7YLCijpUNuth+c4QInp18J5bpi1KHhqMdWkXcPRSOfZfKGkRWABgx4lCHMotBQDsenIi8kprEertjAfW7ceZwkrcuioNg4I9sPGRsS2KR1pr12l9D+OICB/WPCGbY2DppsprGzDh1Z8lt8VYuFrBtIclwtcVjg76jraiyq5bmWNUXFmH69/YBYUCOPfSDcg3qWZ5saRzCtvV1Gvx4d4LuDG2FzRl0lUfP50sFDePNErs6wcHw3LvQA9n3D4yDGt/u4BXt5wCAIyK9MX4KH/0C3TnpMRuKtDDGd8uHAt3tQOWfnsMgLSH5XBuKT5N189xutocMWuZZig/NydcF+GL9AsluOeDdNw7pmU9pa8Mc6jCfF0Q7uuKCD/9kO6EAQFi78pJTQUyc0qR2M+vxeOtcShX3+M45hrPQ9QROCTUTaWeKMCVZt/uzdVeaS7S30382bTglXGiX1c6VaBfLikI+hUYGpPAUlhRJ5kU21GW/XgCL20+iTvf3Yvj+frnv2d0BJ6bOaTV4xclRUmuTxkirbNy28jeeHRyFG4YHsIxfpkzVzgOAKJ7e6NvgLs4N6XCJLB8Y1LVdk5iZCe2EHjttmgAQE2DFvsvXDF73PRh0n9vzX//Txt+t66FcaPUCBZCJBmwOrDs3r0bM2fORGhoKBQKBTZt2tTm8V9//TWmTJmCgIAAeHp6IjExEVu3bpUc89xzz0GhUEgugwYNsrZpdqO+UYf0ZkuSAWBAkLtFjx8R7o237ojFdwvHYdKgQPi764eEimwQWExrmox75We8s/OseF0QgKr6jp/Hsu2YfrXFuaIqcQ7K4BBP3De2D1bMjsXtJru9PjyhH0Y2mxd0XaQv/NycoFAAb9wWg1vjuTtsd2FJnjQGFuOQUINWh82GFTn/uyfe4i8GFrep2VLrCD83RBs29cvMKW1xvJODEo4qBW6Kk24FMDM6FI9e37T/0UlN69sLtKVBq8NHey+Kc2SMPZ6cTE5yYPWQUFVVFWJiYnD//ffj5ptvvurxu3fvxpQpU/DSSy/B29sba9aswcyZM7Fv3z7ExcWJxw0dOhQ7duxoapgDR6vMeXbTUXxmWIIbE+aNQ4YPtahAyz5IFQoFboxt+rBr6mHp+iGhgmZzSIqr6uGkUqJeq18hVF7bCI8OHjt3M1kRIgjA5EGB4pb0s+J6YdqwYHFlhDHMmXJyUOLL+WNQVdeIYb28WtxP3YPZJc7OhsBSpw/TX2bkIr+sFv7uTpgwIKATGmLaJv2V3j4uOJxb1mLV2tBQT3wybzSq6hsR2mwujYuTCo9PHYj+ge54bEMmTmqs62ERBAH3r/0dv2QVwdvVERn/mCL2sIQysJAMWJ0Kpk+fjunTp1t8/IoVKyTXX3rpJXzzzTf47rvvJIHFwcEBwcEc/7+anJJqMawA+rFlY2Bp72RAPzfb9bBoyls+5+TBgfj9QgmKKutRUlmPf6dmYVyUP/4YHWrVuWsbtKjX6iSTBb/MyBVL6BstvmGQZEsCZ0cVZsWGYndWkSTYmepjMqxG3YekN6NFBRQ9j2Y9LMb6PH/5Q78u24Hb3O9yL28XeLk6tjm5vl+AvqfV2knrW48V4JesIgD6wnnr0y6grlEHhQII8pJfhVuyP13ejaHT6VBRUQFfX2k3e1ZWFkJDQ+Hs7IzExEQsW7YM4eHhrZ6jrq4OdXUmy1HLre/67I5e2XJSMmQC6Df+UgCICnJv9y6q/h6W9bDUNmihdlB26DyNwmY9LIC+1+ikpgJFlfV4f885fJOZhw2/51gdWO5bk46Tmgo8M30w0i+U4Nk/DsETJoW14sK98cC4PujfSs/Um7NjOR+lB7JoSMi5aQ5LcWWdOCwzM8a6f38Wt6mV20xLzQ8IcseipAFY8+t5LDEzz8qUcWl0UWU96hq1UDtYFrK2NKvt8vx3xwHoe2AtPQdRZ+rywPL666+jsrISt99+u3hbQkIC1q5di4EDByI/Px/PP/88xo8fj6NHj8LDo+Ufk2XLluH555/vymbLwkd7pZVf/zAgAGE+rnhq2rXN9/F30weWirpG1DZoW/0W+d2hPDz+xSHcMzoCz/7x6h+aliow1F15atpAcdXNkBBPeBr+aBy51FQXRRAEi0NETb0We8/p5/k89dVhAMAVk8qlMWHe2PjIWLOPZ1jp+cxXvdX3XlTWNuLXs8UQBP1QjLkK0R3ZDuPPppNcF0zqjxuGh+AGC6vM+ro5icOqheV1Fq9qMu6TFBXojiyTWi69fTgcRPLQpauEPvnkEzz//PP4/PPPERgYKN4+ffp03HbbbYiOjkZycjI2b96M0tJSfP75562eZ/HixSgrKxMvOTktS6r3NLUNWrHy5mu3RuPwc1Ox/v5R7e5VMeXp4gBHlf48xVUte1m+zMjFo58eRH2jDh/sOY/qdkyEzSmpxutbT7UYdiowDAnF9PbGsF6e6OXtgpGRPvB00f/RMK1DUVWvtfj5zray90qqYbly/0B3bHhwtNWvgbo/hZmfTRl7WAoq6sRhlSEhnp3bMLFN+laN6++Pv17fHx/MGWl2WNLsORQKcQin+Ryxthj3SXpmxmCM7tvUA377yDCrnp+os3RZD8uGDRswb948fPHFF0hKSmrzWG9vbwwYMABnzpxp9X61Wg212r7GVI1/uNUOStwa37tDewAUCgX83NTQlNeiqKKuxfj5+7+ck1zffrwAfm5qbDuuwTM3DLZoXP/hjzJwLK8cmTml+Ghegni7cSm1v7saX88fC50gwNlRBQ9jt7xJ8a7C8lq4B1i2Eqq1wGI0eVDgNRfUou7Jkl8b4xyWQzmlOGfoaeis3hWg9bk0DiolUqYObPc5gz2dkVNS06IwojkNWp0YboaGeuLTB0fj/746jOp6LW7jKjiSiS4JLJ9++inuv/9+bNiwATNmzLjq8ZWVlTh79izuueeeLmhd92AsWR/oqe6U4Qp/DydoymtRXNVyEqwxLE0eFIhUQ2E1Y10KH1cn/G3KALPnzcwpRdrZYhzL088z2nOmSLxPqxNQaqiu6+vmBCeHpg6/1qpqFlbUoa+FgcW0PPn4KH9U1DYiM6cU/u5O+HNC63OjyL6Y+z0aFNI0DG2sxdKpgaWVIaFrZdwvzLSuUVs0ZbXQCfoVcP5u+s+YV2+N6ZjGEHUQqwNLZWWlpOfj/PnzyMzMhK+vL8LDw7F48WJcunQJ69evB6AfBpozZw7eeustJCQkQKPRAABcXFzg5aVfEvrEE09g5syZiIiIQF5eHpYuXQqVSoU777yzI15jt1dd34i5hhL8Ae6d07PkZ5jH0rzabaNWhxLDZmq3jeyNVJOwAug3DGxOqxNwIr8cNQ1a3LYqzexz5l6phmBYtenTbNWDq1PLf5qtlSk351yRfiXQP2YMxrzxfaHTCdiddRlDQjwR6Mn9fuyVaUgxlw1CvFzw3MwheM4w6RSQVofuTB31VcQ48XbD7zlIHhrcYh7L8bxyhPu5ijVncq7oh756ebt0yDAzUWeweg7L/v37ERcXJy5JTklJQVxcHJYsWQIAyM/PR3Z204Zw7777LhobG7FgwQKEhISIl8cee0w8Jjc3F3feeScGDhyI22+/HX5+fti7dy8CAjqh5kE39O7ucyg3DI0EeHROYDHWYjHOMblYXKUPK1X1EARAqQAmDQqU9IIAQGmNNOBodQIeWr8ff/z3Htz13r5Wn0sQBJwuqMCE13YCALxdHcXS90atLbEuvEpg2X+hBCt/PoO/fLgfJ/L1PTrGD2qlUoGJAwMZVsgi/QKlPXmdOyTU8WbF9YKLowpnCitx8zu/SSpGbz6Sjxve/gWPf54p3nbOsNQ/vIO3HSDqSFb3sEycOBGCYL5c+tq1ayXXd+7cedVzbtiwwdpm2JWT+U0FoJqX4+8oxgJpxZX12HX6MuasTseIcG88PX0wAMDPsLTxv38egXnr94uPK6qQBpZtxzTi5FZj8bfm/vn9caz59YJ43detZXE2B1XTx/jcsZFY8+sFyU7OzR3OLcWtrfTmcIUDmdVGUmjeo9KZPSySXp8OSi9DQ73w9SNjMP2tX3C5og6FFXX47WwR+ga4Y9FnmQD0dVdKqurh6+aE44aAPyS0ayYXE7UHy8l2AydMSmxbWn7fWn4m5fk/3afvITuQXYr/7dLXfTEORSU120dHU16LspoGeBlW9ew4Id04sDWmYQUAHFrpgv5b0gBcLK7GA+P64JxhAm1pVVNY0+oEHM4txdBQLzg5KPHDkfwW5wCk9SyIJPNF2jguyKRHRaFoPVR3BnPF7NpjcIgnevu4IPdKDTZlXsLLP55scczWYxqM6++PTwy/8121GoqoPbj5oczlXqnGxWL9+PKdo8Lw5NTO2WPJOCT0TWYethzTiLfvOn1Zf38bQ1HGTdZ0OkE83ijY01kcJzentJVeozBfV3w1fwxuGB4CL1f9H4srhrk0Op2ARz7OwE3//Q3/23UWgiBgr2HvE1Mezg5ikCICLA8EHib/ZgcEenRqXZ7OnDFiXPH388nWv0i8u/scbn7nN/E6e1hIzhhYZO7HI/rwMLqvL5bdHN1mSe5r4W9mMm+jYey7rcm++84VY9qK3bhl1W8oqqyDq5MK3z86Dkv+OAQfPjCq1f14dqT8QfzZuFLIHOOEXGOw+fZQHrYaNjB8Y/tpPLPxKA7llrV4XEgnzjug7q+tEGJ6X/KwTt4ypBNWCRkZexiNc7qMVsyOBQCcL6oSJ7MP7+WFSD9uOUHyxSEhmfv5lP6bkaVVLtsrPsJHct3PzUlSRM50Lshjk6PwVmqWeP31bacljx3Tzx/DenmJGwP6u6txwdBLNHlQICL83NA/0AMxvb1wKLcMU5oNMzXnY+hhKa2phyAIWPmztD7Pp+n67uxFSVF4YFwffJmRize3n8ZMK0v5U89n6ZAQALx/70j8erYICyb169Q2dSbj7215rbTY4x+jQ5B6shDfHcpDoIcamxaMRYiXMys8k6wxsMiccbfUQcGd21XrpnbAjpQJSFq+CwAwZUgQvsjIFXeLHWwytr3w+v5I7OeHS1dq8LjJ3jxGEwZKV3eZ9t58cN914s/v3TsSXx+8dNVKmt6GHpYr1Q04e7lSUjbc1I2xveDh7Ii5Y/vgvjGR/PCla5I0JKjFnK3OIKm+28H/Zs2V5XdQKfHW7Fg8PKEvgj2d4ddJ5RKIOhKHhGTO2F3bWcuZTZnuQOzipBJ7SABgsEkxLUeVEqP7+mFmTCjG9PNrcZ5+AdJu5SeSB8JJpWxRsC3Q0xkPT+h31QmN3sYelup6pBom9Y6P8kffZs8TabL/CsMKtUYaDmzWDInO/LcaF+7d4ravHxkDQL/Uf2ioF8MKdRvsYZGxqrpGcf+crggsKqUCM6JDsP14Ae4eHQEnlRKHDDvVhrWy2sbJQYl194/C0UtluPv9fWJbA5u1tX+gOw4umQLXdpbDN85hadAKYqXcCQMCkDQ4CJsyL2H1nvOYN74vQwpdlXRISB7/XqwZprJWX39pqN/zf5O4co66LQYWGTMWT3NxVMGti/a+eeO2GFTVNcLPXY2HJ/TDnjNFGNff32z1S0eVEnHhPvBycWwKV+4tJ7u6XWWlUFtcHFVwclCivlEnrkjq7eOKSH83LEoagEVJ5rcGIOpOOjpzKxQK/DkhHJ/sy8afE8IZVqhbY2CRMdPhoK7qPXB2VImbGfq4OeGHv4636HFak2KCni4d+89KoVDAx9URBeV14u7OXdHjRD2Paa+KXDrkOrsZi6cPwvRhwRjTz7+Tn4moc3EOi4x15fyVa2Va1LYzwpVxpZBR82EnIkvIJaSYsmR/o2vh4eyI8VEBUHGPIOrmGFhk7LJhSKizNjzsSI261svwd5QBQR6S6+bqxhB1Z5yHRWQeA4uMdaseFq35/aU6QnTvphVL7moHuHTRnB7queQSDuTRCiL5Y2CRse4UWO5OjACgX73TGWLDvMWfPZ059Yrap7OHX9pDskpILo0ikiF+8suYcZVQdwgsi5KiEBfmjdGt1GXpCKY1YcquUsqfqLtiXiEyjz0sMib2sHSD+RpqBxWmDg2Gp3Pn7HXk7KjCuvtHwV3tgAXX9++U56CeT46F4xhTiCzDHhYZ605DQl1hwoAAZC6ZAgcVcza1jxyHXxTyTFFEssNPfpkSBKFplRADi4hhhXoyxhUi8/jpL1NlNQ1oMKy88XNve68dIrKMpDNDJvFAHq0gkj8Gli6WVVCB3y+UXPW43Cv6XZq9XR2hduASXqKOIJelzKbkOExFJEecw9KJDmRfQXFlPZIGB6K0ugFPfnkIOwy7Db85OwY3xfVu8ZitxzRwclBi2zENACDOZDkvEXUcOYYDufT6EMkRA0snadTqcN/qdJTXNuK+MZHo5e0ihhUA+NcPJ3BjTC8olQp8si8b+WU1mBEdgr98mAEAcDCU0f7LhH42aT9RT9SZOyO3lxz3NyKSIwaWTnA4txR/+s+v4vXvD+fj5hG9JMcUVdbjUG4pfr9Qgpc2nwQA/HqmSLy/USfAQanAiHCfrmk0kR2QYx5gSCGyDANLJ3hrR5bkenFVHS6V6uekRPi5IirQHTtOFOKm//4mOe5AdqnkeoSfK5wcOM2IqFPIMCnIr0VE8sHA0gk8mpWOFwQg0xBGUqYMgLvaQRweUikV0Opa34enf6B7p7aTyO7IsTS/6c9yaRSRDPHreydwVbfMgcYelhAvF0weHIR54/og3NcVHz4wCt8sGNvqeRhYiDqWHPOAHFcuEckRA0sH0hl6Skqr680eE+LlDAD4xx+HYPdTkzCmnz+ie3thVmwoACBpcBAclAqEeDljZkxo5zeayE7JMSdwlRCReRwSugbltQ0oLK9DvwA33PX+PmjKazF3TCQ2H9EvSR4a6okBQR7YePCS+JggT+cW51EoFHjj9ljMvi4cceHeUCgAB6USKiU/vIg6khxXCUnIslFE8sDAYqXTBRXYcaIAUwYH4c/v70NRZR2emzkUv50tBgA8+80x8dgnkwfi7OUqMbDE9PYyO4lWpVQgsZN2OiYiPTn2YMixp4dIjhhYrFDfqMPUN3cDAF7dckq8fem3x1o93sfVCTfFeeOF748DAKYMCer8RhKRReQ4d0R+LSKSD85hscLGg7mS614ujpLrzT//fFyd4OvmhH/MGIzxUf64Z3RkJ7eQiNoixyEhaWl+ubSKSH4YWKyw4fccyfWUKQMwaWCAeP2VW6Lx6i3R4nUvV32gmTe+Lz58IEG8TkS2Icc4IMdhKiI5YmCxUE5JNQ5ml0KhAPzd1Qj3dcUt8b3xx2j9Sp6Y3l74U0woevu6iI/xdOaIG5FcybEzQ4ZNIpIN/kW10L7z+h2WR4T74LOHRkMrCFA7qHBTXC+EersgNswbzo4qJPTxwx+jQxDm68ruXSKZkQ4JyeP3k7s1E1mGgcVC+y/oA8t1kb5wUCnFN07ZbHWPSqnAf/48wgYtJKKrkUtIMcVKt0SW4ZCQhTIuXgEAjIzgZoREPQLDAVG3wsBigdoGLc5ergQARPf2snFriKjdZLlKyHR/I7m0ikh+GFgscKawEjoB8HF1RICH2tbNIaJ2kmMc4JAQkWUYWCxwUlMBABgY7MGJtEQ9BH+ViboXBhYLnMwvBwAMCva0cUuI6FrIcvhFJs0gkjsGFgukG1YIcf4KUfcmx2wgHRKSYwuJ5MHqwLJ7927MnDkToaGhUCgU2LRp01Ufs3PnTowYMQJqtRr9+/fH2rVrWxyzcuVKREZGwtnZGQkJCUhPT7e2aZ2irLoBRy6VAQDG9PO3cWuIqKMwGxB1L1YHlqqqKsTExGDlypUWHX/+/HnMmDEDkyZNQmZmJhYtWoR58+Zh69at4jGfffYZUlJSsHTpUhw4cAAxMTFITk5GYWGhtc3rcHvOFEEQgL4Bbgj2crZ1c4joGsixSJt0mIqIzLG6cNz06dMxffp0i49ftWoV+vTpgzfeeAMAMHjwYOzZswdvvvkmkpOTAQDLly/Hgw8+iLlz54qP+eGHH7B69Wo8/fTT1jaxQ/1wJA8AMGUwd1om6u7kElJMcZUQkWU6fQ5LWloakpKSJLclJycjLS0NAFBfX4+MjAzJMUqlEklJSeIxzdXV1aG8vFxy6QxVdY346aS+l8e4ZxAR9QyymXRLRBbp9MCi0WgQFCTtnQgKCkJ5eTlqampQVFQErVbb6jEajabVcy5btgxeXl7iJSwsrFPaXlbTgOsHBWJQsAeG9eIKIaLuzjSkyKU3Q477GxHJUbdcJbR48WKUlZWJl5ycnE55nlBvF/z3rnhs/ut4zt4nok4hxxBFJEedvvlhcHAwCgoKJLcVFBTA09MTLi4uUKlUUKlUrR4THBzc6jnVajXU6q6rOKtU8lOEqCeQeyCQefOIbKrTe1gSExORmpoquW379u1ITEwEADg5OSE+Pl5yjE6nQ2pqqngMEVFHk0uvqUyaQSR7VgeWyspKZGZmIjMzE4B+2XJmZiays7MB6Idr7r33XvH4hx9+GOfOncNTTz2FkydP4r///S8+//xz/O1vfxOPSUlJwXvvvYd169bhxIkTmD9/PqqqqsRVQ0RE9oDhhcg8q4eE9u/fj0mTJonXU1JSAABz5szB2rVrkZ+fL4YXAOjTpw9++OEH/O1vf8Nbb72F3r174/333xeXNAPA7NmzcfnyZSxZsgQajQaxsbHYsmVLi4m4RETXQv41T+TZKiI5sDqwTJw4EYIgmL2/tSq2EydOxMGDB9s878KFC7Fw4UJrm0NEZDE51jyRy9AUkdx1y1VCREQ9hRxDFJEcMbAQkd2Q1jyRHzm2iUguGFiIyG5Ia57IIx7IpBlEssfAQkRkQ9INGZleiMxhYCEiu8EhIaLui4GFiOyGHCe4cv8gIsswsBAR2ZB0SMh27SCSOwYWIrIb0kAgv3TAwEJkHgMLEdkR+e2MLBmmkmGIIpILBhYiIluSS3IikjkGFiKyG3JfJSTPRhHJAwMLEdkNea4Sav1nIpJiYCEisiG5BCciuWNgISK7YVpJVi4TXKW9PvJoE5EcMbAQkd2Q45CQKRk2iUg2GFiIiGyIvSpElmFgISK7IceqsnLv9SGSCwYWIrIb0mXN8ksHcmwTkVwwsBAR2RB7VYgsw8BCRHZD0oMhk6CgkOF2AURyxMBCRPZDfnlFQo5tIpILBhYiIltiqVsiizCwEJHdkGORNnm0gkj+GFiIyG5IK93KD1cJEZnHwEJEZENyrA1DJEcMLERkN+RYpI29KkSWYWAhIruh4Cohom6LgYWIyIakQ0KMLETmMLAQkd2QFmmTRziQSTOIZI+BhYjshuyHhOTYKCKZYGAhIrIhSa+PDdtBJHcMLERkNxRmr9gOe1WILMPAQkR2SY7LiRleiMxjYCEi+yH7QCD7BhLZDAMLEdkN6SohGzbEhGS7AJm0iUiOGFiIyC4xGxB1LwwsRGQ35NiDoTDzMxFJMbAQkd2Q5V5CrHRLZBEGFiKyS3JcJURE5rUrsKxcuRKRkZFwdnZGQkIC0tPTzR47ceJEKBSKFpcZM2aIx9x3330t7p82bVp7mkZEZJYcezA4JERkGQdrH/DZZ58hJSUFq1atQkJCAlasWIHk5GScOnUKgYGBLY7/+uuvUV9fL14vLi5GTEwMbrvtNslx06ZNw5o1a8TrarXa2qYREbVJOvxiu3aY4iohIstY3cOyfPlyPPjgg5g7dy6GDBmCVatWwdXVFatXr271eF9fXwQHB4uX7du3w9XVtUVgUavVkuN8fHza94qIiCzAcEDUvVgVWOrr65GRkYGkpKSmEyiVSEpKQlpamkXn+OCDD3DHHXfAzc1NcvvOnTsRGBiIgQMHYv78+SguLjZ7jrq6OpSXl0suRERXI8eMIh0SkmMLieTBqsBSVFQErVaLoKAgye1BQUHQaDRXfXx6ejqOHj2KefPmSW6fNm0a1q9fj9TUVLzyyivYtWsXpk+fDq1W2+p5li1bBi8vL/ESFhZmzcsgIjsl7VWRRziQ4zAVkRxZPYflWnzwwQcYPnw4Ro0aJbn9jjvuEH8ePnw4oqOj0a9fP+zcuROTJ09ucZ7FixcjJSVFvF5eXs7QQkRWYTgg6l6s6mHx9/eHSqVCQUGB5PaCggIEBwe3+diqqips2LABDzzwwFWfp2/fvvD398eZM2davV+tVsPT01NyISK6OjmmFDm2iUh+rAosTk5OiI+PR2pqqnibTqdDamoqEhMT23zsF198gbq6Otx9991XfZ7c3FwUFxcjJCTEmuYREbVJMvxiu2ZIcEiIyDJWrxJKSUnBe++9h3Xr1uHEiROYP38+qqqqMHfuXADAvffei8WLF7d43AcffIBZs2bBz89PcntlZSWefPJJ7N27FxcuXEBqaipuvPFG9O/fH8nJye18WUREbZNjOOCkWyLzrJ7DMnv2bFy+fBlLliyBRqNBbGwstmzZIk7Ezc7OhlIpzUGnTp3Cnj17sG3bthbnU6lUOHz4MNatW4fS0lKEhoZi6tSpeOGFF1iLhYg6lBzjgBzbRCRH7Zp0u3DhQixcuLDV+3bu3NnitoEDB0IQhFaPd3FxwdatW9vTDCIiq0iKtMkkKnBIiMgy3EuIiOySHMOBHNtEJBcMLERkN+SYB+TS00MkdwwsRGQ3ZL9KSDatIpIfBhYiskuy3LlZfk0ikg0GFiKyG3LswWBIIbIMAwsR2Q05hgPTECXD5hHJBgMLEdklWYYXGbaJSC4YWIiIbIkhhcgiDCxEZDfkuCJH0cY1ImrCwEJEdkmOwy9ybBORXDCwEJHdkEuviinpdgFEZA4DCxHZDVkWjrN1A4i6CQYWIrJLchx+kWMxOyK5YGAhIrshxzwgx14fIjliYCEiuyEp0iaT9CKPVhDJHwMLEdklOQYFmWQoIlliYCEiuyHHQCBdJSTDBhLJBAMLEdkNhdkrtiOTZhDJHgMLEdklOfZmyLEHiEguGFiIyG7IMhDIsU1EMsTAQkR2SS7hRY49PURyxMBCRHZE3mXw5RKiiOSIgYWI7IYcA4GkcJwcG0gkEwwsRGSX5JINZNIMItljYCEiu6GQ/CyPqMDS/ESWYWAhIrsh9yEXmTePyKYYWIjILsklHEj2N2IfC5FZDCxEZDfkGAfkEpyI5I6BhYjshtznizC8EJnHwEJE9kkm6UDuIYpILhhYiMhuyHOOiBzbRCQ/DCxEZDdk35shy0YRyQMDCxHZJZmMCDULUTJpFJEMMbAQEdkQIwqRZRhYiMhuyL03Qy69PkRyxMBCRHZJLuFA9vNqiGSCgYWI7IYcS/MLgq1bQNQ9MLAQkd1QmPlZLuQYqIjkgoGFiOySXLIBh4SILMPAQkR2Qy4hxZTpkJAc20ckF+0KLCtXrkRkZCScnZ2RkJCA9PR0s8euXbsWCoVCcnF2dpYcIwgClixZgpCQELi4uCApKQlZWVntaRoRkVmSnZGZDoi6FasDy2effYaUlBQsXboUBw4cQExMDJKTk1FYWGj2MZ6ensjPzxcvFy9elNz/6quv4u2338aqVauwb98+uLm5ITk5GbW1tda/IiKibkqOS62J5MLqwLJ8+XI8+OCDmDt3LoYMGYJVq1bB1dUVq1evNvsYhUKB4OBg8RIUFCTeJwgCVqxYgX/84x+48cYbER0djfXr1yMvLw+bNm1q14siImqN7DtV5N4+IhuyKrDU19cjIyMDSUlJTSdQKpGUlIS0tDSzj6usrERERATCwsJw44034tixY+J958+fh0ajkZzTy8sLCQkJZs9ZV1eH8vJyyYWI6Gokq4QYDoi6FasCS1FREbRaraSHBACCgoKg0WhafczAgQOxevVqfPPNN/joo4+g0+kwZswY5ObmAoD4OGvOuWzZMnh5eYmXsLAwa14GEZEsh18YoojM6/RVQomJibj33nsRGxuLCRMm4Ouvv0ZAQAD+97//tfucixcvRllZmXjJycnpwBYTUY8l80Ag8+YR2ZRVgcXf3x8qlQoFBQWS2wsKChAcHGzRORwdHREXF4czZ84AgPg4a86pVqvh6ekpuRARXY10lZANG0JEVrMqsDg5OSE+Ph6pqanibTqdDqmpqUhMTLToHFqtFkeOHEFISAgAoE+fPggODpacs7y8HPv27bP4nERE1pJjXuFSayLzHKx9QEpKCubMmYORI0di1KhRWLFiBaqqqjB37lwAwL333otevXph2bJlAIB//vOfGD16NPr374/S0lK89tpruHjxIubNmwdA/wu6aNEivPjii4iKikKfPn3w7LPPIjQ0FLNmzeq4V0pEdk/ueUDmzSOyKasDy+zZs3H58mUsWbIEGo0GsbGx2LJlizhpNjs7G0plU8fNlStX8OCDD0Kj0cDHxwfx8fH47bffMGTIEPGYp556ClVVVXjooYdQWlqKcePGYcuWLS0KzBERXQuuEiLqvhSC0P33Ci0vL4eXlxfKyso4n4WIzCqurEP8izsAAM/cMAgP/aGfjVsE/H3jEXy8LxsAcPyfyXB1svp7JFG3Zc3fb+4lRER2Q+5zROS41JpILhhYiMhuyD0OyDxPEdkUAwsRERHJHgMLEdkN9mAQdV8MLERkN+Q+R4SBisg8BhYiIiKSPQYWIrIfMu/BkHsPEJEtMbAQkd2Q+5CL3NtHZEsMLERERCR7DCxEZDfk3oEh9/YR2RIDCxGRTMi9Ei+RLTGwEJHdYCAg6r4YWIjIbsgxrpjuPivH9hHJBQMLEdklOS4hZgcQkXkMLERkN0wDgSDp27Ad04zCISsi8xhYiMhuyLFXRR6xiUj+GFiIyC7JMbwQkXkMLERkN+Q+JERE5jGwEBHZkDxiE5H8MbAQkV3ikBBR98LAQkR2g0NCRN0XAwsR2Q059qrIIzYRyR8DCxHZJTmGFyIyj4GFiOwGh4SIui8GFiKyG3IMB/KITUTyx8BCRHaJQ0JE3QsDCxHZDdO9euQyJERElmFgISK7wT4Vou6LgYWI7BKHhIi6FwYWIrIbclwlRESWYWAhIrthOoeFiLoXBhYiskscEiLqXhhYiMgucUiIqHthYCEisiH28xBZhoGFiOySXIaE2M9DZBkGFiKySxwSIupeGFiIiIhI9hhYiMguyWVIiIgsw8BCRHaJQ0JE3Uu7AsvKlSsRGRkJZ2dnJCQkID093eyx7733HsaPHw8fHx/4+PggKSmpxfH33XcfFAqF5DJt2rT2NI2IiIh6IKsDy2effYaUlBQsXboUBw4cQExMDJKTk1FYWNjq8Tt37sSdd96Jn3/+GWlpaQgLC8PUqVNx6dIlyXHTpk1Dfn6+ePn000/b94qIiCzAISGi7sXqwLJ8+XI8+OCDmDt3LoYMGYJVq1bB1dUVq1evbvX4jz/+GI888ghiY2MxaNAgvP/++9DpdEhNTZUcp1arERwcLF58fHza94qIiCzAISGi7sWqwFJfX4+MjAwkJSU1nUCpRFJSEtLS0iw6R3V1NRoaGuDr6yu5fefOnQgMDMTAgQMxf/58FBcXmz1HXV0dysvLJRciIiLquawKLEVFRdBqtQgKCpLcHhQUBI1GY9E5/u///g+hoaGS0DNt2jSsX78eqampeOWVV7Br1y5Mnz4dWq221XMsW7YMXl5e4iUsLMyal0FExCEhom7GoSuf7OWXX8aGDRuwc+dOODs7i7ffcccd4s/Dhw9HdHQ0+vXrh507d2Ly5MktzrN48WKkpKSI18vLyxlaiMgqHBIi6l6s6mHx9/eHSqVCQUGB5PaCggIEBwe3+djXX38dL7/8MrZt24bo6Og2j+3bty/8/f1x5syZVu9Xq9Xw9PSUXIiIiKjnsiqwODk5IT4+XjJh1jiBNjEx0ezjXn31VbzwwgvYsmULRo4cedXnyc3NRXFxMUJCQqxpHhGRxTgkRNS9WL1KKCUlBe+99x7WrVuHEydOYP78+aiqqsLcuXMBAPfeey8WL14sHv/KK6/g2WefxerVqxEZGQmNRgONRoPKykoAQGVlJZ588kns3bsXFy5cQGpqKm688Ub0798fycnJHfQyiYikOCRE1L1YPYdl9uzZuHz5MpYsWQKNRoPY2Fhs2bJFnIibnZ0NpbIpB73zzjuor6/HrbfeKjnP0qVL8dxzz0GlUuHw4cNYt24dSktLERoaiqlTp+KFF16AWq2+xpdHREREPUG7Jt0uXLgQCxcubPW+nTt3Sq5fuHChzXO5uLhg69at7WkGEVG7cUiIqHvhXkJEREQkewwsREREJHsMLERERCR7DCxEREQkewwsREREJHsMLERERCR7DCxEREQkewwsREREJHsMLERENiRwhwAiizCwEBERkewxsBAR2ZCCOwQQWYSBhYjIhjgkRGQZBhYiIiKSPQYWIiIb4pAQkWUYWIiIbIhDQkSWYWAhIiIi2WNgISKyIQ4JEVmGgYWIyIY4JERkGQYWIiIikj0GFiIiG+KQEJFlGFiIiGyIQ0JElmFgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIrskQLB1E4jICgwsRGRX7hkdgRAvZ8y+LtzWTSEiKzjYugFERF3phVnD8M8bh0KhUNi6KURkBfawEJHdYVgh6n4YWIiIiEj22hVYVq5cicjISDg7OyMhIQHp6eltHv/FF19g0KBBcHZ2xvDhw7F582bJ/YIgYMmSJQgJCYGLiwuSkpKQlZXVnqYRERFRD2R1YPnss8+QkpKCpUuX4sCBA4iJiUFycjIKCwtbPf63337DnXfeiQceeAAHDx7ErFmzMGvWLBw9elQ85tVXX8Xbb7+NVatWYd++fXBzc0NycjJqa2vb/8qIiIiox1AIgmDV2r6EhARcd911+M9//gMA0Ol0CAsLw6OPPoqnn366xfGzZ89GVVUVvv/+e/G20aNHIzY2FqtWrYIgCAgNDcXjjz+OJ554AgBQVlaGoKAgrF27FnfcccdV21ReXg4vLy+UlZXB09PTmpdDRGRTi78+gk/TswEAF16eYePWEHUta/5+W9XDUl9fj4yMDCQlJTWdQKlEUlIS0tLSWn1MWlqa5HgASE5OFo8/f/48NBqN5BgvLy8kJCSYPWddXR3Ky8slFyKi7ig2zMvWTSDqFqxa1lxUVAStVougoCDJ7UFBQTh58mSrj9FoNK0er9FoxPuNt5k7prlly5bh+eeft6bpRESydFt8GAQBiI/wsXVTiGStW64SWrx4McrKysRLTk6OrZtERNQuSqUCd4wKR1SQh62bQiRrVgUWf39/qFQqFBQUSG4vKChAcHBwq48JDg5u83jjf605p1qthqenp+RCREREPZdVgcXJyQnx8fFITU0Vb9PpdEhNTUViYmKrj0lMTJQcDwDbt28Xj+/Tpw+Cg4Mlx5SXl2Pfvn1mz0lERET2xerS/CkpKZgzZw5GjhyJUaNGYcWKFaiqqsLcuXMBAPfeey969eqFZcuWAQAee+wxTJgwAW+88QZmzJiBDRs2YP/+/Xj33XcB6CtOLlq0CC+++CKioqLQp08fPPvsswgNDcWsWbM67pUSERFRt2V1YJk9ezYuX76MJUuWQKPRIDY2Flu2bBEnzWZnZ0OpbOq4GTNmDD755BP84x//wDPPPIOoqChs2rQJw4YNE4956qmnUFVVhYceegilpaUYN24ctmzZAmdn5w54iURERNTdWV2HRY5Yh4WIiKj76bQ6LERERES2wMBCREREssfAQkRERLLHwEJERESyx8BCREREssfAQkRERLLHwEJERESyx8BCREREsmd1pVs5Mta+Ky8vt3FLiIiIyFLGv9uW1LDtEYGloqICABAWFmbjlhAREZG1Kioq4OXl1eYxPaI0v06nQ15eHjw8PKBQKDr03OXl5QgLC0NOTg7L/reC7495fG/axvenbXx/zON707bu9P4IgoCKigqEhoZK9iFsTY/oYVEqlejdu3enPoenp6fs/8fbEt8f8/jetI3vT9v4/pjH96Zt3eX9uVrPihEn3RIREZHsMbAQERGR7DGwXIVarcbSpUuhVqtt3RRZ4vtjHt+btvH9aRvfH/P43rStp74/PWLSLREREfVs7GEhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNguYqVK1ciMjISzs7OSEhIQHp6uq2b1Ol2796NmTNnIjQ0FAqFAps2bZLcLwgClixZgpCQELi4uCApKQlZWVmSY0pKSnDXXXfB09MT3t7eeOCBB1BZWdmFr6JzLFu2DNdddx08PDwQGBiIWbNm4dSpU5JjamtrsWDBAvj5+cHd3R233HILCgoKJMdkZ2djxowZcHV1RWBgIJ588kk0NjZ25UvpFO+88w6io6PFglWJiYn48ccfxfvt+b1p7uWXX4ZCocCiRYvE2+z5/XnuueegUCgkl0GDBon32/N7Y3Tp0iXcfffd8PPzg4uLC4YPH479+/eL9/f4z2aBzNqwYYPg5OQkrF69Wjh27Jjw4IMPCt7e3kJBQYGtm9apNm/eLPz9738Xvv76awGAsHHjRsn9L7/8suDl5SVs2rRJOHTokPCnP/1J6NOnj1BTUyMeM23aNCEmJkbYu3ev8Msvvwj9+/cX7rzzzi5+JR0vOTlZWLNmjXD06FEhMzNTuOGGG4Tw8HChsrJSPObhhx8WwsLChNTUVGH//v3C6NGjhTFjxoj3NzY2CsOGDROSkpKEgwcPCps3bxb8/f2FxYsX2+Ildahvv/1W+OGHH4TTp08Lp06dEp555hnB0dFROHr0qCAI9v3emEpPTxciIyOF6Oho4bHHHhNvt+f3Z+nSpcLQoUOF/Px88XL58mXxfnt+bwRBEEpKSoSIiAjhvvvuE/bt2yecO3dO2Lp1q3DmzBnxmJ7+2czA0oZRo0YJCxYsEK9rtVohNDRUWLZsmQ1b1bWaBxadTicEBwcLr732mnhbaWmpoFarhU8//VQQBEE4fvy4AED4/fffxWN+/PFHQaFQCJcuXeqytneFwsJCAYCwa9cuQRD074Wjo6PwxRdfiMecOHFCACCkpaUJgqAPhEqlUtBoNOIx77zzjuDp6SnU1dV17QvoAj4+PsL777/P98agoqJCiIqKErZv3y5MmDBBDCz2/v4sXbpUiImJafU+e39vBEEQ/u///k8YN26c2fvt4bOZQ0Jm1NfXIyMjA0lJSeJtSqUSSUlJSEtLs2HLbOv8+fPQaDSS98XLywsJCQni+5KWlgZvb2+MHDlSPCYpKQlKpRL79u3r8jZ3prKyMgCAr68vACAjIwMNDQ2S92fQoEEIDw+XvD/Dhw9HUFCQeExycjLKy8tx7NixLmx959JqtdiwYQOqqqqQmJjI98ZgwYIFmDFjhuR9APhvBwCysrIQGhqKvn374q677kJ2djYAvjcA8O2332LkyJG47bbbEBgYiLi4OLz33nvi/fbw2czAYkZRURG0Wq3kHz8ABAUFQaPR2KhVtmd87W29LxqNBoGBgZL7HRwc4Ovr26PeO51Oh0WLFmHs2LEYNmwYAP1rd3Jygre3t+TY5u9Pa++f8b7u7siRI3B3d4darcbDDz+MjRs3YsiQIXxvAGzYsAEHDhzAsmXLWtxn7+9PQkIC1q5diy1btuCdd97B+fPnMX78eFRUVNj9ewMA586dwzvvvIOoqChs3boV8+fPx1//+lesW7cOgH18NveI3ZqJbGHBggU4evQo9uzZY+umyMrAgQORmZmJsrIyfPnll5gzZw527dpl62bZXE5ODh577DFs374dzs7Otm6O7EyfPl38OTo6GgkJCYiIiMDnn38OFxcXG7ZMHnQ6HUaOHImXXnoJABAXF4ejR49i1apVmDNnjo1b1zXYw2KGv78/VCpVi1noBQUFCA4OtlGrbM/42tt6X4KDg1FYWCi5v7GxESUlJT3mvVu4cCG+//57/Pzzz+jdu7d4e3BwMOrr61FaWio5vvn709r7Z7yvu3NyckL//v0RHx+PZcuWISYmBm+99ZbdvzcZGRkoLCzEiBEj4ODgAAcHB+zatQtvv/02HBwcEBQUZNfvT3Pe3t4YMGAAzpw5Y/f/dgAgJCQEQ4YMkdw2ePBgcdjMHj6bGVjMcHJyQnx8PFJTU8XbdDodUlNTkZiYaMOW2VafPn0QHBwseV/Ky8uxb98+8X1JTExEaWkpMjIyxGN++ukn6HQ6JCQkdHmbO5IgCFi4cCE2btyIn376CX369JHcHx8fD0dHR8n7c+rUKWRnZ0venyNHjkg+OLZv3w5PT88WH0g9gU6nQ11dnd2/N5MnT8aRI0eQmZkpXkaOHIm77rpL/Nme35/mKisrcfbsWYSEhNj9vx0AGDt2bIsSCqdPn0ZERAQAO/lstvWsXznbsGGDoFarhbVr1wrHjx8XHnroIcHb21syC70nqqioEA4ePCgcPHhQACAsX75cOHjwoHDx4kVBEPRL57y9vYVvvvlGOHz4sHDjjTe2unQuLi5O2Ldvn7Bnzx4hKiqq2yyda8v8+fMFLy8vYefOnZLll9XV1eIxDz/8sBAeHi789NNPwv79+4XExEQhMTFRvN+4/HLq1KlCZmamsGXLFiEgIKBHLL98+umnhV27dgnnz58XDh8+LDz99NOCQqEQtm3bJgiCfb83rTFdJSQI9v3+PP7448LOnTuF8+fPC7/++quQlJQk+Pv7C4WFhYIg2Pd7Iwj6pfAODg7Cv/71LyErK0v4+OOPBVdXV+Gjjz4Sj+npn80MLFfx73//WwgPDxecnJyEUaNGCXv37rV1kzrdzz//LABocZkzZ44gCPrlc88++6wQFBQkqNVqYfLkycKpU6ck5yguLhbuvPNOwd3dXfD09BTmzp0rVFRU2ODVdKzW3hcAwpo1a8RjampqhEceeUTw8fERXF1dhZtuuknIz8+XnOfChQvC9OnTBRcXF8Hf3194/PHHhYaGhi5+NR3v/vvvFyIiIgQnJychICBAmDx5shhWBMG+35vWNA8s9vz+zJ49WwgJCRGcnJyEXr16CbNnz5bUGLHn98bou+++E4YNGyao1Wph0KBBwrvvviu5v6d/NisEQRBs07dDREREZBnOYSEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItn7f+4JLLZurOMWAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df_account_value.account_value.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b8a4f8b-82b5-4709-b1f6-fdb65983f458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vansh\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pyfolio\\pos.py:26: UserWarning: Module \"zipline.assets\" not found; mutltipliers will not be applied to position notionals.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return            0.220646\n",
      "Cumulative returns       0.646168\n",
      "Annual volatility      562.434010\n",
      "Sharpe ratio             0.656889\n",
      "Calmar ratio             0.220848\n",
      "Stability                0.044634\n",
      "Max drawdown            -0.999085\n",
      "Omega ratio            195.379420\n",
      "Sortino ratio          413.397451\n",
      "Skew                          NaN\n",
      "Kurtosis                      NaN\n",
      "Tail ratio               1.195995\n",
      "Daily value at risk    -69.393927\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from finrl.plot import backtest_stats\n",
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8efd816-e70b-450c-96c1-305ddf391f42",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DRLEnsembleAgent' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mensemble_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensemble_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DRLEnsembleAgent' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "ensemble_agent.save(\"ensemble_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83bbeac7-f787-4bc5-b945-56d646ace168",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_ppo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Save trained models\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# model_a2c.save(os.path.join(save_dir, \"model_a2c\"))\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel_ppo\u001b[49m\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_ppo\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      8\u001b[0m model_ddpg\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_ddpg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Save metadata\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_ppo' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "save_dir = \"saved_strategy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save trained models\n",
    "model_a2c.save(os.path.join(save_dir, \"model_a2c\"))\n",
    "model_ppo.save(os.path.join(save_dir, \"model_ppo\"))\n",
    "model_ddpg.save(os.path.join(save_dir, \"model_ddpg\"))\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"A2C_model_kwargs\": A2C_model_kwargs,\n",
    "    \"PPO_model_kwargs\": PPO_model_kwargs,\n",
    "    \"DDPG_model_kwargs\": DDPG_model_kwargs,\n",
    "    \"timesteps_dict\": timesteps_dict,\n",
    "    # Add any other relevant metadata\n",
    "}\n",
    "with open(os.path.join(save_dir, \"metadata.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "# Save summary DataFrame\n",
    "df_summary.to_csv(os.path.join(save_dir, \"summary.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4311dc-1307-4f28-8be1-c10e374e426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.to_csv('df_summary')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
